{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Book Review 2\"\n",
        "description: \"[Your title here]\"\n",
        "author: \"Eric Zou\"\n",
        "date: \"9/28/2025\"\n",
        "categories:\n",
        "  - Review\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What It's About\n",
        "\n",
        "The section notes that human language is so far only known to be learnable by humans, showcasing some failed primate studies. This section then dives deep into exploring the development of our understanding of language, again highlighting the clash between empiricists and formal linguists. Formal linguistics seemed to be more explanatory and powerful than empirical approaches until recently, with Chomsky's advances in grammar and syntax leading the charge. However, as computing power improved over the years, new ways of working with language emerged that didn't require strict rules to produce great results. Advances in word embeddings and neural network architectures like RNNs and transformers resulted in the developments that power moderm LLMs.\n",
        "\n",
        "## What I Thought\n",
        "\n",
        "I was actually caught off guard with how much I learned from this section. I never knew how advanced and interesting rules-based systems were. ELIZA was incredibly interesting to learn about and play around with on the free web emulator, and it really made me think about how much of human communication is just reflecting back the words and ideas of the main speaker. Of course, the difference is, ELIZA wasn't listening. SHRDLU was also really impressive, just imagining the sheer amount of effort that went into designing these rules and this system. I wonder now though, even as statistical models and LLMs have really taken off, if these models are truly \"listening\", so to speak, or if their outputs are just getting better to the point where sometimes we're not able to notice.\n",
        "\n",
        "## Reflecting\n",
        "\n",
        "I think this section of the book has made me think a lot more about how to combine rules-based approaches (regex parsing, for example), into work with powerful statistical models to build on the strengths of each of those approaches instead of solely using one or the other. What I've been noticing so far is that there's a lot more that can be accomplished if we treat the LLM output as more than just text.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
