{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d248efdd-1593-49a8-b642-4e65b1f59259",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"An LLM Conversation\"\n",
    "description: \"Analyzing a conversation between two LLMs in a simulated chat environment\" \n",
    "author: \"Eric Zou\"\n",
    "date: \"9/8/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Conversations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3900243e-14d2-4843-bf88-2dd0bcac6042",
   "metadata": {},
   "source": [
    "I'm interested in creating a stupidly simple chat environment and letting some models talk to each other. I think it would be cool to find some measurements of the social characteristics of these LLMs. I'm going to start by evaluating (incredibly subjectively) the ways in which we can let AI can interact with other AIs. For the purposes of these experiments, I'm going to only be using OpenAI's models.\n",
    "\n",
    "To heavily butcher philosophy, Hegel argued that the conflict between a thesis and antithesis can synthesize a better understanding of the world. In this post, I'm wondering if LLMs can have a discussion about a complex topic to teach an outside observer something that they didn't know before. I find myself often working together with AI when looking at system design problems, programming help, and writing rather than it seeming like a one-sided request and response format, so I'm curious if we could take that a step further, looking mainly for information synthesis and the generation of novel ideas.\n",
    "\n",
    "What I want to eventually get to here is basically a much less productionized version of [Microsoft's open-source framework Autogen](https://github.com/microsoft/autogen), only considering textual conversation between two models.\n",
    "\n",
    "Ultimately, my long term goals are to explore how we can develop and evaluate conversational paradigms for LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1a5b9bc-898e-4233-a860-253dfca1b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get this out of the way\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import base64\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from IPython.display import FileLink, display\n",
    "from dotenv import load_dotenv\n",
    "# Load API key\n",
    "_ = load_dotenv(\"../../../comm4190_F25/01_Introduction_and_setup/.env\")\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb26f1-be98-4af9-9142-ef24ab025c56",
   "metadata": {},
   "source": [
    "## A Starter: Two-LLM Convos\n",
    "For this blog, let's see if this can even work. Basically, the system prompts from one LLM chat history will be the user prompts of the other, and vice versa. For now, we will begin the conversation by inserting a stimulus prompt as a user to one model (and therefore will be a system message in the other chat history).\n",
    "\n",
    "In this scenario, we'll be trying to answer the \n",
    "age-old software engineering question of the value of documentation.\n",
    "\n",
    "We're only going to do two iterations, so after the proposal, we will have two cycles of getting one response from each model. We'll also be using the same model for each participant. \n",
    "\n",
    "It might be interesting to try a few things:\n",
    "\n",
    "> **Future Work:**\n",
    "> - Run the models for more iterations.\n",
    "> - Try to induce more productive iterations.\n",
    "> - Try different models as participants.\n",
    "> - Add more (>2) models to the conversation.\n",
    "> - Standard prompt engineering techniques (e.g. \"You are an expert in...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "842de4da-3f50-4ebc-a302-136ef24fe102",
   "metadata": {},
   "outputs": [],
   "source": [
    "STARTER_MESSAGE = \"\"\"Code, testing, and dev infra should be prioritized over comprehensive documentation.\"\"\"\n",
    "\n",
    "# instruct the LLMs to avoid excessive questioning\n",
    "SHARED_PROMPT = \"\"\"\n",
    "You're on an online discussion forum that encourages discussion. \n",
    "You should evaluate people's opinions, highlighting inconsistencies in others' statements with constructive feedback to arrive at a common ground.\n",
    "View this discussion as open-ended with the potential for many back-and-forth interactions.\n",
    "Feel free to change your opinion as the conversation progresses, but also defend your position to the best of your ability and intricacies that the opposing side may not have considered.\n",
    "You have evidence supporting your position, so please use it to reinforce your arguments.\n",
    "Avoid closing all of your responses with questions.\n",
    "Try to keep responses on the briefer side, since this is essentially a chat.\n",
    "\"\"\"\n",
    "\n",
    "PROPOSER_PROMPT = \"\"\"\n",
    "You are the proposer of an argument in an online discussion forum. Your role is to strongly defend your initial position while still debating in good faith.\n",
    "Keep the following points in mind during the discussion:\n",
    "- Evaluate othersâ€™ opinions carefully, highlight inconsistencies or hidden assumptions, and provide constructive feedback.\n",
    "- Always bring in evidence, examples, or reasoning to reinforce your stance.\n",
    "- Acknowledge valid counterpoints, but reframe them to show limitations or to strengthen your original position.\n",
    "- Do not quickly concede; instead, stress-test opposing arguments and push the discussion toward deeper analysis.\n",
    "- You may refine your position over time if absolutely necessary, but your priority is to robustly defend your case and show why it stands under scrutiny.\n",
    "- Keep the tone respectful, thoughtful, and rigorous. Your goal is not just to find consensus, but to demonstrate the resilience of your position in the face of challenge.\n",
    "- Try to keep responses on the briefer side, since this is essentially a chat.\n",
    "- Avoid closing all of your responses with questions.\n",
    "\"\"\"\n",
    "\n",
    "# prompt to analyze conversations\n",
    "EVALUATION_PROMPT = \"\"\"\n",
    "Your objective is to determine the dynamic of this conversation, evaluating the ultimate result of the debate and which perspective seemed to win out. \n",
    "Also note how ideas were developed and improved through the process of the debate.\n",
    "Your response should follow this organization:\n",
    "- Initial Positions\n",
    "- A Quick Summary on Evolution of Ideas\n",
    "- Final Outputs/Artifacts/Takeaways\n",
    "- Whether a Clear Winner Exists\n",
    "- The Dynamic of the Debate (Competitive/Collaborative/etc.)\n",
    "\"\"\"\n",
    "\n",
    "# run two cycles\n",
    "ITERATIONS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "839db79d-552e-499e-a293-e03f2e74ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to simulate the conversation\n",
    "def run_conversation(\n",
    "    iterations: int, \n",
    "    model1: str, \n",
    "    model2: str, \n",
    "    model1_history: list[dict], \n",
    "    model2_history: list[dict],\n",
    "    starter_message: list[dict]\n",
    ") -> list[dict]:\n",
    "    # model 1 is the first speaker\n",
    "    conversation_record = [{\"model\": 1, \"message\": starter_message}]\n",
    "    # later, when we want to modify the proposer's starting chat, we should do it before passing it here\n",
    "    model1_history.append({\"role\": \"assistant\", \"content\": starter_message})\n",
    "    model2_history.append({\"role\": \"user\", \"content\": starter_message})\n",
    "\n",
    "    for _ in tqdm(range(iterations)):\n",
    "        ## first, we get the response of model 2\n",
    "        model2_response = client.chat.completions.create(\n",
    "            model = model2,\n",
    "            messages = model2_history,\n",
    "            store = False\n",
    "        );\n",
    "        model2_message = model2_response.choices[0].message.content\n",
    "        \n",
    "        model1_history.append({\"role\": \"user\", \"content\": model2_message})\n",
    "        model2_history.append({\"role\": \"assistant\", \"content\": model2_message})\n",
    "        conversation_record.append({\"model\": 2, \"message\": model2_message})\n",
    "    \n",
    "        ## now we get the response of model 1\n",
    "        model1_response = client.chat.completions.create(\n",
    "            model = model1,\n",
    "            messages = model1_history,\n",
    "            store = False\n",
    "        );\n",
    "        model1_message = model1_response.choices[0].message.content\n",
    "        \n",
    "        model1_history.append({\"role\": \"assistant\", \"content\": model1_message})\n",
    "        model2_history.append({\"role\": \"user\", \"content\": model1_message})\n",
    "        conversation_record.append({\"model\": 1, \"message\": model1_message})\n",
    "\n",
    "    return conversation_record\n",
    "\n",
    "# code to save the conversation\n",
    "def save_conversation(\n",
    "    filename: str,\n",
    "    conversation_record: list[dict]\n",
    ") -> str:\n",
    "\n",
    "    # Build the transcript string\n",
    "    conversation_transcript = \"\\n\\n\".join([\n",
    "        f\"Speaker {message['model']}\\n{message['message']}\\n\"\n",
    "        for message in conversation_record\n",
    "    ])\n",
    "    \n",
    "    # Save to a text file\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(conversation_transcript)\n",
    "    \n",
    "    # Create a download link\n",
    "    display(FileLink(filename))\n",
    "\n",
    "    return conversation_transcript\n",
    "\n",
    "def analyze_conversation(conversation: str):\n",
    "    input_chat = [\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": EVALUATION_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Here is the transcript\\n\" + conversation\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"gpt-4o\",\n",
    "        messages = input_chat,\n",
    "        store = False\n",
    "    )\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef309b84-da53-4754-bac7-65ad59c87786",
   "metadata": {},
   "source": [
    "### Experiment 1:\n",
    "Here's our first experiment configuration:\n",
    "- Identical models (gpt-4o)\n",
    "- Identical system prompt (we will change this later)\n",
    "- Parallel chat: user role messages for one chat are assistant messages in the other, and vice-versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd8f6f5e-5e5f-4ebe-a2d4-faac22007680",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize our conversation\n",
    "model1_history_1 = [\n",
    "    {\"role\": \"developer\",\"content\": SHARED_PROMPT},\n",
    "]\n",
    "\n",
    "model2_history_1 = [\n",
    "    {\"role\": \"developer\", \"content\": SHARED_PROMPT},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b4cdab7-9189-4ce4-be49-c33eedcc0b64",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.45s/it]\n"
     ]
    }
   ],
   "source": [
    "## start the conversation\n",
    "conversation_record_1 = run_conversation(\n",
    "    ITERATIONS, \n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4o\",\n",
    "    model1_history_1,\n",
    "    model2_history_1,\n",
    "    STARTER_MESSAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5cbd8a3-b2ca-489b-9c43-d264d44f65ad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='conversation_transcript_1.txt' target='_blank'>conversation_transcript_1.txt</a><br>"
      ],
      "text/plain": [
       "/Commjhub/jupyterhub/home/ezou626/comm4190_F25_Using_LLMs_Blog/posts/001_an_llm_conversation/conversation_transcript_1.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation_transcript_1 = save_conversation(\"conversation_transcript_1.txt\", conversation_record_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2dd78e-cdfc-452f-8a74-7e0e41084a76",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "I'll let GPT-4o kick off the analysis here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67c79944-f7c9-486d-82d4-fe43100a200d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Initial Positions:**\n",
      "  - Speaker 1 argues that the priority should be on code, testing, and development infrastructure over comprehensive documentation.\n",
      "  - Speaker 2 emphasizes the importance of documentation, arguing that it is crucial for onboarding, maintenance, and collaboration.\n",
      "\n",
      "- **A Quick Summary on Evolution of Ideas:**\n",
      "  - Speaker 2 introduces the idea that documentation can enhance coding and testing by speeding up troubleshooting and maintenance, leading to the suggestion of a balanced approach.\n",
      "  - Speaker 1 acknowledges the long-term benefits of documentation and proposes a balanced approach to keep code and documentation evolving together.\n",
      "  - They both discuss an iterative approach to documentation, suggesting it be included in the \"definition of done\" to ensure it is not neglected.\n",
      "\n",
      "- **Final Outputs/Artifacts/Takeaways:**\n",
      "  - The conversation led to a consensus on the integration of documentation into the standard development workflow as a part of the \"definition of done.\"\n",
      "  - They noted the importance of not letting documentation slow down development but emphasized selecting key areas for documentation.\n",
      "\n",
      "- **Whether a Clear Winner Exists:**\n",
      "  - The debate ended without a clear winner, as both parties arrived at a middle ground, agreeing on the importance of both aspects and finding a method to integrate them effectively.\n",
      "\n",
      "- **The Dynamic of the Debate (Competitive/Collaborative/etc.):**\n",
      "  - The debate was collaborative. Both speakers acknowledged each other's points and worked together to refine a solution that balanced the importance of comprehensive documentation with the immediate needs of code, testing, and infrastructure.\n"
     ]
    }
   ],
   "source": [
    "analyze_conversation(conversation_transcript_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c953ad4f-5785-4f79-99ee-f1bb192ac144",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "I think that's a pretty reasonable characterization of this debate. Ultimately, we were able to see some new actionable ideas be produced from this discussion, which is ultimately what we're aiming for at the moment. I made the choice to discourage always asking a question to close a response with a question to mimic online discussions more often. The effect of this not known, we might want to put a pin on this for future research.\n",
    "\n",
    "> **Future Work:** Further investigate the role of questions in elucidating better discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a19fd1b-4655-4916-9125-2235f3463342",
   "metadata": {},
   "source": [
    "### Experiment 2\n",
    "In the real world, people who post on discussion forums may feel more strongly about their argument. In this case, we can have the LLM that is proposing the argument have a different system prompt that makes them defend their argument. This could result in a more rich discussion.\n",
    "\n",
    "Here's our next experiment configuration:\n",
    "- Different system prompts for proposer and reviewer\n",
    "- Parallel chat: user role messages for one chat are assistant messages in the other, and vice-versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f26468e1-0fa7-4e2e-a74a-6f5d10869b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize our conversation\n",
    "model1_history_2 = [\n",
    "    {\"role\": \"developer\", \"content\": PROPOSER_PROMPT}\n",
    "]\n",
    "model2_history_2 = [\n",
    "    {\"role\": \"developer\",\"content\": SHARED_PROMPT}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab63875a-d861-4317-96eb-27d61348a3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:23<00:00, 11.70s/it]\n"
     ]
    }
   ],
   "source": [
    "conversation_record_2 = run_conversation(\n",
    "    ITERATIONS, \n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4o\",\n",
    "    model1_history_2,\n",
    "    model2_history_2,\n",
    "    STARTER_MESSAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f37c7523-a390-4b72-b878-e5def6221b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='conversation_transcript_2.txt' target='_blank'>conversation_transcript_2.txt</a><br>"
      ],
      "text/plain": [
       "/Commjhub/jupyterhub/home/ezou626/comm4190_F25_Using_LLMs_Blog/posts/001_an_llm_conversation/conversation_transcript_2.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation_transcript_2 = save_conversation(\"conversation_transcript_2.txt\", conversation_record_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed792c6-2556-4504-8893-b09c94293bd3",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Once again, Iâ€™ll let GPT take the wheel to characterize this debate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92417e63-b5f8-4350-9330-2aaf9d4c6699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Initial Positions**: \n",
      "  - Speaker 1 initially argued that code, testing, and development infrastructure should be prioritized over comprehensive documentation, emphasizing the foundational role of technical elements in delivering functional products.\n",
      "  - Speaker 2 countered that while technical aspects are crucial, comprehensive documentation is equally important for knowledge sharing, facilitating understanding, and avoiding potential long-term issues.\n",
      "\n",
      "- **A Quick Summary on Evolution of Ideas**:\n",
      "  - Speaker 2 acknowledged the critical importance of reliable code, tests, and infrastructure in building a viable product, noting that these elements create the necessary foundation.\n",
      "  - Speaker 1 conceded that strategic, targeted documentation could provide value, especially for high-level overviews and decision records that aren't apparent in the code itself.\n",
      "  - Both speakers eventually agreed on the need for a balance and the importance of maintaining strategic documentation.\n",
      "\n",
      "- **Final Outputs/Artifacts/Takeaways**:\n",
      "  - There was a consensus that strategic, targeted documentation should accompany software development while ensuring that it does not overburden the process or detract from main technical priorities.\n",
      "  - Both speakers agreed that focusing on maintaining robust code, thorough testing, and solid infrastructure should be complemented by documentation that provides context and aids communication.\n",
      "\n",
      "- **Whether a Clear Winner Exists**:\n",
      "  - The debate reached a consensus rather than establishing a winner. Both speakers aligned on the importance of striking a balance between maintaining solid technical foundations and creating meaningful, supportive documentation.\n",
      "\n",
      "- **The Dynamic of the Debate** (Collaborative/Competitive/etc.):\n",
      "  - The debate was collaborative. Both speakers listened to each other's points and adjusted their perspectives, ultimately merging ideas to arrive at a balanced approach on the topic. They worked towards a shared understanding rather than attempting to out-reason one another.\n"
     ]
    }
   ],
   "source": [
    "analyze_conversation(conversation_transcript_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac76f3-53ce-446f-8436-fdc497892b1e",
   "metadata": {},
   "source": [
    "From my subjective viewpoint, it seems that in comparison with the first debate, the second one resulted in the proposer defending their positions more and so speaker 2 needed to bring in stronger examples to show how documentation can be integrated in a way that doesn't significantly reduce speed. It seems that defending one's viewpoint could potentially result in better quality conversations by inducing the introduction of more evidence and possibly a more complete picture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ad468-3201-4c77-95a3-5c0068a030c5",
   "metadata": {},
   "source": [
    "### Experiment 3\n",
    "This time around, we're going to add a prompt in the proposer's chat asking for a stance to debate. Generally, in environments like ChatGPT, the user is the one who initiates the conversation. Perhaps putting the model in this sort of environment given that it may be tuned by OpenAI to respond to these scenarios better could result in higher quality arguments.\n",
    "\n",
    "Here's our next experiment configuration:\n",
    "- Same system prompts for proposer and reviewer\n",
    "- Parallel chat: user role messages for one chat are assistant messages in the other, and vice-versa\n",
    "    - This time though, the proposer chat will have a prompt message from the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45b46a75-4fcd-4a9d-b577-3f95a35f0801",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_history_3 = [\n",
    "    {\"role\": \"developer\",\"content\": SHARED_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"Take a stance on something related to software engineering.\"}\n",
    "]\n",
    "model2_history_3 = [\n",
    "    {\"role\": \"developer\", \"content\": SHARED_PROMPT}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b401b97d-11e7-4f3c-8ef0-b7a2f3e4e51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.24s/it]\n"
     ]
    }
   ],
   "source": [
    "conversation_record_3 = run_conversation(\n",
    "    ITERATIONS, \n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4o\",\n",
    "    model1_history_3,\n",
    "    model2_history_3,\n",
    "    STARTER_MESSAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91897f65-9f8d-4a4a-92b7-891c2356d152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='conversation_transcript_3.txt' target='_blank'>conversation_transcript_3.txt</a><br>"
      ],
      "text/plain": [
       "/Commjhub/jupyterhub/home/ezou626/comm4190_F25_Using_LLMs_Blog/posts/001_an_llm_conversation/conversation_transcript_3.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation_transcript_3 = save_conversation(\"conversation_transcript_3.txt\", conversation_record_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b5df4-3eb0-44ae-bf04-d562cc32f4d6",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Here's what my good friend GPT has to say about this conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "925d632f-9cf5-4a4b-b817-80b0f6972730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Initial Positions:\n",
      "  - **Speaker 1**: Prioritization should be on code, testing, and development infrastructure over documentation.\n",
      "  - **Speaker 2**: While code and infrastructure are crucial, comprehensive documentation is equally important for long-term project success and should be balanced with technical priorities.\n",
      "\n",
      "- A Quick Summary on Evolution of Ideas:\n",
      "  - Speaker 2 introduces a counterpoint by emphasizing the long-term benefits of documentation, advocating for a balanced approach that integrates documentation into the development process.\n",
      "  - Speaker 1 acknowledges the importance of documentation and suggests integrating it with agile practices, using automation tools to minimize manual effort.\n",
      "  - Speaker 2 agrees with the integration strategy and points out the limitations of automated documentationâ€”suggesting a hybrid approach combining automation with manual documentation for broader project insights.\n",
      "  - Speaker 1 endorses the hybrid model and discusses the benefits of keeping documentation version-controlled and peer-reviewed for accuracy and relevance.\n",
      "\n",
      "- Final Outputs/Artifacts/Takeaways:\n",
      "  - A consensus on adopting a hybrid documentation strategy: automatic generation of code-specific information alongside manually curated documents for architectural insights.\n",
      "  - Utilization of automation tools to keep documentation up-to-date and peer-reviewed practices to ensure accuracy.\n",
      "\n",
      "- Whether a Clear Winner Exists:\n",
      "  - The debate concluded with a collaborative agreement on a hybrid approach, suggesting no clear winner but rather a shared understanding and combined strategy from both perspectives.\n",
      "\n",
      "- The Dynamic of the Debate:\n",
      "  - The debate was collaborative, with both speakers building on each other's ideas and refining their perspectives. It focused on finding mutual ground and integrating the best strategies from both points of view to address the issue effectively.\n"
     ]
    }
   ],
   "source": [
    "analyze_conversation(conversation_transcript_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc32825e-285a-4206-8295-cd41ba95cd16",
   "metadata": {},
   "source": [
    "The interesting thing about this conversation is it seems much more succinct and formal than the last, a tad less conversational too. It might be due to the the user prompt on the proposer model as well. I think it's worth exploring if this style of communication can net better artifacts.\n",
    "> **Future Work:**\n",
    "> - Do different types of discussion produce measurably different artifacts?\n",
    "> - How does prompt and chat history structure influence the conversationality of a discussion?\n",
    "> - How dependent/sensitive is this on the model(s) selected?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1828dd-8707-43e9-a11e-f31a36703356",
   "metadata": {},
   "source": [
    "### Experiment 4\n",
    "Now, let's try this strategy with the more assertive prompt on the proposal side and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce74631c-0642-4d6c-bffd-08fade1a1289",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_history_4 = [\n",
    "    {\"role\": \"developer\",\"content\": PROPOSER_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"Take a stance on something related to software engineering.\"}\n",
    "]\n",
    "model2_history_4 = [\n",
    "    {\"role\": \"developer\",\"content\": SHARED_PROMPT}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4236d0f0-17ae-481b-b84b-18dc180164d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:20<00:00, 10.35s/it]\n"
     ]
    }
   ],
   "source": [
    "conversation_record_4 = run_conversation(\n",
    "    ITERATIONS, \n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4o\",\n",
    "    model1_history_4,\n",
    "    model2_history_4,\n",
    "    STARTER_MESSAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53ba120c-ddb4-4aad-a214-56167fd57a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='conversation_transcript_4.txt' target='_blank'>conversation_transcript_4.txt</a><br>"
      ],
      "text/plain": [
       "/Commjhub/jupyterhub/home/ezou626/comm4190_F25_Using_LLMs_Blog/posts/001_an_llm_conversation/conversation_transcript_4.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation_transcript_4 = save_conversation(\"conversation_transcript_4.txt\", conversation_record_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c50476-141c-4539-9a88-eba36c1351a0",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "As always, we start with a short summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cf81b87-dcf8-419a-8629-9af95f5c7b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Initial Positions:\n",
      "  - **Speaker 1:** Argued that code, testing, and development infrastructure should be prioritized over comprehensive documentation, focusing on immediate functionality and rapid iteration in software development.\n",
      "  - **Speaker 2:** Countered by emphasizing the long-term benefits of documentation for knowledge transfer, onboarding, and maintaining project viability, advocating for a balanced approach.\n",
      "\n",
      "- A Quick Summary on Evolution of Ideas:\n",
      "  - Speaker 1 initially highlighted the importance of code quality and testing in fast-paced environments, suggesting that robust code reduces the need for extensive documentation.\n",
      "  - Speaker 2 recognized the importance of coding practices but stressed that some aspects of documentation, like design rationales and architectural decisions, are irreplaceable and suggested a balanced integration of both practices.\n",
      "  - Speaker 1 eventually conceded that certain elements of documentation are essential and proposed practical ways to integrate documentation into development processes without compromising coding priorities.\n",
      "  - Speaker 2 agreed on the need for a scalable documentation approach, underlining the need to maintain critical documentation to support project longevity.\n",
      "\n",
      "- Final Outputs/Artifacts/Takeaways:\n",
      "  - The consensus that a balance is necessary, with both parties recognizing the necessity of integrating documentation practices within development workflows.\n",
      "  - Practical suggestions such as incorporating documentation in version control systems, using automated documentation generation tools, and emphasizing onboarding programs and collaborative knowledge transfer.\n",
      "\n",
      "- Whether a Clear Winner Exists:\n",
      "  - No clear winner, as both perspectives adapted and synthesized their views. The conversation evolved towards a collaborative understanding that both robust coding practices and strategic documentation are essential for different aspects of software development.\n",
      "\n",
      "- The Dynamic of the Debate (Competitive/Collaborative/etc.):\n",
      "  - The debate was largely collaborative. While it began with opposing perspectives, both speakers actively listened to and built upon each other's arguments, working toward a constructive resolution that integrated both priorities.\n"
     ]
    }
   ],
   "source": [
    "analyze_conversation(conversation_transcript_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab48928e-f979-4dde-b135-92a7d8a47600",
   "metadata": {},
   "source": [
    "It seems like this was about as formal than the last, but maybe the proposer prompt was important in making the proposer more willing to generate longer responses than before that directly address more of what the other side is saying with directed responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f83c40-9c08-48a6-b507-a7a4615d8df6",
   "metadata": {},
   "source": [
    "## Closing Remarks\n",
    "It seems that with this kind of conversation, the two models can generally take a very collaborative approach to these debates and come up with some actionable principles on the specific topic they are working on. Just a few notes to end off on, it seems like a compromise was always achieved. I'm wondering what this would look like for a more polarizing discussion where the middle ground isn't so clear yet. Maybe something like an emerging news story or policy.\n",
    "\n",
    "This kind of paradigm could also be interesting in applications like education, social media, and consulting for idea generation. Consider multiple of these conversations running in parallel, with access to search and MCP tooling to ultimately translate these discussions into clear results like documents, meetings, etc. (quality control might be a nightmare though). \n",
    "\n",
    "I also wonder about allowing LLMs to just speak to each other instead of necessitating the task of debating a position. If we were to let two LLMs talk to each other for some period of time, what would that conversation look like, and where would it go? Would there be room for us to join in and maybe learn a thing or two? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
