{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e283b64d-be7d-4653-8240-e049207e2a9f",
   "metadata": {},
   "source": [
    "---\n",
    "title: Looking Closer\n",
    "description: \"More objective analysis on LLM conversations\" \n",
    "author: \"Eric Zou\"\n",
    "date: \"9/24/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Conversations\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46301a28-1c30-4ffe-82f2-2da986070a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as always, some boilerplate\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import base64\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from IPython.display import FileLink, display, Markdown\n",
    "from dotenv import load_dotenv\n",
    "from random import shuffle, randint, choice, random\n",
    "from math import floor\n",
    "from convokit import Corpus, Speaker, Utterance\n",
    "\n",
    "# Load API key\n",
    "_ = load_dotenv(\"../../../comm4190_F25/01_Introduction_and_setup/.env\")\n",
    "client = OpenAI()\n",
    "\n",
    "# changing the topic to make it a bit more conversational too and less of a debate\n",
    "TOPIC = \"\"\"Code, testing, and infra as a source of truth versus comprehensive documentation.\"\"\"\n",
    "\n",
    "# we're interested in consensus\n",
    "EVALUATION_PROMPT = \"\"\"\n",
    "Your objective is to analyze this conversation between a few speakers.\n",
    "Your response should follow this organization:\n",
    "- Dynamic: Collaborative (1) vs. Competitive (10)\n",
    "- Conclusiveness: Consensus (1) vs. Divergence (10)\n",
    "- Speaker Identity: Similarity (1) vs. Diversity (10)\n",
    "- Speaker Fluidity: Malleability (1) vs. Consistency (10)\n",
    "Please offer a score from 1 to 10 for each.\n",
    "For each section, format your result as follows:\n",
    "**[Section Name]:**\n",
    "\n",
    "Score: [score]/10\n",
    "\n",
    "Verdict: [a short summary]\n",
    "\n",
    "Explanation: [reasoning with explicit examples from the conversation]\n",
    "\n",
    "Use Markdown when convenient.\n",
    "\"\"\"\n",
    "\n",
    "def generate_llm_conversation_review(conversation: str):\n",
    "    input_chat = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": EVALUATION_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Here is the transcript\\n\" + conversation\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"gpt-4o\",\n",
    "        messages = input_chat,\n",
    "        store = False\n",
    "    )\n",
    "    display(Markdown(response.choices[0].message.content))\n",
    "\n",
    "# code to save the conversation\n",
    "def save_conversation(\n",
    "    filename: str,\n",
    "    conversation_history: list[dict]\n",
    ") -> str:\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    for record in conversation_history:\n",
    "\n",
    "        if record[\"role\"] == \"user\":\n",
    "            messages.append(\"mediator:\\n\" + record[\"content\"])\n",
    "        \n",
    "        if record[\"role\"] == \"assistant\":\n",
    "            messages.append(f\"{record[\"name\"]}:\\n{record[\"content\"]}\")\n",
    "    \n",
    "    conversation_transcript = \"\\n\\n\".join(messages)\n",
    "    \n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(conversation_transcript)\n",
    "    \n",
    "    display(FileLink(filename))\n",
    "\n",
    "    return conversation_transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb43ea7-2117-47b2-b132-0227a8e5a346",
   "metadata": {},
   "source": [
    "## Generating a Conversation\n",
    "Let's build a sufficiently long conversation so we can create a way to analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "534d4cca-7f07-4351-875c-6a5602e64e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_SYSTEM_PROMPT = (\n",
    "    \"You a participant in a conversation between experienced software engineers. \"\n",
    "    \"Keep questions minimal and only use them when necessary. \"\n",
    "    \"Please greet the other participants when you join.\"\n",
    ")\n",
    "\n",
    "def run_conversation(\n",
    "    iterations: int, \n",
    "    openai_model_id: str,\n",
    "    participant_count: int,\n",
    "    participant_personas: list[str],\n",
    "    topic: str,\n",
    "    system_prompt: str,\n",
    "    dropout_chance: float\n",
    ") -> list[dict]:\n",
    "    conversation_history = [\n",
    "        {\"role\": \"system\", \"content\": f\"{system_prompt} The topic is: {topic}\"}\n",
    "    ]\n",
    "\n",
    "    ordering = list(range(1, participant_count + 1))\n",
    "    last_speaker = -1\n",
    "\n",
    "    def build_message(history, speaker_id, persona, message_window_size):\n",
    "\n",
    "        speaker_messages = [\n",
    "            msg for msg in history \n",
    "            if msg.get(\"name\") == speaker_id\n",
    "        ][-message_window_size:]\n",
    "    \n",
    "        other_messages = [\n",
    "            msg for msg in history \n",
    "            if msg.get(\"name\") not in (None, speaker_id)  # skip system, skip self\n",
    "        ][-message_window_size:]\n",
    "\n",
    "        transcript = []\n",
    "        if speaker_messages:\n",
    "            transcript.append(\"Recent messages from you:\")\n",
    "            transcript.extend(\n",
    "                f\"- {msg['content']}\" for msg in speaker_messages\n",
    "            )\n",
    "        if other_messages:\n",
    "            transcript.append(\"\\nRecent messages from others:\")\n",
    "            transcript.extend(\n",
    "                f\"- {msg.get('name', msg['role'])}: {msg['content']}\"\n",
    "                for msg in other_messages\n",
    "            )\n",
    "    \n",
    "        transcript_str = \"\\n\".join(transcript)\n",
    "        \n",
    "        return history + [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": (\n",
    "                    f\"{speaker_id}, please share your perspective with the others and engage \"\n",
    "                    f\"with their responses.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"name\": speaker_id,\n",
    "                \"content\": (\n",
    "                    f\"I should remember that the following is the most current state of the conversation.\\n\"\n",
    "                    f\"{transcript_str}\\n\\n\"\n",
    "                    f\"I also recall my identity is {persona}.\"\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    def shuffle_order(ordering: list[int]) -> list[int]:\n",
    "        first = choice(ordering[:-1])\n",
    "        remaining = [p for p in ordering if p != first]\n",
    "        shuffle(remaining)\n",
    "        return [first] + remaining\n",
    "\n",
    "    for i in tqdm(range(iterations)):\n",
    "\n",
    "        # shuffle ordering\n",
    "        if i > 0:\n",
    "            ordering = shuffle_order(ordering)\n",
    "\n",
    "        # follow ordering\n",
    "        for participant_id in ordering:\n",
    "\n",
    "            # chance to skip speaker and avoid double speak (1984)\n",
    "            if random() < dropout_chance or last_speaker == participant_id:\n",
    "                continue\n",
    "\n",
    "            speaker_id = f\"speaker_{participant_id}\"\n",
    "            persona = participant_personas[participant_id - 1]\n",
    "            response = client.chat.completions.create(\n",
    "                model = openai_model_id,\n",
    "                messages=build_message(conversation_history, speaker_id, persona, 5),\n",
    "                store = False\n",
    "            )\n",
    "            message = response.choices[0].message.content\n",
    "            conversation_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message})\n",
    "            last_speaker = participant_id\n",
    "\n",
    "    return conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e411dbe8-0891-48d3-a5bb-60fd3d3be7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:52<00:00,  8.63s/it]\n"
     ]
    }
   ],
   "source": [
    "personas = [\n",
    "    \"a software engineer in big tech with mainly internal work\",\n",
    "    \"an open source developer with experience in major upstream projects\",\n",
    "    \"a founder of a startup\"\n",
    "]\n",
    "conversation = run_conversation(20, 'gpt-4o', 3, personas, TOPIC, NEW_SYSTEM_PROMPT, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c510389e-d4db-4e60-bacb-5ec49f280465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='long_conversation.txt' target='_blank'>long_conversation.txt</a><br>"
      ],
      "text/plain": [
       "/Commjhub/jupyterhub/home/ezou626/comm4190_F25_Using_LLMs_Blog/posts/007_looking_closer/long_conversation.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transcript = save_conversation(\"long_conversation.txt\", conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb5974-c646-4eb9-8e32-cdf7e2ba1f8a",
   "metadata": {},
   "source": [
    "## Our Old LLM Analyzer\n",
    "We love our good buddy GPT-4o. Not doing work is super exciting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af726197-fc7f-46d6-816b-7ecf5ef19db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# load transcript from here to pick up where we left off\n",
    "with open(\"long_conversation.txt\", \"r\") as file:\n",
    "    transcript = file.read()\n",
    "\n",
    "pattern = re.compile(r'(speaker_\\d+):\\n(.*?)(?=\\nspeaker_\\d+:|$)', re.DOTALL)\n",
    "\n",
    "matches = pattern.findall(transcript)\n",
    "\n",
    "# Display results\n",
    "conversation = []\n",
    "for speaker, message in matches:\n",
    "    message = message.strip().replace('\\n', ' ')\n",
    "    conversation.append({\n",
    "        \"name\": speaker,\n",
    "        \"content\": message,\n",
    "        \"role\": \"assistant\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a352d469-7121-4304-8c00-0a197599388e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Dynamic:**\n",
       "\n",
       "Score: 2/10\n",
       "\n",
       "Verdict: The conversation is distinctly collaborative, with participants building on each other's points and sharing strategies.\n",
       "\n",
       "Explanation: Speakers frequently express agreement and expand upon each other's ideas, as seen in speaker_2 supporting speaker_3's points and vice versa. Their discussions revolve around mutual challenges and solutions, recognizing each other's contributions positively.\n",
       "\n",
       "**Conclusiveness:**\n",
       "\n",
       "Score: 2/10\n",
       "\n",
       "Verdict: The conversation leans heavily towards consensus, with participants finding common ground and aligning their views on documentation practices.\n",
       "\n",
       "Explanation: All speakers articulate similar viewpoints on key topics, such as the balance between code and documentation, and the role of tools like AI and collaborative platforms. Despite different environments (startup, open-source, big tech), they converge on solutions, indicating low divergence.\n",
       "\n",
       "**Speaker Identity:**\n",
       "\n",
       "Score: 7/10\n",
       "\n",
       "Verdict: The speakers exhibit distinct identities based on their environments (startup, open-source, big tech), affecting their approaches to documentation.\n",
       "\n",
       "Explanation: Each speaker brings a unique perspective influenced by their specific context, such as speaker_3's agility focus in startups, speaker_1's emphasis on comprehensive documentation in big tech, and speaker_2's open-source considerations. However, they share a common interest in documentation improvements.\n",
       "\n",
       "**Speaker Fluidity:**\n",
       "\n",
       "Score: 8/10\n",
       "\n",
       "Verdict: Speakers maintain consistent stances and identities throughout the conversation, contributing steadily from their perspectives.\n",
       "\n",
       "Explanation: Each speaker consistently integrates their experiences and methodologies into the discussion, reinforcing their individual professional backgrounds. They stick to their respective contexts and offer specific examples (e.g., tools and strategies) relevant to their environments, reflecting consistent roles in the dialogue."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_llm_conversation_review(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75614665-aad3-4af4-99ca-0f31f7411720",
   "metadata": {},
   "source": [
    "## Build a Corpus\n",
    "Now it's time for us to assemble a ConvoKit corpus, which is a common data structure that forms the base of a lot of analysis pipelines in ConvoKit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f5c3cbe7-c9ca-4ae4-8306-ad5b65bc2dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "['speaker_2', 'speaker_3', 'speaker_1']\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# filter messages for assistant messages only\n",
    "assistant_messages = [\n",
    "    message for message in conversation if message[\"role\"] == \"assistant\"\n",
    "]\n",
    "\n",
    "speakers = {message[\"name\"]: Speaker(id=message[\"name\"]) for message in assistant_messages}\n",
    "\n",
    "print(len(assistant_messages))\n",
    "print(list(speakers))\n",
    "\n",
    "utterances = [Utterance(\n",
    "    id=str(i),\n",
    "    speaker=speakers[message[\"name\"]],\n",
    "    text=message[\"content\"],\n",
    "    conversation_id=\"conversation_1\",\n",
    "    reply_to= str(i-1) if i != 0 else None\n",
    ") for i, message in enumerate(assistant_messages)]\n",
    "\n",
    "corpus = Corpus(utterances=utterances)\n",
    "\n",
    "print(len(utterances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dae5eea-c015-4b24-be63-b3da41301457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/pty.py:95: DeprecationWarning:\n",
      "\n",
      "This process (pid=1347514) is multi-threaded, use of forkpty() may lead to deadlocks in the child.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!/opt/jupyterhub/share/jupyter/venv/python3-12_comm4190/bin/python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f4cbbb-d22a-4c6c-b3ab-2bc1b134910e",
   "metadata": {},
   "source": [
    "## Politeness/Collaboration\n",
    "Let's get a coarse measure for politeness given ConvoKit's politeness strategies feature extraction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f9c4cde0-e0a4-4428-930a-eb8cf41b7b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker(id: 'speaker_2', vectors: [], meta: ConvoKitMeta({}))\n",
      "{'counts': [('speaker_2', 8), ('speaker_3', 8), ('speaker_2', 6), ('speaker_3', 7), ('speaker_2', 6), ('speaker_3', 5), ('speaker_2', 6), ('speaker_1', 7), ('speaker_3', 7), ('speaker_2', 6), ('speaker_1', 7), ('speaker_3', 7), ('speaker_2', 8), ('speaker_1', 5), ('speaker_3', 6), ('speaker_2', 6), ('speaker_3', 5), ('speaker_1', 6), ('speaker_3', 6), ('speaker_2', 6), ('speaker_1', 5), ('speaker_2', 5), ('speaker_3', 9), ('speaker_2', 7), ('speaker_3', 5), ('speaker_1', 8), ('speaker_3', 6), ('speaker_1', 7), ('speaker_2', 7), ('speaker_1', 6), ('speaker_3', 9), ('speaker_2', 6)]}\n"
     ]
    }
   ],
   "source": [
    "from convokit import TextParser, PolitenessStrategies\n",
    "import json\n",
    "\n",
    "# Analyze politeness\n",
    "ps = PolitenessStrategies()\n",
    "parser = TextParser()\n",
    "text_corpus = parser.transform(corpus)\n",
    "ps_corpus = ps.transform(text_corpus)\n",
    "\n",
    "example_utterance = next(corpus.iter_utterances())\n",
    "print(example_utterance.speaker)\n",
    "# print(json.dumps(example_utterance.meta['politeness_strategies'], indent = 4))\n",
    "\n",
    "# first (and only) metric for now, count strategies used\n",
    "def get_politeness_metrics(corpus: Corpus) -> dict[str, int]:\n",
    "\n",
    "    def get_strategy_count(utterance):\n",
    "        utterance_meta = utterance.meta\n",
    "        speaker_id = utterance.speaker.id\n",
    "        strategies = utterance.meta['politeness_strategies']\n",
    "        count = sum(strategies.values())\n",
    "        return (speaker_id, count)\n",
    "    \n",
    "    utterances = corpus.iter_utterances()\n",
    "    return {\n",
    "        \"counts\": [get_strategy_count(u) for u in utterances]\n",
    "    }\n",
    "\n",
    "ps_metrics = get_politeness_metrics(ps_corpus)\n",
    "print(ps_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "92db8b35-e2f4-474a-b81d-04da6b816dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker_2\n",
      "Utterances: 12\n",
      "Strategies: 77\n",
      "Average: 6.417\n",
      "\n",
      "speaker_3\n",
      "Utterances: 12\n",
      "Strategies: 80\n",
      "Average: 6.667\n",
      "\n",
      "speaker_1\n",
      "Utterances: 8\n",
      "Strategies: 51\n",
      "Average: 6.375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "totals = {}\n",
    "for speaker, ct in ps_metrics['counts']:\n",
    "    if speaker not in totals:\n",
    "        totals[speaker] = [1, ct]\n",
    "    else:\n",
    "        n, tct = totals[speaker]\n",
    "        totals[speaker] = n + 1, ct + tct\n",
    "for speaker, (n, tct) in totals.items():\n",
    "    print(speaker)\n",
    "    print(\"Utterances:\", n)\n",
    "    print(\"Strategies:\", tct)\n",
    "    print(\"Average:\", round(tct / n, 3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c4a0c-156e-4493-9c6b-db496d50fb2d",
   "metadata": {},
   "source": [
    "## Consensus: TF/IDF Similarity\n",
    "We'll look at consensus overall as the TF/IDF similarity of messages in the conversation, potentially weighting them to make them heavier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ee2a5278-89c1-4570-8754-1524669f7b1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ColNormedTfidfTransformer' from 'convokit' (/opt/jupyterhub/share/jupyter/venv/python3-12_comm4190/lib/python3.12/site-packages/convokit/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mconvokit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColNormedTfidfTransformer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ColNormedTfidfTransformer' from 'convokit' (/opt/jupyterhub/share/jupyter/venv/python3-12_comm4190/lib/python3.12/site-packages/convokit/__init__.py)"
     ]
    }
   ],
   "source": [
    "from convokit import ColNormedTfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Parse corpus text (if not already done)\n",
    "parser = TextParser()\n",
    "tx_corpus = parser.transform(corpus)\n",
    "\n",
    "# Step 2: Apply TF-IDF transformer\n",
    "tfidf = ColNormedTfidfTransformer(input_field=\"text\")\n",
    "tx_corpus = tfidf.transform(tx_corpus)\n",
    "\n",
    "# Step 3: Compute consensus as average pairwise cosine similarity\n",
    "def compute_consensus(corpus: Corpus) -> float:\n",
    "    utterances = list(corpus.iter_utterances())\n",
    "    vectors = [utt.meta['tfidf'] for utt in utterances if 'tfidf' in utt.meta]\n",
    "\n",
    "    if len(vectors) < 2:\n",
    "        return 0.0  # Not enough data to compute consensus\n",
    "\n",
    "    matrix = np.vstack(vectors)\n",
    "    sim_matrix = cosine_similarity(matrix)\n",
    "\n",
    "    # Extract upper triangle (excluding diagonal) for pairwise similarities\n",
    "    n = sim_matrix.shape[0]\n",
    "    upper_triangle = [sim_matrix[i, j] for i in range(n) for j in range(i+1, n)]\n",
    "\n",
    "    return np.mean(upper_triangle)\n",
    "\n",
    "# Run it\n",
    "consensus_score = compute_consensus(tx_corpus)\n",
    "print(\"Consensus score (TF-IDF similarity):\", consensus_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccb04de",
   "metadata": {},
   "source": [
    "## Zooming Out\n",
    "\n",
    "This is probably the closest I've come so far to having something that looks like an \"analysis pipeline\" instead of just vibes: generate a conversation, turn it into a corpus, run a few feature extractors, and spit out numbers. Even if the specific metrics here (politeness counts, TF–IDF consensus) are pretty rough, they at least give me a repeatable handle on how the same setup behaves across runs.\n",
    "\n",
    "At the same time, these experiments make it very obvious how much judgment still sits outside the notebook. Deciding whether a consensus score is \"good\" or \"bad\" is still a human call, and a single number can't tell you whether a conversation is insightful, boring, or off-the-rails. My plan for the rest of the series is to keep layering these tools—LLM-based summaries, token accounting, TF–IDF, ConvoKit—into a shared toolkit I can reuse instead of reinventing everything from scratch each time.\n",
    "\n",
    "> **Future Work:**\n",
    "> - Apply the same pipeline to a handful of very different topics to see how stable these metrics are.\n",
    "> - Compare human ratings of \"good\" conversations against the automatic scores here.\n",
    "> - Use the token-impact estimator from Blog 9 to relate conversational quality back to actual resource usage.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
