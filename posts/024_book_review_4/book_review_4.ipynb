{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Book Review 4\"\n",
        "description: \"Section 4\"\n",
        "author: \"Eric Zou\"\n",
        "date: \"10/12/2025\"\n",
        "categories:\n",
        "  - Review\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What It's About\n",
        "\n",
        "This section of the book begins to delve into the real world implications of systems like ChatGPT and their influence through language. The section details posttraining strategies like RLHF and SFT, which take pretrained models and tune them for specific tasks like being an assistant. The authors note examples of modern LLMs' power of persuasion and the illusion that they are real, while also mentioning hallucinations.\n",
        "\n",
        "## What I Thought\n",
        "\n",
        "I thought this section was pretty jam-packed with info, which was nice. There's a lot of different opinions provided, and I think that's what makes this book really great. The fact that you get to see all of these viewpoints and decide for yourself the weight to place on each of these issues really feels empowering as a reader. I think also this draws attention to the real issues of LLMs, not being extinction-level weapons like nuclear bombs, but more being agents of control and deception that can hurt human progress.\n",
        "\n",
        "## Reflecting\n",
        "\n",
        "I think it's important to keep these things in mind as I use LLMs throughout this blog, both as an explorer and a designer. When we use and study the workings of various LLMs, we need to acknowledge that there are very real flaws with the technology that may manifest in our experiments, and these can cloud our results and conclusions. We should be careful to identify what may or may not be factual and make this clear to whoever we communicate our work to. As designers/creators, we must also ensure that we don't create environments where the negatives of LLMs can become overpowering or result in harm. For example, a digital open world game filled with agents may result in people spending an overly large amount of time with agents, or be exposed to wrong/harmful outputs created by agents in such a world. It's the responsibility of the wielders of the technology to manage it correctly. \n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
