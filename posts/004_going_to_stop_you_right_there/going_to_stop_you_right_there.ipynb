{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144a941d-53e0-4d8f-a657-13e2d02d9c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "title: \"Going to Have to Stop You Right There\"\n",
    "description: \"Probabilistic rudeness for better discussions\" \n",
    "author: \"Eric Zou\"\n",
    "date: \"9/15/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Conversations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea3d884-46ea-444d-bef6-53f8848e06e4",
   "metadata": {},
   "source": [
    "## Randomness as Interjections\n",
    "We're going to try to continue improving on the authenticity and naturalness of the conversations we're generating. One issue with LLM conversations is how structured they are, even without incredibly formal language or long responses. The fact that each model gets to (and needs to) completely finish before the next one can speak means that things that often happen in real conversations (interruptions, affirmations, etc.). This is not something that our paradigm explored [last time](https://ezou626.github.io/comm4190_F25_Using_LLMs_Blog/posts/003_approaching_a_true_discussion/Spontaneity.html) has changed with random speaker ordering and potentially skipping speakers. \n",
    "\n",
    "We're going to try to add these things to hopefully make these conversations even more real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1ff5f2a-088b-422f-81e2-b0a1df58faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, some boilerplate\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import base64\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from IPython.display import FileLink, display\n",
    "from dotenv import load_dotenv\n",
    "from random import shuffle, randint, choice, random\n",
    "from math import floor\n",
    "# Load API key\n",
    "_ = load_dotenv(\"../../../comm4190_F25/01_Introduction_and_setup/.env\")\n",
    "client = OpenAI()\n",
    "\n",
    "# changing the topic to make it a bit more conversational too and less of a debate\n",
    "TOPIC = \"\"\"Code, testing, and infra as a source of truth versus comprehensive documentation.\"\"\"\n",
    "\n",
    "# prompt to analyze conversations\n",
    "EVALUATION_PROMPT = \"\"\"\n",
    "Your objective is to analyze this conversation between speakers.\n",
    "Your response should follow this organization:\n",
    "- A Brief Summary\n",
    "- Final Outputs/Artifacts/Takeaways\n",
    "- Characteristics/Dynamic (Competitive/Collaborative/etc.)\n",
    "\"\"\"\n",
    "\n",
    "def analyze_conversation(conversation: str):\n",
    "    input_chat = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": EVALUATION_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Here is the transcript\\n\" + conversation\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"gpt-4o\",\n",
    "        messages = input_chat,\n",
    "        store = False\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "\n",
    "# code to save the conversation\n",
    "def save_conversation(\n",
    "    filename: str,\n",
    "    conversation_history: list[dict]\n",
    ") -> str:\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    for record in debate_history:\n",
    "\n",
    "        if record[\"role\"] == \"user\":\n",
    "            messages.append(\"mediator:\\n\" + record[\"content\"])\n",
    "        \n",
    "        if record[\"role\"] == \"assistant\":\n",
    "            messages.append(f\"{record[\"name\"]}:\\n{record[\"content\"]}\")\n",
    "    \n",
    "    conversation_transcript = \"\\n\\n\".join(messages)\n",
    "    \n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(conversation_transcript)\n",
    "    \n",
    "    display(FileLink(filename))\n",
    "\n",
    "    return conversation_transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1038db1d-1ad3-4795-8554-aea2ff365579",
   "metadata": {},
   "source": [
    "### Experiment 1: Cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a76e70fa-add5-4487-b6a6-780fa1961177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:28<00:00,  3.59s/it]\n"
     ]
    }
   ],
   "source": [
    "NEW_SYSTEM_PROMPT = \"You a participant in a conversation between experienced software engineers. Keep questions minimal and only use them when necessary. Please greet the other participants when you join.\"\n",
    "interruption_prompt = lambda x: f\" You are interrupting speaker {x}.\" if x != -1 else \"\"\n",
    "\n",
    "def run_conversation(\n",
    "    iterations: int, \n",
    "    openai_model_id: str,\n",
    "    participant_count: int,\n",
    "    topic: str,\n",
    "    system_prompt: str,\n",
    "    dropout_chance: float,\n",
    "    cutoff_chance: float\n",
    ") -> list[dict]:\n",
    "    # model 1 is the first speaker\n",
    "    conversation_history = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt + \" The topic is: \" + topic}\n",
    "    ]\n",
    "\n",
    "    ordering = list(range(1, participant_count + 1))\n",
    "    last_speaker = -1\n",
    "    speaker_interrupted = -1\n",
    "\n",
    "    for i in tqdm(range(iterations)):\n",
    "        if i > 0:\n",
    "            first = choice(ordering[:-1])\n",
    "            remaining = [i for i in ordering if i != first]\n",
    "            shuffle(remaining)\n",
    "            ordering = [first] + remaining\n",
    "        for model in ordering: # RANDOM ORDERING\n",
    "            if random() < dropout_chance:\n",
    "                continue # SKIP\n",
    "            if last_speaker == model:\n",
    "                continue\n",
    "            speaker_id = f\"speaker_{model}\"\n",
    "            response = client.chat.completions.create(\n",
    "                model = openai_model_id,\n",
    "                messages = conversation_history + [\n",
    "                    {\n",
    "                        \"role\": \"user\", \n",
    "                        \"content\": f\"{speaker_id}, please share your perspective with the others and engage with the responses of the other participants.\" + interruption_prompt(speaker_interrupted)\n",
    "                    }\n",
    "                ],\n",
    "                store = False\n",
    "            );\n",
    "            speaker_interrupted = -1\n",
    "            message = response.choices[0].message.content\n",
    "\n",
    "            # cutoff message\n",
    "            if random() < cutoff_chance:\n",
    "                chunks = message.split(\" \")\n",
    "                message = \" \".join(chunks[:floor(len(chunks) * random())])\n",
    "                speaker_interrupted = model\n",
    "            \n",
    "            conversation_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message})\n",
    "            last_speaker = model\n",
    "\n",
    "    return conversation_history\n",
    "\n",
    "conversation = run_conversation(8, 'gpt-4o', 3, TOPIC, NEW_SYSTEM_PROMPT, 0.5, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f53399a-1638-4678-8e46-b4f3c2ce5c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='conversation_1.txt' target='_blank'>conversation_1.txt</a><br>"
      ],
      "text/plain": [
       "/Commjhub/jupyterhub/home/ezou626/comm4190_F25_Using_LLMs_Blog/posts/004_going_to_stop_you_right_there/conversation_1.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation_1 = save_conversation(\"conversation_1.txt\", conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdc8472-3d50-471f-970d-f958d257a701",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "We can now use AI to take a look at this conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a437bb8a-2eec-4cd7-8145-f4f6d106aaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **A Brief Summary**: The conversation focuses on the balance between treating code and infrastructure as the source of truth versus relying on extensive documentation in a development environment. All speakers advocate for leaning towards the code-centric approach, highlighting automated tests and infrastructure definitions. However, they also acknowledge the necessity of certain high-level documentation for onboarding and providing context, especially about architectural decisions. Mentorship and interactive documentation platforms are mentioned as effective onboarding strategies.\n",
      "\n",
      "- **Final Outputs/Artifacts/Takeaways**: The takeaways include the consensus on using code as the primary source of truth while maintaining some documentation for context. Key artifacts include the use of automated tools like Swagger, practices like Architecture Decision Records (ADRs), and platforms such as Confluence for maintaining \"living\" documentation. Also, integrating documentation updates into development workflows using PR processes and code reviews is crucial.\n",
      "\n",
      "- **Characteristics/Dynamic (Collaborative/Competitive/etc.)**: The conversation exhibits a collaborative dynamic. Each speaker builds on the others' points, agreeing on the importance of code while adding nuances about documentation and onboarding. They share practices and tips, ask questions to delve deeper into topics, and seek collective wisdom on optimizing processes, especially around onboarding in code-centric environments. There's a mutual respect for each participant's insights, fostering a constructive dialogue.\n"
     ]
    }
   ],
   "source": [
    "analyze_conversation(conversation_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3785bfa-01a6-42ba-b615-a34ccf137c33",
   "metadata": {},
   "source": [
    "I think the interruptions aren't helping so much. For example, let's look at this section in the conversation:\n",
    "> (speaker_3)\n",
    "> Speaker_1's point about onboarding emphasizes that while self-documenting code is ideal, some introductory material will always be necessary to bring new team members up to speed efficiently. It’s about > finding the right balance where both the code and key supporting documents align to provide clarity and understanding across the board. \n",
    "> How do others in the group\n",
    ">\n",
    "> speaker_1:\n",
    "> Thanks for the reminder! To jump into the responses—speaker_3, your point about using tools like Swagger for APIs is spot on. It helps keep documentation from going stale by being inherently tied to the codebase. I've also seen success with ReadMe documentation that's auto-generated from comments embedded within code, though it requires developers to maintain those comments diligently.\n",
    "\n",
    "Speaker 1 doesn't really acknowledge that they have cut someone off, despite the prompt indicating that this was the case. \n",
    "\n",
    "There are some cases where it is recognized though, and the speaker is able to continue off the other person's text.\n",
    "> speaker_1:\n",
    "Apologies for the interruption earlier. Thanks for catching that!\n",
    "\n",
    "In these cases, it definitely improves (from the subjective perspective of me as a reader) the spontaneity of the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f163543d-bc3e-4c40-95d5-7431c95060d6",
   "metadata": {},
   "source": [
    "### Experiment 2: Affirmations\n",
    "We can also see if affirmations does anything for the conversation. Our model of affirmations is a message from one model embedded in another model. We'll randomly select from a list of affirmations generated by ChatGPT (GPT5 on the chatgpt.com website as of 9/20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33ce70ab-f490-4662-a9d4-335c1f6d66fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "affirmations = [\n",
    "    \"Okay\",\n",
    "    \"Sure\",\n",
    "    \"Yep\",\n",
    "    \"Mm\",\n",
    "    \"Absolutely\",\n",
    "    \"Definitely\",\n",
    "    \"For sure\",\n",
    "    \"Sounds good\",\n",
    "    \"Of course\",\n",
    "    \"Right on\"\n",
    "]\n",
    "\n",
    "def run_conversation_interjections(\n",
    "    iterations: int, \n",
    "    openai_model_id: str,\n",
    "    participant_count: int,\n",
    "    topic: str,\n",
    "    system_prompt: str,\n",
    "    dropout_chance: float,\n",
    "    interjection_chance: float,\n",
    "    interjections: list[str]\n",
    ") -> list[dict]:\n",
    "    # model 1 is the first speaker\n",
    "    conversation_history = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt + \" The topic is: \" + topic}\n",
    "    ]\n",
    "\n",
    "    ordering = list(range(1, participant_count + 1))\n",
    "    last_speaker = -1\n",
    "    speaker_interrupted = -1\n",
    "\n",
    "    for i in tqdm(range(iterations)):\n",
    "        if i > 0:\n",
    "            first = choice(ordering[:-1])\n",
    "            remaining = [i for i in ordering if i != first]\n",
    "            shuffle(remaining)\n",
    "            ordering = [first] + remaining\n",
    "        for model in ordering: # RANDOM ORDERING\n",
    "            if random() < dropout_chance:\n",
    "                continue # SKIP\n",
    "            if last_speaker == model:\n",
    "                continue\n",
    "            speaker_id = f\"speaker_{model}\"\n",
    "            response = client.chat.completions.create(\n",
    "                model = openai_model_id,\n",
    "                messages = conversation_history + [\n",
    "                    {\n",
    "                        \"role\": \"user\", \n",
    "                        \"content\": f\"{speaker_id}, please share your perspective with the others and engage with the responses of the other participants.\"\n",
    "                    }\n",
    "                ],\n",
    "                store = False\n",
    "            );\n",
    "            message = response.choices[0].message.content\n",
    "\n",
    "            # affirm message\n",
    "            if random() < interjection_chance:\n",
    "                chunks = message.split(\" \")\n",
    "                interjection_index = floor(len(chunks) * random())\n",
    "                message_1 = \" \".join(chunks[:interjection_index])\n",
    "                message_2 = \" \".join(chunks[interjection_index:])\n",
    "                interjection = choice(interjections)\n",
    "                # randomly pick an interjector\n",
    "                interjection_model = f\"speaker_{choice([i for i in range(1, participant_count + 1) if i != model])}\"\n",
    "                if message_1:\n",
    "                    conversation_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message_1})\n",
    "                conversation_history.append({\"role\": \"assistant\", \"name\": interjection_model, \"content\": interjection})\n",
    "                if message_2:\n",
    "                    conversation_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message_2})\n",
    "            else:\n",
    "                conversation_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message})\n",
    "            \n",
    "            last_speaker = model\n",
    "\n",
    "    return conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa28c4b2-ce72-449b-b35f-a1aca6cce41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:44<00:00,  5.60s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='conversation_2.txt' target='_blank'>conversation_2.txt</a><br>"
      ],
      "text/plain": [
       "/Commjhub/jupyterhub/home/ezou626/comm4190_F25_Using_LLMs_Blog/posts/004_going_to_stop_you_right_there/conversation_2.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation = run_conversation_interjections(8, 'gpt-4o', 3, TOPIC, NEW_SYSTEM_PROMPT, 0.5, 0.2, affirmations)\n",
    "conversation_2 = save_conversation(\"conversation_2.txt\", conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d12af73-d1f8-483b-a426-94922960bf1e",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "Let's first generate an AI summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37c3c39c-e0b8-41e7-aa42-fa22f4099a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Brief Summary**: The conversation involves three speakers discussing the role of code, tests, infrastructure as code, and documentation in development workflows. They explore balancing code as the source of truth with the need for human-readable documentation, especially for onboarding and communication with non-technical stakeholders. They also discuss strategies to integrate documentation effectively into workflows and the possibility of using documentation sprints or hackathons.\n",
      "\n",
      "- **Final Outputs/Artifacts/Takeaways**: Key takeaways include the acknowledgment that while code and tests should be the primary sources of truth due to their inherent reliability, documentation plays a crucial role in explaining the rationale behind code, particularly for onboarding and broader communication. There is agreement on the importance of integrating documentation efforts into standard development practices using tools that automate document generation and conducting regular documentation reviews. Documentation sprints and mini-hackathons are proposed as strategies to keep documentation up-to-date and reduce documentation debt.\n",
      "\n",
      "- **Characteristics/Dynamic**: The conversation is collaborative, with all speakers agreeing on the fundamentals and building on each other's points. They collectively emphasize the integration of code and documentation into cohesive workflows and discuss practical solutions to potential challenges. Each speaker contributes to the discussion by expanding on strategies and sharing their experiences with documentation processes in development teams.\n"
     ]
    }
   ],
   "source": [
    "analyze_conversation(conversation_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f7bb35-c557-43b6-b203-fe11bbceaff8",
   "metadata": {},
   "source": [
    "It seems adding the interjections doesn't do much for the conversation itself, but it certainly \"looks\" more like a regular conversation now at a glance because these kinds of things are normal to see in a spoken setting. \n",
    "> One tactic I've found effective is peer reviews for documentation updates, much like code reviews. This ensures any significant changes in the codebase get reflected in the accompanying documentation with a second pair of eyes checking for\n",
    ">\n",
    ">speaker_3:\n",
    "Okay\n",
    ">\n",
    "> speaker_1:\n",
    "consistency. Implementing this as a standard part of our workflow has significantly improved the reliability and relevance of our documentation.\n",
    "\n",
    "I think one limitation of this approach is that since the interjections are basically completely random, it could be that the interjector is contradicting themselves by interjecting in a way that isn't consistent with their original position. In order to control this factor, we can use neutral interjections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec6d5fc-2287-4a10-8b27-10094b5d0ed5",
   "metadata": {},
   "source": [
    "### Experiment 3: Other Interruptions\n",
    "I had ChatGPT (GPT5 on 9/20) generate me a list of 10 neutral interjections that we can select from randomly. These should blend better with the average speaker position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca282a7d-c21d-4eb8-b434-c759682715d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "discussion_interjections = [\n",
    "    \"I see\",\n",
    "    \"Fair enough\",\n",
    "    \"Makes sense\",\n",
    "    \"Right\",\n",
    "    \"Got it\",\n",
    "    \"Noted\",\n",
    "    \"True\",\n",
    "    \"All right\",\n",
    "    \"That’s something\",\n",
    "    \"Point taken\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d703f56-d3a7-42cb-a370-23a848d867a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:45<00:00, 13.23s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='conversation_3.txt' target='_blank'>conversation_3.txt</a><br>"
      ],
      "text/plain": [
       "/Commjhub/jupyterhub/home/ezou626/comm4190_F25_Using_LLMs_Blog/posts/004_going_to_stop_you_right_there/conversation_3.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation = run_conversation_interjections(8, 'gpt-4o', 3, TOPIC, NEW_SYSTEM_PROMPT, 0.5, 0.2, discussion_interjections)\n",
    "conversation_3 = save_conversation(\"conversation_3.txt\", conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af9bc44-de4e-4502-931e-9e3bcd37a7cc",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "Let's see how this conversation fares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b16330d-9544-4ef6-a4f2-1edf33f074f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Brief Summary**: The conversation revolves around the balance between using code and tests as primary sources of truth in software projects versus maintaining documentation for broader understanding and context. The speakers agree on the importance of both, emphasizing integrated practices for documentation updates and the use of tools such as ADRs, JSDoc, and Sphinx for maintaining relevant documentation.\n",
      "\n",
      "- **Final Outputs/Artifacts/Takeaways**: \n",
      "  - Code and tests are considered the most accurate reflection of the system, given their evolving nature.\n",
      "  - Documentation should focus on context, rationale, and high-level architecture.\n",
      "  - Establishing documentation as part of the development workflow can help keep it relevant.\n",
      "  - ADRs are valuable for capturing architectural decisions.\n",
      "  - Prioritizing documentation based on complexity and impact is a common strategy.\n",
      "  - The use of structured documentation tools aids in seamless documentation upkeep.\n",
      "  - Regular checkpoints and reviews ensure documentation aligns with business changes.\n",
      "\n",
      "- **Characteristics/Dynamic**: The dynamic of the conversation is collaborative, with participants building on each other's points, sharing experiences, and collectively exploring solutions. There is mutual agreement on key practices like integrating documentation into workflows and the use of certain tools and strategies, highlighting a shared understanding and team-based learning culture.\n"
     ]
    }
   ],
   "source": [
    "analyze_conversation(conversation_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f92e448-461e-41f2-bd30-553862f7359d",
   "metadata": {},
   "source": [
    "I think the interjections are sometimes a little misplaced, even considering that these would be simultaneous in a real conversation with the other person's words. \n",
    "\n",
    "> (speaker_2)\n",
    "> How do you all handle the challenge of ensuring that documentation remains accessible and easily digestible, especially when integrated closely with the code? Do you find structured documentation tools\n",
    ">\n",
    "> speaker_3:\n",
    "Point taken\n",
    ">\n",
    "> speaker_2:\n",
    "helpful, or do you rely more on traditional documentation practices?\n",
    "\n",
    "I think some of the issues with the affirmation approach are still present. I think a next step in a future blog may be to take a look at how we can place these interjections a little more smartly, given what the speaker themselves would know about the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14678deb-0bc1-4ea9-8e60-f70b1c9cdfbf",
   "metadata": {},
   "source": [
    "### Experiment 4: Combining Cutoffs and Interruptions\n",
    "The last thing we'll take a look at is the combination of both of these approaches. For every message, there is a chance it will be cut off or interjected, or both. I don't expect anything super surprising from this conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "224edbff-2d37-4bd9-be1f-ef0ab391e32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_affirmations = affirmations + discussion_interjections\n",
    "\n",
    "def run_conversation_interrupt_interject(\n",
    "    iterations: int, \n",
    "    openai_model_id: str,\n",
    "    participant_count: int,\n",
    "    topic: str,\n",
    "    system_prompt: str,\n",
    "    dropout_chance: float,\n",
    "    cutoff_chance: float,\n",
    "    interjection_chance: float,\n",
    "    both_chance: float,\n",
    "    interjections: list[str]\n",
    ") -> list[dict]:\n",
    "    # model 1 is the first speaker\n",
    "    conversation_history = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt + \" The topic is: \" + topic}\n",
    "    ]\n",
    "\n",
    "    ordering = list(range(1, participant_count + 1))\n",
    "    last_speaker = -1\n",
    "    speaker_interrupted = -1\n",
    "\n",
    "    for i in tqdm(range(iterations)):\n",
    "        if i > 0:\n",
    "            first = choice(ordering[:-1])\n",
    "            remaining = [i for i in ordering if i != first]\n",
    "            shuffle(remaining)\n",
    "            ordering = [first] + remaining\n",
    "        for model in ordering: # RANDOM ORDERING\n",
    "            if random() < dropout_chance:\n",
    "                continue # SKIP\n",
    "            if last_speaker == model:\n",
    "                continue\n",
    "            speaker_id = f\"speaker_{model}\"\n",
    "            response = client.chat.completions.create(\n",
    "                model = openai_model_id,\n",
    "                messages = conversation_history + [\n",
    "                    {\n",
    "                        \"role\": \"user\", \n",
    "                        \"content\": f\"{speaker_id}, please share your perspective with the others and engage with the responses of the other participants.\" + interruption_prompt(speaker_interrupted)\n",
    "                    }\n",
    "                ],\n",
    "                store = False\n",
    "            );\n",
    "            message = response.choices[0].message.content\n",
    "            speaker_interrupted = -1\n",
    "\n",
    "            # affirm message\n",
    "            value = random()\n",
    "            if value < interjection_chance:\n",
    "                chunks = message.split(\" \")\n",
    "                interjection_index = floor(len(chunks) * random())\n",
    "                message_1 = \" \".join(chunks[:interjection_index])\n",
    "                message_2 = \" \".join(chunks[interjection_index:])\n",
    "                interjection = choice(interjections)\n",
    "                # randomly pick an interjector\n",
    "                interjection_model = f\"speaker_{choice([i for i in range(1, participant_count + 1) if i != model])}\"\n",
    "                if message_1:\n",
    "                    conversation_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message_1})\n",
    "                conversation_history.append({\"role\": \"assistant\", \"name\": interjection_model, \"content\": interjection})\n",
    "                if message_2:\n",
    "                    conversation_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message_2})\n",
    "            elif value < interjection_chance + cutoff_chance:\n",
    "                chunks = message.split(\" \")\n",
    "                message = \" \".join(chunks[:floor(len(chunks) * random())])\n",
    "                conversation_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message})\n",
    "                speaker_interrupted = model\n",
    "            elif value < interjection_chance + cutoff_chance + both_chance:\n",
    "                chunks = message.split(\" \")\n",
    "                interjection_index = floor(len(chunks) * random())\n",
    "                message_1 = \" \".join(chunks[:interjection_index])\n",
    "                interjection = choice(interjections)\n",
    "                # randomly pick an interjector\n",
    "                interjection_model = f\"speaker_{choice([i for i in range(1, participant_count + 1) if i != model])}\"\n",
    "                if message_1:\n",
    "                    conversation_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message_1})\n",
    "                conversation_history.append({\"role\": \"assistant\", \"name\": interjection_model, \"content\": interjection})\n",
    "                speaker_interrupted = model\n",
    "            else:\n",
    "                conversation_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message})\n",
    "            \n",
    "            last_speaker = model\n",
    "\n",
    "    return conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92f64543-d5e3-4167-9fa0-ff5a92c19bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:40<00:00,  5.10s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='conversation_4.txt' target='_blank'>conversation_4.txt</a><br>"
      ],
      "text/plain": [
       "/Commjhub/jupyterhub/home/ezou626/comm4190_F25_Using_LLMs_Blog/posts/004_going_to_stop_you_right_there/conversation_4.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation = run_conversation_interrupt_interject(8, 'gpt-4o', 3, TOPIC, NEW_SYSTEM_PROMPT, 0.5, 0.2, 0.2, 0.1, discussion_interjections)\n",
    "conversation_4 = save_conversation(\"conversation_4.txt\", conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a8ca5c-f37d-4d1c-bb08-0dbc46364c58",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "I'll skip the AI analysis for this one. It feels like more of the same which we've been seeing before. I think for word-granularity interruptions from randomness, this might be close the limit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9661b14-c631-4662-9dfb-d180c674b6d8",
   "metadata": {},
   "source": [
    "## Closing Remarks\n",
    "In general, while these seem to make the conversation appear more natural from a bird's eye view, there are evidently issues with the approach. \n",
    "\n",
    "First, because LLM's operate with text, the introduction of interjections in the way done here doesn't really model the way an interjection appears in real life conversations (separation vs. mixing, in that order). This could cause interjections to not be as effective in their intended purpose, or worse, interrupt the conversation flow. A potential remedy to this would be to reframe the problem as looking at an online chatroom and only allowing interjections at punctuation marks like periods or paragraph breaks. In that way, it's much more like a chat conversation where messages are delivered in whole pieces without chance of interruption in the middle.\n",
    "\n",
    "Second, I think there's also an opportunity to develop speaker personas to enforce consistent actions across multiple messages. When speakers are prompted, there is a chance they may not remember what they are trying to work on, and there's no way in the current simulation environment to differentiate one speaker from the other except in the order in which they speak. Maintaining a consistent state for each speaker could go a long way in ensuring better defined speakers, and as a result, more diverse conversations. Additionally, we may be able to use these personas to inform actions like interruptions or cutoffs.\n",
    "\n",
    "> **Future Work:**\n",
    "> - There's definitely an opportunity to develop speaker-level context that can be delivered in the message generation prompt. This would allow a more rich representation of the speaker's internal thoughts, and this could even be managed by an LLM.\n",
    "> - There's also an opportunity to make some of these random things motivated by this context or the conversation, asking the AI if it wants to do a specific action given it's current position.\n",
    "> - It might be cool to have a general framework that considers all of these possible scenarios and allows for the simulation of these kinds of conversations with different models, parameters, etc. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
