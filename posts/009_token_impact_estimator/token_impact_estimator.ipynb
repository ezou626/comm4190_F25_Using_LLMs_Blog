{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Counting Tokens, Counting Impact\"\n",
        "description: \"A tiny token-usage decorator for LLM calls\"\n",
        "author: \"Eric Zou\"\n",
        "date: \"11/27/2025\"\n",
        "categories:\n",
        "  - LLMs\n",
        "  - Tooling\n",
        "  - Experiments\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why I Care About Tokens At All\n",
        "\n",
        "Up to this point, I've mostly treated API calls as opaque little boxes: I send some text in, I get some text out, and I vaguely know I'm spending \"tokens\" in the process. That's fine for toy experiments, but once these conversations get longer and more complicated, it becomes a lot less obvious what I'm actually burning through.\n",
        "\n",
        "In this post, I want to treat token usage as a first-class signal instead of an afterthought. Rather than hard-coding a particular notion of \"cost\" (dollars, latency, carbon, whatever), I'll build a small wrapper that:\n",
        "\n",
        "- keeps track of **how many tokens** a function is using (split by type), and\n",
        "- lets me plug in my own **impact calculus** that turns those raw counts into something I care about.\n",
        "\n",
        "The end result is a Python decorator I can drop onto any function that makes LLM calls, with a long-lived object quietly accumulating stats in the background while I run my experiments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Callable, Dict, Mapping\n",
        "\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load API key from the project root .env\n",
        "_ = load_dotenv(\".env\")\n",
        "client = OpenAI()\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TokenUsageSnapshot:\n",
        "    \"\"\"A simple view of one LLM call's token usage.\n",
        "\n",
        "    The OpenAI client exposes more detailed usage objects, but for this blog\n",
        "    I'm mainly interested in the usual trio: prompt, completion, total.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt_tokens: int = 0\n",
        "    completion_tokens: int = 0\n",
        "    total_tokens: int = 0\n",
        "\n",
        "    @classmethod\n",
        "    def from_raw(cls, raw: Mapping[str, Any]) -> \"TokenUsageSnapshot\":\n",
        "        return cls(\n",
        "            prompt_tokens=int(raw.get(\"prompt_tokens\", 0) or 0),\n",
        "            completion_tokens=int(raw.get(\"completion_tokens\", 0) or 0),\n",
        "            total_tokens=int(raw.get(\"total_tokens\", 0) or 0),\n",
        "        )\n",
        "\n",
        "    def to_dict(self) -> Dict[str, int]:\n",
        "        return {\n",
        "            \"prompt_tokens\": self.prompt_tokens,\n",
        "            \"completion_tokens\": self.completion_tokens,\n",
        "            \"total_tokens\": self.total_tokens,\n",
        "        }\n",
        "\n",
        "\n",
        "ImpactFn = Callable[[Dict[str, int]], Dict[str, int]]\n",
        "\n",
        "\n",
        "class TokenImpactTracker:\n",
        "    \"\"\"Wraps a decorator that tracks token usage across many LLM calls.\n",
        "\n",
        "    - Every decorated function call tries to read a `.usage` field on the\n",
        "      returned OpenAI response (or a `{\"usage\": ...}` mapping).\n",
        "    - The raw token counts are accumulated over the lifetime of this object.\n",
        "    - An `impact_fn` converts a single-call usage dict into keyâ†’int scores\n",
        "      (my personal \"impact calculus\"), which are also accumulated.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, impact_fn: ImpactFn):\n",
        "        self.impact_fn: ImpactFn = impact_fn\n",
        "        # Raw counts straight from the API, aggregated over time\n",
        "        self.raw_totals: Dict[str, int] = defaultdict(int)\n",
        "        # Impact-space totals, defined by whatever function I care about\n",
        "        self.impact_totals: Dict[str, int] = defaultdict(int)\n",
        "\n",
        "    # --- internals -----------------------------------------------------\n",
        "\n",
        "    def _extract_usage_dict(self, result: Any) -> Dict[str, int]:\n",
        "        \"\"\"Best-effort extraction of a usage dict from an OpenAI response.\n",
        "\n",
        "        This handles the current OpenAI Python client objects as well as\n",
        "        plain dicts. If no usage information is available, we just return\n",
        "        an empty dict and quietly skip accounting for that call.\n",
        "        \"\"\"\n",
        "\n",
        "        usage_obj: Any = getattr(result, \"usage\", None)\n",
        "\n",
        "        if usage_obj is None and isinstance(result, Mapping):\n",
        "            usage_obj = result.get(\"usage\")\n",
        "\n",
        "        if usage_obj is None:\n",
        "            return {}\n",
        "\n",
        "        # New-style OpenAI objects are pydantic-like; try a few options.\n",
        "        if hasattr(usage_obj, \"to_dict\"):\n",
        "            raw = usage_obj.to_dict()\n",
        "        elif hasattr(usage_obj, \"model_dump\"):\n",
        "            raw = usage_obj.model_dump()\n",
        "        elif isinstance(usage_obj, Mapping):\n",
        "            raw = dict(usage_obj)\n",
        "        else:\n",
        "            # Last resort: nothing we understand\n",
        "            return {}\n",
        "\n",
        "        snapshot = TokenUsageSnapshot.from_raw(raw)\n",
        "        return snapshot.to_dict()\n",
        "\n",
        "    def _record_usage(self, usage: Dict[str, int]) -> None:\n",
        "        for key, value in usage.items():\n",
        "            self.raw_totals[key] += int(value)\n",
        "\n",
        "        impact = self.impact_fn(usage)\n",
        "        for key, value in impact.items():\n",
        "            self.impact_totals[key] += int(value)\n",
        "\n",
        "    # --- the decorator interface --------------------------------------\n",
        "\n",
        "    def decorator(self, fn: Callable[..., Any]) -> Callable[..., Any]:\n",
        "        \"\"\"Turn this tracker into a decorator for an LLM-calling function.\"\"\"\n",
        "\n",
        "        def wrapped(*args: Any, **kwargs: Any) -> Any:\n",
        "            result = fn(*args, **kwargs)\n",
        "            usage = self._extract_usage_dict(result)\n",
        "            if usage:\n",
        "                self._record_usage(usage)\n",
        "            return result\n",
        "\n",
        "        # Be a polite decorator and preserve the original function name\n",
        "        wrapped.__name__ = fn.__name__\n",
        "        wrapped.__doc__ = fn.__doc__\n",
        "        return wrapped\n",
        "\n",
        "    # Convenience helpers for inspection in the notebook\n",
        "    def as_dict(self) -> Dict[str, Dict[str, int]]:\n",
        "        return {\n",
        "            \"raw_totals\": dict(self.raw_totals),\n",
        "            \"impact_totals\": dict(self.impact_totals),\n",
        "        }\n",
        "\n",
        "\n",
        "def example_pricing_impact(usage: Dict[str, int]) -> Dict[str, int]:\n",
        "    \"\"\"A tiny, opinionated impact function.\n",
        "\n",
        "    For this assignment I'm not going to pull live pricing tables; instead,\n",
        "    I'll hard-code a rough toy pricing model for `gpt-4o-mini` in terms of\n",
        "    **micro-dollars** (1e-6 USD) so everything stays integer-based:\n",
        "\n",
        "    - prompt tokens: 0.15 USD / 1M tokens\n",
        "    - completion tokens: 0.60 USD / 1M tokens\n",
        "\n",
        "    That works out to:\n",
        "      0.15 / 1_000_000 = 1.5e-7 USD/token\n",
        "      0.60 / 1_000_000 = 6e-7 USD/token\n",
        "\n",
        "    We store everything as integers so downstream code doesn't have to think\n",
        "    about floats.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = usage.get(\"prompt_tokens\", 0)\n",
        "    completion = usage.get(\"completion_tokens\", 0)\n",
        "    total = usage.get(\"total_tokens\", prompt + completion)\n",
        "\n",
        "    # micro-dollars per token\n",
        "    prompt_per_token = 15  # 1.5e-7 USD == 15 micro-dollars\n",
        "    completion_per_token = 60  # 6e-7 USD == 60 micro-dollars\n",
        "\n",
        "    micro_usd = prompt * prompt_per_token + completion * completion_per_token\n",
        "\n",
        "    return {\n",
        "        \"prompt_tokens\": prompt,\n",
        "        \"completion_tokens\": completion,\n",
        "        \"total_tokens\": total,\n",
        "        \"micro_usd\": micro_usd,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the context of large language models (LLMs), a token is a unit of text, which can be a word, part of a word, or even a punctuation mark, used by the model to process and generate language. Tokens serve as the basic building blocks for understanding and producing text, with each token representing a specific piece of information.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'raw_totals': {'prompt_tokens': 38,\n",
              "  'completion_tokens': 69,\n",
              "  'total_tokens': 107},\n",
              " 'impact_totals': {'prompt_tokens': 38,\n",
              "  'completion_tokens': 69,\n",
              "  'total_tokens': 107,\n",
              "  'micro_usd': 4710}}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## A Tiny Decorated Helper Around `client.chat.completions.create`\n",
        "\n",
        "tracker = TokenImpactTracker(impact_fn=example_pricing_impact)\n",
        "\n",
        "\n",
        "@tracker.decorator\n",
        "def call_gpt_4o_mini(prompt: str):\n",
        "    \"\"\"Single-turn helper so I have something concrete to decorate.\n",
        "\n",
        "    In a \"real\" system this would probably be buried a few layers down in an\n",
        "    agent loop or conversation runner. Here I just want something that:\n",
        "\n",
        "    - calls the OpenAI Chat Completions API,\n",
        "    - returns the raw response object so we can introspect `.usage`, and\n",
        "    - plays nicely with the notebook.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a friendly, concise assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        store=False,\n",
        "    )\n",
        "    return response\n",
        "\n",
        "\n",
        "# quick smoke test (you can re-run this cell a few times to accumulate stats)\n",
        "example_response = call_gpt_4o_mini(\"In one or two sentences, explain what a token is in the context of LLMs.\")\n",
        "print(example_response.choices[0].message.content)\n",
        "\n",
        "tracker.as_dict()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Playing With Impact Calculi\n",
        "\n",
        "The nice thing about this setup is that I never hard-code what \"impact\" means.\n",
        "All the tracker ever sees is a mapping from strings to integers. Today, I'm\n",
        "using micro-dollars and raw token counts, but a future experiment could plug in\n",
        "something very different:\n",
        "\n",
        "- **budgeting**: keep separate counters for prompt vs. completion tokens and\n",
        "  alert when one crosses a threshold,\n",
        "- **carbon**: approximate energy usage per 1K tokens and track an emissions\n",
        "  budget alongside dollars, or\n",
        "- **fairness**: tag calls by scenario/user and see which groups are soaking up\n",
        "  the most capacity.\n",
        "\n",
        "For now, I'm happy that I can decorate a plain old function and get a running\n",
        "summary of how \"expensive\" my experiments are without changing the surrounding\n",
        "code. It feels like a good building block for the messier multi-agent setups\n",
        "I've been playing with in the earlier posts.\n",
        "\n",
        "> **Future Work:**\n",
        "> - Try plugging this into one of the multi-speaker simulations and track\n",
        ">   impact per speaker/persona.\n",
        "> - Add simple budget guards that short-circuit a function once a quota is hit.\n",
        "> - Layer in richer token-type distinctions (cached vs. uncached, tools, etc.)\n",
        ">   as the API surface evolves.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
