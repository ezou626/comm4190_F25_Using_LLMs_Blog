{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d248efdd-1593-49a8-b642-4e65b1f59259",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Another LLM Conversation\"\n",
    "description: \"Analyzing a conversation between two LLMs in a shared chat window\" \n",
    "author: \"Eric Zou\"\n",
    "date: \"9/10/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Conversations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3900243e-14d2-4843-bf88-2dd0bcac6042",
   "metadata": {},
   "source": [
    "Last time, we looked at LLMs conversing with two separate chat windows. This time, I wonder if we can do it with one. OpenAI provides a \"name\" field in the input messages which we can use to identify model 1 versus model 2. In this case, we tell one model to simulate the debate, with the user acting as the mediator. The interesting thing about this is that we can scale this much better. With the user as the orchestrator, we can actually add as many models/positions to the debate as we want. We can also fall away from the debate structure and come up with collaborative scenarios as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1a5b9bc-898e-4233-a860-253dfca1b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import base64\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from IPython.display import FileLink, display\n",
    "from dotenv import load_dotenv\n",
    "# Load API key\n",
    "_ = load_dotenv(\"../../../comm4190_F25/01_Introduction_and_setup/.env\")\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb26f1-be98-4af9-9142-ef24ab025c56",
   "metadata": {},
   "source": [
    "## LLM Debates\n",
    "Let's try LLM debates first as a continuation of our prior work. We're going to use the same topic as [last time](https://ezou626.github.io/comm4190_F25_Using_LLMs_Blog/posts/001_an_llm_conversation/an_llm_conversation.html).\n",
    "\n",
    "We'll be a bit rigid for now. We're going to use the same model for both, an incrementing speaker naming system, and a round robin speaking format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "842de4da-3f50-4ebc-a302-136ef24fe102",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBATE_TOPIC = \"\"\"Code, testing, and dev infra should be prioritized over comprehensive documentation.\"\"\"\n",
    "\n",
    "# prompt to analyze conversations\n",
    "EVALUATION_PROMPT = \"\"\"\n",
    "Your objective is to analyze this conversation between speakers.\n",
    "Your response should follow this organization:\n",
    "- A Brief Summary\n",
    "- Final Outputs/Artifacts/Takeaways\n",
    "- Characteristics/Dynamic (Competitive/Collaborative/etc.)\n",
    "\"\"\"\n",
    "\n",
    "def analyze_conversation(conversation: str):\n",
    "    input_chat = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": EVALUATION_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Here is the transcript\\n\" + conversation\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"gpt-4o\",\n",
    "        messages = input_chat,\n",
    "        store = False\n",
    "    )\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "839db79d-552e-499e-a293-e03f2e74ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"You are simulating a debate between AI agents. Each agent should respond in turn, logically arguing their point. Do not speak for both sides in one message.\"\n",
    "\n",
    "# code to simulate the conversation\n",
    "def run_conversation(\n",
    "    iterations: int, \n",
    "    openai_model_id: str,\n",
    "    participant_count: int,\n",
    "    debate_topic: str\n",
    ") -> list[dict]:\n",
    "    # model 1 is the first speaker\n",
    "    debate_history = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": \"The debate topic is: \" + debate_topic}\n",
    "    ]\n",
    "\n",
    "    for _ in tqdm(range(iterations)):\n",
    "        for model in range(1, participant_count + 1):\n",
    "            speaker_id = f\"speaker_{model}\"\n",
    "            debate_history.append({\"role\": \"user\", \"content\": f\"{speaker_id}, it's your turn to speak.\"})\n",
    "            response = client.chat.completions.create(\n",
    "                model = openai_model_id,\n",
    "                messages = debate_history,\n",
    "                store = False\n",
    "            );\n",
    "            message = response.choices[0].message.content\n",
    "            debate_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message})\n",
    "\n",
    "    return debate_history\n",
    "\n",
    "# code to save the conversation\n",
    "def save_conversation(\n",
    "    filename: str,\n",
    "    debate_history: list[dict]\n",
    ") -> str:\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    for record in debate_history:\n",
    "\n",
    "        if record[\"role\"] == \"user\":\n",
    "            messages.append(\"mediator:\\n\" + record[\"content\"])\n",
    "        \n",
    "        if record[\"role\"] == \"assistant\":\n",
    "            messages.append(f\"{record[\"name\"]}:\\n{record[\"content\"]}\")\n",
    "    \n",
    "    conversation_transcript = \"\\n\\n\".join(messages)\n",
    "    \n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(conversation_transcript)\n",
    "    \n",
    "    display(FileLink(filename))\n",
    "\n",
    "    return conversation_transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef309b84-da53-4754-bac7-65ad59c87786",
   "metadata": {},
   "source": [
    "### Experiment 1:\n",
    "Here's our first experiment configuration:\n",
    "- 2 speakers\n",
    "- Explicit mediator: Include mediator messages in the chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd8f6f5e-5e5f-4ebe-a2d4-faac22007680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:33<00:00, 16.92s/it]\n"
     ]
    }
   ],
   "source": [
    "debate_1 = run_conversation(2, 'gpt-4o', 2, DEBATE_TOPIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5cbd8a3-b2ca-489b-9c43-d264d44f65ad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='conversation_transcript_1.txt' target='_blank'>conversation_transcript_1.txt</a><br>"
      ],
      "text/plain": [
       "/Commjhub/jupyterhub/home/ezou626/comm4190_F25_Using_LLMs_Blog/posts/002_another_llm_conversation/conversation_transcript_1.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation_transcript_1 = save_conversation(\"conversation_transcript_1.txt\", debate_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2dd78e-cdfc-452f-8a74-7e0e41084a76",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Like last time, we're going to analyze the conversation with AI as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67c79944-f7c9-486d-82d4-fe43100a200d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Brief Summary**: This debate centers on whether code quality, testing, and development infrastructure should take priority over comprehensive documentation. Speaker 1 argues that focusing on code and testing leads to more robust products and allows for quicker adaptations in the fast-paced tech world. Speaker 2 counters that documentation is crucial for knowledge transfer, collaboration, and regulatory compliance, emphasizing its role in preserving the long-term vision and understanding of software projects.\n",
      "\n",
      "- **Final Outputs/Artifacts/Takeaways**: Speaker 1 stresses the importance of sustainable code quality and testing as a form of self-documentation and for enabling continuous improvement. On the other hand, Speaker 2 highlights the necessity of documentation for maintaining architectural understanding and ensuring compliance with industry standards, as well as for effective stakeholder communication.\n",
      "\n",
      "- **Characteristics/Dynamic**: The conversation is collaborative as both speakers present structured arguments recognizing the value of the opposing views while reinforcing their own. Each speaker builds upon or refutes the other's points in a respectful and logical manner, making a case for the integration of both perspectives rather than outright exclusion of one approach over the other.\n"
     ]
    }
   ],
   "source": [
    "analyze_conversation(conversation_transcript_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c953ad4f-5785-4f79-99ee-f1bb192ac144",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "There's some noticable changes from last time. The participant's positions are noticably more rigid and as a result, the final outcome is less convergent. A side effect of this is that there is no natural closing spot. I think this is probably due to our significantly less cooperative system prompt. Last time, we explicitly asked for the model's to aim for a common ground. We would probably see the same thing if we were to add that to our system prompt. We could change the prompt for the next experiment to include that so we can gradually move to some concrete artifacts. However, I believe a thorough evaluation of the different kinds of prompts we can use here is probably best saved for another blog systematically evaluating various system prompts and the effect of various phrases on the model's final output. It would also be interesting to use something like [ConvoKit](https://convokit.cornell.edu/) to perform these kinds of analyses, so we can save that for a future blog.\n",
    "\n",
    "> **Future Work:**\n",
    "> - Use ConvoKit to systematically evaluate conversational features in a quantitative way based on various prompting and chat structuring strategies.\n",
    "\n",
    "For now, I'm interested in seeing if a third model can bring new things to the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a19fd1b-4655-4916-9125-2235f3463342",
   "metadata": {},
   "source": [
    "### Experiment 2\n",
    "Let's try this with 3 models instead of 2. I'm wondering what a third model will do in a seemingly binary scenario. Will it support one of the positions, or will it look to take a middle ground? What if it comes up with a third position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f26468e1-0fa7-4e2e-a74a-6f5d10869b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:34<00:00, 17.31s/it]\n"
     ]
    }
   ],
   "source": [
    "debate_2 = run_conversation(2, 'gpt-4o', 3, DEBATE_TOPIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f37c7523-a390-4b72-b878-e5def6221b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='conversation_transcript_2.txt' target='_blank'>conversation_transcript_2.txt</a><br>"
      ],
      "text/plain": [
       "/Commjhub/jupyterhub/home/ezou626/comm4190_F25_Using_LLMs_Blog/posts/002_another_llm_conversation/conversation_transcript_2.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation_transcript_2 = save_conversation(\"conversation_transcript_2.txt\",debate_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed792c6-2556-4504-8893-b09c94293bd3",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Let's see what GPT-4o has to say about this debate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92417e63-b5f8-4350-9330-2aaf9d4c6699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Brief Summary:** The debate centers around whether code, testing, and development infrastructure should be prioritized over comprehensive documentation. Speaker 1 argues for prioritizing code and infrastructure, emphasizing agility and efficiency, particularly in initial project phases. Speaker 2 champions the importance of documentation for usability, maintainability, and long-term benefits. Speaker 3 presents a middle ground, suggesting integrated processes that balance both priorities as complementary elements.\n",
      "\n",
      "- **Final Outputs/Artifacts/Takeaways:** The conversation ends with speaker 3 advocating for an integrated approach where tools and methodologies make documentation a part of the development process. This includes using automated tools for inline documentation and ensuring documentation is part of the project’s definition of \"done.\" It suggests both speed in delivery and sustainable practices can coexist without sacrificing one for the other.\n",
      "\n",
      "- **Characteristics/Dynamic:** The discussion is collaborative, focusing on finding a balanced approach rather than intensifying opposing views. While there are differing opinions, particularly between speakers 1 and 2, the mediator’s role and speaker 3's perspective facilitate an integration of ideas, aiming for a synthesis that acknowledges the value in both sides of the argument.\n"
     ]
    }
   ],
   "source": [
    "analyze_conversation(conversation_transcript_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac76f3-53ce-446f-8436-fdc497892b1e",
   "metadata": {},
   "source": [
    "Despite not changing the prompt at all, it seems that speaker 3 naturally took a middle path between documentation versus using the codebase as the primary source of truth in software projects. Perhaps speaker 3 taking the middle ground could result in a better synthesis of new ideas by naturally bridging the gap in a 2-way debate. In this scenario, I'm now really interested to see what would happen if we had 4 speakers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ad468-3201-4c77-95a3-5c0068a030c5",
   "metadata": {},
   "source": [
    "### Experiment 3\n",
    "Now we're going to try two rounds of communication where we have 4 speakers. Pretty straightforward. I wonder if speaker 4 may agree with speaker 3, since I expect the start of the conversation to go about the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45b46a75-4fcd-4a9d-b577-3f95a35f0801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:02<00:00, 31.17s/it]\n"
     ]
    }
   ],
   "source": [
    "debate_3 = run_conversation(2, 'gpt-4o', 4, DEBATE_TOPIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91897f65-9f8d-4a4a-92b7-891c2356d152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='conversation_transcript_3.txt' target='_blank'>conversation_transcript_3.txt</a><br>"
      ],
      "text/plain": [
       "/Commjhub/jupyterhub/home/ezou626/comm4190_F25_Using_LLMs_Blog/posts/002_another_llm_conversation/conversation_transcript_3.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation_transcript_3 = save_conversation(\"conversation_transcript_3.txt\", debate_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b5df4-3eb0-44ae-bf04-d562cc32f4d6",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Now, let's get a quick summary of what happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "925d632f-9cf5-4a4b-b817-80b0f6972730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Brief Summary:**\n",
      "  The conversation revolves around the debate on whether code, testing, and development infrastructure should be prioritized over comprehensive documentation in software development. Speaker_1 advocates for prioritizing core code and testing, especially in early project stages, while speaker_2 emphasizes the necessity of documentation for continuity and avoiding knowledge silos. Speaker_3 introduces a balanced approach depending on project context and phase, and speaker_4 underscores the integration of documentation with development for synergy and resilience.\n",
      "\n",
      "- **Final Outputs/Artifacts/Takeaways:**\n",
      "  - Recognition of the importance of a balanced approach between prioritizing code, testing, and infrastructure and maintaining comprehensive documentation.\n",
      "  - Understanding that the project's phase and specific needs should dictate the balance between these priorities.\n",
      "  - Emphasis on tools and methodologies that integrate documentation into the development process, like documentation-as-code.\n",
      "  - Advocacy for agile and iterative documentation alongside development to maintain project velocity and knowledge retention.\n",
      "\n",
      "- **Characteristics/Dynamic:**\n",
      "  The dynamic is **collaborative**, even though there's an inherent tension in prioritizing different project facets. The speakers build on each other's points, aiming for a cohesive strategy that respects both swift development and comprehensive documentation. They support a nuanced, context-sensitive approach rather than a one-size-fits-all method, indicating a shared goal of sustainable and efficient software development.\n"
     ]
    }
   ],
   "source": [
    "analyze_conversation(conversation_transcript_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc32825e-285a-4206-8295-cd41ba95cd16",
   "metadata": {},
   "source": [
    "It seems that speaker 4 also takes a middle ground, but is a bit biased towards the second position of integrating documentation. I think we might get diminishing returns with adding more speakers in the current rigid structure without some more complicated instructions, so we could just use 3 for now. I think ideally, we'd like to move to self-organizing conversations that aren't necessarily debates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1828dd-8707-43e9-a11e-f31a36703356",
   "metadata": {},
   "source": [
    "### Experiment 4\n",
    "I think one final interesting thing we could do is remove the mediator from the chat history, since they don't really provide anything. We can just inject the instructional message at the end of the chat history right before we call the API to direct the next speaker, but we don't need to persist it. The transcript technically won't be complete, but maybe we can see some differences in the formality of the responses if the mediator is not considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce74631c-0642-4d6c-bffd-08fade1a1289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to simulate the conversation\n",
    "def run_organic_conversation(\n",
    "    iterations: int, \n",
    "    openai_model_id: str,\n",
    "    participant_count: int,\n",
    "    debate_topic: str\n",
    ") -> list[dict]:\n",
    "    # model 1 is the first speaker\n",
    "    debate_history = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": \"The debate topic is: \" + debate_topic}\n",
    "    ]\n",
    "\n",
    "    for _ in tqdm(range(iterations)):\n",
    "        for model in range(1, participant_count + 1):\n",
    "            speaker_id = f\"speaker_{model}\"\n",
    "            response = client.chat.completions.create(\n",
    "                model = openai_model_id,\n",
    "                messages = debate_history + [{\"role\": \"user\", \"content\": f\"{speaker_id}, it's your turn to speak.\"}],\n",
    "                store = False\n",
    "            );\n",
    "            message = response.choices[0].message.content\n",
    "            debate_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message})\n",
    "\n",
    "    return debate_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4236d0f0-17ae-481b-b84b-18dc180164d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:28<00:00, 14.20s/it]\n"
     ]
    }
   ],
   "source": [
    "debate_4 = run_organic_conversation(2, 'gpt-4o', 3, DEBATE_TOPIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53ba120c-ddb4-4aad-a214-56167fd57a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='conversation_transcript_4.txt' target='_blank'>conversation_transcript_4.txt</a><br>"
      ],
      "text/plain": [
       "/Commjhub/jupyterhub/home/ezou626/comm4190_F25_Using_LLMs_Blog/posts/002_another_llm_conversation/conversation_transcript_4.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation_transcript_4 = save_conversation(\"conversation_transcript_4.txt\", debate_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c50476-141c-4539-9a88-eba36c1351a0",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Let's see if that had any impact on the debate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7cf81b87-dcf8-419a-8629-9af95f5c7b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Brief Summary:** The discussion revolves around the prioritization of code, testing, and development infrastructure versus comprehensive documentation in software projects. Speaker 1 advocates for prioritizing code and testing to ensure product reliability and rapid market delivery. Speaker 2 argues for the importance of comprehensive documentation to aid in knowledge transfer, maintainability, and regulatory compliance. Speaker 3 suggests a balanced approach, tailored to project requirements, integrating documentation into development practices through tools and automation.\n",
      "\n",
      "- **Final Outputs/Artifacts/Takeaways:** The conversation concludes with a consensus on the necessity of balance and context-specific approaches. Key artifacts to consider include the integration of automated documentation tools, ensuring documentation evolves with the codebase, and prioritizing based on project needs and industry regulations. Additionally, a continuous review process is recommended to align with evolving project goals.\n",
      "\n",
      "- **Characteristics/Dynamic:** The dynamic of the conversation is collaborative and constructive, with each speaker acknowledging the validity of the other's perspectives. Speaker 3 plays the role of a mediator, emphasizing the importance of balance and flexibility, while Speaker 1 and 2 provide robust arguments for their respective stances. The conversation remains solution-focused, highlighting strategies to integrate the strengths of both approaches effectively.\n"
     ]
    }
   ],
   "source": [
    "analyze_conversation(conversation_transcript_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab48928e-f979-4dde-b135-92a7d8a47600",
   "metadata": {},
   "source": [
    "I don't think this changed much about the date. It seems like the roles are very similar to before, with the positions being very comparable. I think we may need some fundamental changes to the structure of the conversation in order to bring about more natural responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f83c40-9c08-48a6-b507-a7a4615d8df6",
   "metadata": {},
   "source": [
    "## Closing Remarks\n",
    "Compared to the other approach that we explored in the [previous blog](https://ezou626.github.io/comm4190_F25_Using_LLMs_Blog/posts/001_an_llm_conversation/an_llm_conversation.html), it seems that sharing a chat window allows us to more easily manage the flow of a debate with similar results to before, accounting for the changes in the system prompt. To truly identify the differences between these two approaches, I think it is necessary to construct more complicated social scenarios and develop a standardized way to evaluate in what ways we see some true differences. To generalize, the importance of using \"user\" versus \"assistant\" responses to represent different participants in conversations involving multiple LLMs to influence desired results is a possible research path that we can look into for the blog.\n",
    "\n",
    "However, with this new system prompt, it seems we come out with less concrete conclusions than before. That could be a detriment to the usefulness of this system prompt as a tool, but it could also bring us closer to real conversations. In reality, not all conversations end in a way that can be nicely wrapped up and applied to a real world problem. However, we do see many useful things come out of conversations in practice in our lives, with both AI and others. Seamlessly integrating AI in a way that adds to conversations seems like it requires a more \"realistic\" formulation.\n",
    "\n",
    "> Note: Just wanted to acknowledge that we could probably specify in the system prompt that models should introduce more evidence and be clearer in their rebuttal structure, which could help make the conversation more helpful for listeners.\n",
    "\n",
    "In terms of immediate ways we can build on this work, I'm interested in simulating a less rigid environment. In the real world, chats are not strictly turn-based (in that they follow a set order, and that each speaker always speaks). If we can introduce spontaneity (with randomness, for example), I wonder if we could increase the breadth of chats that we see.\n",
    "\n",
    "> **Future Work**:\n",
    "> - Introduce spotaneous elements to the conversation (immediate focus)\n",
    "> - Evaluate using \"user\" versus \"assistant\" to represent various participants in the conversation\n",
    "> - System prompt changes (mentioned before)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
