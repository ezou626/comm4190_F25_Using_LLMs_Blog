{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d248efdd-1593-49a8-b642-4e65b1f59259",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Spontaneity\"\n",
    "description: \"Adding randomness to an LLM conversation\" \n",
    "author: \"Eric Zou\"\n",
    "date: \"9/12/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Conversations\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1a5b9bc-898e-4233-a860-253dfca1b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import base64\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from IPython.display import FileLink, display\n",
    "from dotenv import load_dotenv\n",
    "from random import shuffle, randint, choice, random\n",
    "# Load API key\n",
    "_ = load_dotenv(\"../../../comm4190_F25/01_Introduction_and_setup/.env\")\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb26f1-be98-4af9-9142-ef24ab025c56",
   "metadata": {},
   "source": [
    "## LLM Conversations\n",
    "We're going to build on our progress in the [last post](https://ezou626.github.io/comm4190_F25_Using_LLMs_Blog/posts/002_another_llm_conversation/another_llm_conversation.html) to introduce more spontaneity and randomness into our chats. Let's first try adding randomness in the order in which models speak. By the second iteration, this could result in less predictable conversations. \n",
    "\n",
    "We're also going to change the prompt a little bit too. I'd like to switch more into understanding how we can inspire the LLM agents to collaborate and engage in a less structured way than a debate setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "842de4da-3f50-4ebc-a302-136ef24fe102",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBATE_TOPIC = \"\"\"Code, testing, and dev infra should be prioritized over comprehensive documentation.\"\"\"\n",
    "SYSTEM_PROMPT = \"You are participating in a conversation between experienced software engineers. Each speaker should respond when directed. Keep questions minimal and only use them when necessary.\"\n",
    "\n",
    "# prompt to analyze conversations\n",
    "EVALUATION_PROMPT = \"\"\"\n",
    "Your objective is to analyze this conversation between speakers.\n",
    "Your response should follow this organization:\n",
    "- A Brief Summary\n",
    "- Final Outputs/Artifacts/Takeaways\n",
    "- Characteristics/Dynamic (Competitive/Collaborative/etc.)\n",
    "\"\"\"\n",
    "\n",
    "def analyze_conversation(conversation: str):\n",
    "    input_chat = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": EVALUATION_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Here is the transcript\\n\" + conversation\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"gpt-4o\",\n",
    "        messages = input_chat,\n",
    "        store = False\n",
    "    )\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "839db79d-552e-499e-a293-e03f2e74ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_organic_conversation_v1(\n",
    "    iterations: int, \n",
    "    openai_model_id: str,\n",
    "    participant_count: int,\n",
    "    topic: str,\n",
    "    system_prompt: str,\n",
    ") -> list[dict]:\n",
    "    # model 1 is the first speaker\n",
    "    debate_history = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"The topic is: \" + topic}\n",
    "    ]\n",
    "\n",
    "    ordering = list(range(1, participant_count + 1))\n",
    "\n",
    "    for i in tqdm(range(iterations)):\n",
    "        if i > 0:\n",
    "            first = choice(ordering[:-1])\n",
    "            remaining = [i for i in ordering if i != first]\n",
    "            shuffle(remaining)\n",
    "            ordering = [first] + remaining\n",
    "        for model in ordering: # RANDOM ORDERING\n",
    "            speaker_id = f\"speaker_{model}\"\n",
    "            response = client.chat.completions.create(\n",
    "                model = openai_model_id,\n",
    "                messages = debate_history + [{\"role\": \"user\", \"content\": f\"{speaker_id}, it's your turn to speak.\"}],\n",
    "                store = False\n",
    "            );\n",
    "            message = response.choices[0].message.content\n",
    "            debate_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message})\n",
    "\n",
    "    return debate_history\n",
    "\n",
    "# code to save the conversation\n",
    "def save_conversation(\n",
    "    filename: str,\n",
    "    debate_history: list[dict]\n",
    ") -> str:\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    for record in debate_history:\n",
    "\n",
    "        if record[\"role\"] == \"user\":\n",
    "            messages.append(\"mediator:\\n\" + record[\"content\"])\n",
    "        \n",
    "        if record[\"role\"] == \"assistant\":\n",
    "            messages.append(f\"{record[\"name\"]}:\\n{record[\"content\"]}\")\n",
    "    \n",
    "    conversation_transcript = \"\\n\\n\".join(messages)\n",
    "    \n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(conversation_transcript)\n",
    "    \n",
    "    display(FileLink(filename))\n",
    "\n",
    "    return conversation_transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef309b84-da53-4754-bac7-65ad59c87786",
   "metadata": {},
   "source": [
    "### Experiment 1:\n",
    "Let's keep 3 speakers throughout. It will probably be more interesting this way, since a conversation between two speakers could just have the messages be joined together (though this might be interesting to evaluate, we can add it as future work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd8f6f5e-5e5f-4ebe-a2d4-faac22007680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:28<00:00, 14.19s/it]\n"
     ]
    }
   ],
   "source": [
    "debate_1 = run_organic_conversation_v1(2, 'gpt-4o', 3, DEBATE_TOPIC, SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5cbd8a3-b2ca-489b-9c43-d264d44f65ad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='conversation_transcript_1.txt' target='_blank'>conversation_transcript_1.txt</a><br>"
      ],
      "text/plain": [
       "/Commjhub/jupyterhub/home/ezou626/comm4190_F25_Using_LLMs_Blog/posts/003_approaching_a_true_discussion/conversation_transcript_1.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation_transcript_1 = save_conversation(\"conversation_transcript_1.txt\", debate_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2dd78e-cdfc-452f-8a74-7e0e41084a76",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Like last time, we're going to analyze the conversation with AI as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67c79944-f7c9-486d-82d4-fe43100a200d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Brief Summary**: The conversation revolves around the prioritization between code, testing, and development infrastructure versus comprehensive documentation. Speaker 1 advocates for prioritizing the former due to its direct impact on software reliability and efficiency, suggesting documentation can follow as a secondary priority. Speaker 2 underscores the importance of comprehensive documentation, especially for onboarding and scalability. Speaker 3 proposes a balanced integration of documentation into the development process through automation and self-documenting techniques, aligning with regulatory requirements when necessary.\n",
      "\n",
      "- **Final Outputs/Artifacts/Takeaways**:\n",
      "  - There is an acknowledgment that both development infrastructure and documentation hold significant value, each crucial for different reasons such as reliability, maintainability, onboarding, and compliance.\n",
      "  - The consensus is towards integrating documentation seamlessly into the development process using automation and tools that ensure documentation remains current and synchronized with code changes.\n",
      "  - Practical strategies proposed include employing self-documenting code practices, utilizing documentation generation tools, and incorporating documentation updates into standard development workflows like code reviews and commit messaging.\n",
      "\n",
      "- **Characteristics/Dynamic (Collaborative/Competitive/etc.)**: \n",
      "  - The discussion is collaborative, with each speaker building upon the ideas of the others to arrive at a nuanced understanding of the balance between development priorities and documentation. \n",
      "  - Speakers recognize and incorporate differing viewpoints, ultimately finding a middle ground that supports both development efficiency and the comprehensive documentation needs.\n",
      "  - There is a collective move towards practical solutions that address potential conflicts between maintaining quality development practices and ensuring documentation does not become outdated or sidelined.\n"
     ]
    }
   ],
   "source": [
    "analyze_conversation(conversation_transcript_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c953ad4f-5785-4f79-99ee-f1bb192ac144",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We're shifting back to the conversational tone that we had in the [first post](https://ezou626.github.io/comm4190_F25_Using_LLMs_Blog/posts/002_another_llm_conversation/another_llm_conversation.html), probably due mainly to the changes in the prompt. We can still see some disagreement in the beginning, but as the conversation continues, the speakers' discussions converge, while still maintaining the individual opinions. However, each speaker always speaks once during each turn. This is not necessarily true for real world conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a19fd1b-4655-4916-9125-2235f3463342",
   "metadata": {},
   "source": [
    "### Experiment 2\n",
    "Let's see if we can extend the conversation, but this time, let's leverage randomness to simulate the fact that people don't always participate in each conversation. We'll keep a fixed order for now, but every time a model speaks, we'll track it to make sure that the next speaker is not the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f26468e1-0fa7-4e2e-a74a-6f5d10869b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:56<00:00,  7.10s/it]\n"
     ]
    }
   ],
   "source": [
    "def run_organic_conversation_v2(\n",
    "    iterations: int, \n",
    "    openai_model_id: str,\n",
    "    participant_count: int,\n",
    "    topic: str,\n",
    "    system_prompt: str,\n",
    "    dropout_chance: float\n",
    ") -> list[dict]:\n",
    "    # model 1 is the first speaker\n",
    "    debate_history = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"The topic is: \" + topic}\n",
    "    ]\n",
    "\n",
    "    ordering = list(range(1, participant_count + 1))\n",
    "    last_speaker = -1\n",
    "\n",
    "    for i in tqdm(range(iterations)):\n",
    "        # if i > 0:\n",
    "        #     first = choice(ordering[:-1])\n",
    "        #     remaining = [i for i in ordering if i != first]\n",
    "        #     shuffle(remaining)\n",
    "        #     ordering = [first] + remaining\n",
    "        for model in ordering: # RANDOM ORDERING\n",
    "            if random() < dropout_chance:\n",
    "                continue # SKIP\n",
    "            if last_speaker == model:\n",
    "                continue\n",
    "            speaker_id = f\"speaker_{model}\"\n",
    "            response = client.chat.completions.create(\n",
    "                model = openai_model_id,\n",
    "                messages = debate_history + [{\"role\": \"user\", \"content\": f\"{speaker_id}, it's your turn to speak.\"}],\n",
    "                store = False\n",
    "            );\n",
    "            message = response.choices[0].message.content\n",
    "            debate_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message})\n",
    "            last_speaker = model\n",
    "\n",
    "    return debate_history\n",
    "\n",
    "debate_2 = run_organic_conversation_v2(8, 'gpt-4o', 3, DEBATE_TOPIC, SYSTEM_PROMPT, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f37c7523-a390-4b72-b878-e5def6221b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='conversation_transcript_2.txt' target='_blank'>conversation_transcript_2.txt</a><br>"
      ],
      "text/plain": [
       "/Commjhub/jupyterhub/home/ezou626/comm4190_F25_Using_LLMs_Blog/posts/003_approaching_a_true_discussion/conversation_transcript_2.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation_transcript_2 = save_conversation(\"conversation_transcript_2.txt\",debate_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed792c6-2556-4504-8893-b09c94293bd3",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "First, let's generate an AI summary of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92417e63-b5f8-4350-9330-2aaf9d4c6699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Brief Summary:**  \n",
      "The conversation revolves around the prioritization of code, testing, and development infrastructure over comprehensive documentation. Speaker 1 argues for the importance of code and infrastructure as they ensure the quality and reliability of the software, especially in agile environments. Speaker 2 emphasizes the long-term benefits of documentation, particularly for knowledge retention and project sustainability. Speaker 3 brings a balanced perspective, advocating for adaptable strategies based on the project's lifecycle and promoting a development culture that values all aspects equally.\n",
      "\n",
      "- **Final Outputs/Artifacts/Takeaways:**  \n",
      "  - The need for balance between immediate development needs and sustainable documentation practices.\n",
      "  - Integration of documentation into the development process using automation tools to keep it current.\n",
      "  - Building a comprehensive development culture where all elements are seen as integral.\n",
      "  - Leadership roles in guiding priorities and maintaining balance.\n",
      "  - Utilizing data-driven approaches and feedback loops to inform strategy and resource allocation.\n",
      "\n",
      "- **Characteristics/Dynamic:**  \n",
      "  - **Collaborative:** The conversation exhibits a collaborative dynamic where all speakers acknowledge each other's points and build on them to reach a common ground.\n",
      "  - **Balanced Approach:** Emphasis on finding a balance between immediate and long-term project needs.\n",
      "  - **Adaptability:** Discusses the need for strategies that adapt to different stages of the project's lifecycle.\n",
      "  - **Leadership and Culture:** Highlights the role of leadership and team culture in achieving a balanced approach to software development.\n"
     ]
    }
   ],
   "source": [
    "analyze_conversation(conversation_transcript_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac76f3-53ce-446f-8436-fdc497892b1e",
   "metadata": {},
   "source": [
    "At a glance, it certainly seems more conversational. I feel that the LLMs are making too many redundant summaries. Perhaps we can adjust the prompt to let the conversation build off the other statements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ad468-3201-4c77-95a3-5c0068a030c5",
   "metadata": {},
   "source": [
    "### Experiment 3\n",
    "We're going to join the two prior approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45b46a75-4fcd-4a9d-b577-3f95a35f0801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:43<00:00,  5.39s/it]\n"
     ]
    }
   ],
   "source": [
    "def run_organic_conversation_v3(\n",
    "    iterations: int, \n",
    "    openai_model_id: str,\n",
    "    participant_count: int,\n",
    "    topic: str,\n",
    "    system_prompt: str,\n",
    "    dropout_chance: float\n",
    ") -> list[dict]:\n",
    "    # model 1 is the first speaker\n",
    "    debate_history = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"The topic is: \" + topic}\n",
    "    ]\n",
    "\n",
    "    ordering = list(range(1, participant_count + 1))\n",
    "    last_speaker = -1\n",
    "\n",
    "    for i in tqdm(range(iterations)):\n",
    "        if i > 0:\n",
    "            first = choice(ordering[:-1])\n",
    "            remaining = [i for i in ordering if i != first]\n",
    "            shuffle(remaining)\n",
    "            ordering = [first] + remaining\n",
    "        for model in ordering: # RANDOM ORDERING\n",
    "            if random() < dropout_chance:\n",
    "                continue # SKIP\n",
    "            if last_speaker == model:\n",
    "                continue\n",
    "            speaker_id = f\"speaker_{model}\"\n",
    "            response = client.chat.completions.create(\n",
    "                model = openai_model_id,\n",
    "                messages = debate_history + [{\"role\": \"user\", \"content\": f\"{speaker_id}, it's your turn to speak.\"}],\n",
    "                store = False\n",
    "            );\n",
    "            message = response.choices[0].message.content\n",
    "            debate_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message})\n",
    "            last_speaker = model\n",
    "\n",
    "    return debate_history\n",
    "\n",
    "debate_3 = run_organic_conversation_v3(8, 'gpt-4o', 3, DEBATE_TOPIC, SYSTEM_PROMPT, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91897f65-9f8d-4a4a-92b7-891c2356d152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='conversation_transcript_3.txt' target='_blank'>conversation_transcript_3.txt</a><br>"
      ],
      "text/plain": [
       "/Commjhub/jupyterhub/home/ezou626/comm4190_F25_Using_LLMs_Blog/posts/003_approaching_a_true_discussion/conversation_transcript_3.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation_transcript_3 = save_conversation(\"conversation_transcript_3.txt\", debate_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b5df4-3eb0-44ae-bf04-d562cc32f4d6",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Now, let's get a quick summary of what happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "925d632f-9cf5-4a4b-b817-80b0f6972730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Brief Summary**: The discussion revolves around the balance between code, testing, development infrastructure, and comprehensive documentation. Speaker 1 supports prioritizing code and infrastructure, especially in agile settings, but acknowledges the value of documentation. Speaker 2 emphasizes the importance of integrating key documentation practices alongside development to prevent knowledge bottlenecks. Speaker 3 advocates for a balanced approach where documentation evolves with development, employing tools and practices that ensure both areas are maintained effectively.\n",
      "\n",
      "- **Final Outputs/Artifacts/Takeaways**:\n",
      "  - The idea of iterative and lightweight documentation, evolving with the product, to enhance team onboarding and collaboration.\n",
      "  - Integration of documentation into the definition of \"done\" for each task and feature.\n",
      "  - Leveraging automated tools to maintain documentation accuracy and reduce manual burdens.\n",
      "  - Cultivating a culture where both code quality and documentation are seen as interconnected responsibilities through pair programming, continuous integration, and collaboration tools.\n",
      "\n",
      "- **Characteristics/Dynamics**: The conversation is collaborative and consensus-driven, with each speaker building on the ideas of others. There is a collective push towards achieving a balanced development process that recognizes the importance of both technical infrastructure and documentation. The dynamic reflects a mutual acknowledgment of each other's viewpoints, seeking common ground and practical solutions that integrate both priorities effectively in software development workflows.\n"
     ]
    }
   ],
   "source": [
    "analyze_conversation(conversation_transcript_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc32825e-285a-4206-8295-cd41ba95cd16",
   "metadata": {},
   "source": [
    "Surprisingly, I think this set was much more conversational than the last. I don't know if I can declare this as reproducible though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1828dd-8707-43e9-a11e-f31a36703356",
   "metadata": {},
   "source": [
    "### Experiment 4\n",
    "The final thing we can do is make the prompts more like thoughts. Instead of saying \"it's your turn to speak\", we can say \"please share your current perspective with the crowd\". We can also alter the system prompt to not mention a simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ce74631c-0642-4d6c-bffd-08fade1a1289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [02:05<00:00, 15.69s/it]\n"
     ]
    }
   ],
   "source": [
    "NEW_SYSTEM_PROMPT = \"You a participant in a conversation between experienced software engineers. Keep questions minimal and only use them when necessary. Please greet the other participants when you join.\"\n",
    "\n",
    "def run_organic_conversation_v4(\n",
    "    iterations: int, \n",
    "    openai_model_id: str,\n",
    "    participant_count: int,\n",
    "    topic: str,\n",
    "    system_prompt: str,\n",
    "    dropout_chance: float\n",
    ") -> list[dict]:\n",
    "    # model 1 is the first speaker\n",
    "    debate_history = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt + \" The topic is: \" + topic}\n",
    "    ]\n",
    "\n",
    "    ordering = list(range(1, participant_count + 1))\n",
    "    last_speaker = -1\n",
    "\n",
    "    for i in tqdm(range(iterations)):\n",
    "        if i > 0:\n",
    "            first = choice(ordering[:-1])\n",
    "            remaining = [i for i in ordering if i != first]\n",
    "            shuffle(remaining)\n",
    "            ordering = [first] + remaining\n",
    "        for model in ordering: # RANDOM ORDERING\n",
    "            if random() < dropout_chance:\n",
    "                continue # SKIP\n",
    "            if last_speaker == model:\n",
    "                continue\n",
    "            speaker_id = f\"speaker_{model}\"\n",
    "            response = client.chat.completions.create(\n",
    "                model = openai_model_id,\n",
    "                messages = debate_history + [{\"role\": \"user\", \"content\": f\"{speaker_id}, please share your perspective with the others and engage with the responses of the other participants.\"}],\n",
    "                store = False\n",
    "            );\n",
    "            message = response.choices[0].message.content\n",
    "            debate_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message})\n",
    "            last_speaker = model\n",
    "\n",
    "    return debate_history\n",
    "\n",
    "debate_4 = run_organic_conversation_v4(8, 'gpt-4o', 3, DEBATE_TOPIC, NEW_SYSTEM_PROMPT, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "53ba120c-ddb4-4aad-a214-56167fd57a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='conversation_transcript_4.txt' target='_blank'>conversation_transcript_4.txt</a><br>"
      ],
      "text/plain": [
       "/Commjhub/jupyterhub/home/ezou626/comm4190_F25_Using_LLMs_Blog/posts/003_approaching_a_true_discussion/conversation_transcript_4.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation_transcript_4 = save_conversation(\"conversation_transcript_4.txt\", debate_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c50476-141c-4539-9a88-eba36c1351a0",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Let's see how that changed the debate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7cf81b87-dcf8-419a-8629-9af95f5c7b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Brief Summary**: The conversation revolves around the balance between prioritizing code quality, testing, and development infrastructure over comprehensive documentation in software development. Speaker_1 initiates the discussion by emphasizing the importance of clean code and robust testing. Speaker_3 agrees while acknowledging the potential challenges of inadequate documentation in complex systems. Speaker_2 advocates for a balance, stressing the need for essential documentation and lightweight approaches like API documentation and high-level overviews. The conversation further explores innovative documentation methods like video tutorials and AI-assisted documentation.\n",
      "\n",
      "- **Final Outputs/Artifacts/Takeaways**: The primary takeaways include the consensus on the importance of balancing documentation with code quality and infrastructure. The participants also share practical strategies such as embedding documentation updates into the code review process, using version control for documentation, leveraging video tutorials, and experimenting with AI-driven documentation tools.\n",
      "\n",
      "- **Characteristics/Dynamic**: The conversation is highly collaborative, with each speaker building upon the others' points, sharing personal experiences, and exploring various strategies to achieve the common goal of effective software development practices. The speakers demonstrate a willingness to learn from each other's insights and adapt successful methods to their own workflows, fostering a dynamic environment of mutual respect and shared learning.\n"
     ]
    }
   ],
   "source": [
    "analyze_conversation(conversation_transcript_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab48928e-f979-4dde-b135-92a7d8a47600",
   "metadata": {},
   "source": [
    "The conversation is much more conversational this time, with explicit instructions to address the other speakers, there's a lot more structure in the engagement between the three speakers. Additionally, it seems that near the end of the conversation, the speakers converge toward some actionable items, despite there not being explicit instructions to find common ground. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f83c40-9c08-48a6-b507-a7a4615d8df6",
   "metadata": {},
   "source": [
    "## Closing Remarks\n",
    "With the new shared chat window approach, some prompt tuning, and some randomization, we approach a more natural conversational vibe in the discussion between the 3 LLMs. It also feels less structured, and that could potentially allow for integration with human speakers and produce results similar to meetings or panels. It would be interesting to see what kinds of topics LLMs can excel in, or if they can even do well without having a predefined topic.\n",
    "\n",
    "It's also important to consider that this is probably not the only way to do this. Potentially, we could add more prompts before we get each speaker LLM to answer.\n",
    "\n",
    "> **Future Work**:\n",
    "> - Experiment with more topics\n",
    "> - Ignore predefined topic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
