{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f2ec12-4f83-4021-beb7-28e6f4236116",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "title: Looking Closer\n",
    "description: \"More objective analysis on LLM conversations\" \n",
    "author: \"Eric Zou\"\n",
    "date: \"9/22/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Conversations\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46301a28-1c30-4ffe-82f2-2da986070a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerDecoderModel requires ML dependencies. Run 'pip install convokit[llm]' to install them.\n",
      "UnslothUtteranceSimulatorModel requires ML dependencies. Run 'pip install convokit[llm]' to install them.\n"
     ]
    }
   ],
   "source": [
    "# as always, some boilerplate\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import base64\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from IPython.display import FileLink, display, Markdown\n",
    "from dotenv import load_dotenv\n",
    "from random import shuffle, randint, choice, random\n",
    "from math import floor\n",
    "from convokit import Corpus, Speaker, Utterance\n",
    "\n",
    "# Load API key\n",
    "_ = load_dotenv(\"../../../comm4190_F25/01_Introduction_and_setup/.env\")\n",
    "client = OpenAI()\n",
    "\n",
    "# changing the topic to make it a bit more conversational too and less of a debate\n",
    "TOPIC = \"\"\"Code, testing, and infra as a source of truth versus comprehensive documentation.\"\"\"\n",
    "\n",
    "# we're interested in consensus\n",
    "EVALUATION_PROMPT = \"\"\"\n",
    "Your objective is to analyze this conversation between a few speakers.\n",
    "Your response should follow this organization:\n",
    "- Dynamic: Collaborative (1) vs. Competitive (10)\n",
    "- Conclusiveness: Consensus (1) vs. Divergence (10)\n",
    "- Speaker Identity: Similarity (1) vs. Diversity (10)\n",
    "- Speaker Fluidity: Malleability (1) vs. Consistency (10)\n",
    "Please offer a score from 1 to 10 for each.\n",
    "For each section, format your result as follows:\n",
    "**[Section Name]:**\n",
    "\n",
    "Score: [score]/10\n",
    "\n",
    "Verdict: [a short summary]\n",
    "\n",
    "Explanation: [reasoning with explicit examples from the conversation]\n",
    "\n",
    "Use Markdown when convenient.\n",
    "\"\"\"\n",
    "\n",
    "def generate_llm_conversation_review(conversation: str):\n",
    "    input_chat = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": EVALUATION_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Here is the transcript\\n\" + conversation\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"gpt-4o\",\n",
    "        messages = input_chat,\n",
    "        store = False\n",
    "    )\n",
    "    display(Markdown(response.choices[0].message.content))\n",
    "\n",
    "# code to save the conversation\n",
    "def save_conversation(\n",
    "    filename: str,\n",
    "    conversation_history: list[dict]\n",
    ") -> str:\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    for record in conversation_history:\n",
    "\n",
    "        if record[\"role\"] == \"user\":\n",
    "            messages.append(\"mediator:\\n\" + record[\"content\"])\n",
    "        \n",
    "        if record[\"role\"] == \"assistant\":\n",
    "            messages.append(f\"{record[\"name\"]}:\\n{record[\"content\"]}\")\n",
    "    \n",
    "    conversation_transcript = \"\\n\\n\".join(messages)\n",
    "    \n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(conversation_transcript)\n",
    "    \n",
    "    display(FileLink(filename))\n",
    "\n",
    "    return conversation_transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb43ea7-2117-47b2-b132-0227a8e5a346",
   "metadata": {},
   "source": [
    "## Generating a Conversation\n",
    "Let's build a sufficiently long conversation so we can create a way to analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "534d4cca-7f07-4351-875c-6a5602e64e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_SYSTEM_PROMPT = (\n",
    "    \"You a participant in a conversation between experienced software engineers. \"\n",
    "    \"Keep questions minimal and only use them when necessary. \"\n",
    "    \"Please greet the other participants when you join.\"\n",
    ")\n",
    "\n",
    "def run_conversation(\n",
    "    iterations: int, \n",
    "    openai_model_id: str,\n",
    "    participant_count: int,\n",
    "    participant_personas: list[str],\n",
    "    topic: str,\n",
    "    system_prompt: str,\n",
    "    dropout_chance: float\n",
    ") -> list[dict]:\n",
    "    conversation_history = [\n",
    "        {\"role\": \"system\", \"content\": f\"{system_prompt} The topic is: {topic}\"}\n",
    "    ]\n",
    "\n",
    "    ordering = list(range(1, participant_count + 1))\n",
    "    last_speaker = -1\n",
    "\n",
    "    def build_message(history, speaker_id, persona, message_window_size):\n",
    "\n",
    "        speaker_messages = [\n",
    "            msg for msg in history \n",
    "            if msg.get(\"name\") == speaker_id\n",
    "        ][-message_window_size:]\n",
    "    \n",
    "        other_messages = [\n",
    "            msg for msg in history \n",
    "            if msg.get(\"name\") not in (None, speaker_id)  # skip system, skip self\n",
    "        ][-message_window_size:]\n",
    "\n",
    "        transcript = []\n",
    "        if speaker_messages:\n",
    "            transcript.append(\"Recent messages from you:\")\n",
    "            transcript.extend(\n",
    "                f\"- {msg['content']}\" for msg in speaker_messages\n",
    "            )\n",
    "        if other_messages:\n",
    "            transcript.append(\"\\nRecent messages from others:\")\n",
    "            transcript.extend(\n",
    "                f\"- {msg.get('name', msg['role'])}: {msg['content']}\"\n",
    "                for msg in other_messages\n",
    "            )\n",
    "    \n",
    "        transcript_str = \"\\n\".join(transcript)\n",
    "        \n",
    "        return history + [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": (\n",
    "                    f\"{speaker_id}, please share your perspective with the others and engage \"\n",
    "                    f\"with their responses.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"name\": speaker_id,\n",
    "                \"content\": (\n",
    "                    f\"I should remember that the following is the most current state of the conversation.\\n\"\n",
    "                    f\"{transcript_str}\\n\\n\"\n",
    "                    f\"I also recall my identity is {persona}.\"\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    def shuffle_order(ordering: list[int]) -> list[int]:\n",
    "        first = choice(ordering[:-1])\n",
    "        remaining = [p for p in ordering if p != first]\n",
    "        shuffle(remaining)\n",
    "        return [first] + remaining\n",
    "\n",
    "    for i in tqdm(range(iterations)):\n",
    "\n",
    "        # shuffle ordering\n",
    "        if i > 0:\n",
    "            ordering = shuffle_order(ordering)\n",
    "\n",
    "        # follow ordering\n",
    "        for participant_id in ordering:\n",
    "\n",
    "            # chance to skip speaker and avoid double speak (1984)\n",
    "            if random() < dropout_chance or last_speaker == participant_id:\n",
    "                continue\n",
    "\n",
    "            speaker_id = f\"speaker_{participant_id}\"\n",
    "            persona = participant_personas[participant_id - 1]\n",
    "            response = client.chat.completions.create(\n",
    "                model = openai_model_id,\n",
    "                messages=build_message(conversation_history, speaker_id, persona, 5),\n",
    "                store = False\n",
    "            )\n",
    "            message = response.choices[0].message.content\n",
    "            conversation_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message})\n",
    "            last_speaker = participant_id\n",
    "\n",
    "    return conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e411dbe8-0891-48d3-a5bb-60fd3d3be7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:52<00:00,  8.63s/it]\n"
     ]
    }
   ],
   "source": [
    "personas = [\n",
    "    \"a software engineer in big tech with mainly internal work\",\n",
    "    \"an open source developer with experience in major upstream projects\",\n",
    "    \"a founder of a startup\"\n",
    "]\n",
    "conversation = run_conversation(20, 'gpt-4o', 3, personas, TOPIC, NEW_SYSTEM_PROMPT, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c510389e-d4db-4e60-bacb-5ec49f280465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='long_conversation.txt' target='_blank'>long_conversation.txt</a><br>"
      ],
      "text/plain": [
       "/Commjhub/jupyterhub/home/ezou626/comm4190_F25_Using_LLMs_Blog/posts/007_looking_closer/long_conversation.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transcript = save_conversation(\"long_conversation.txt\", conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb5974-c646-4eb9-8e32-cdf7e2ba1f8a",
   "metadata": {},
   "source": [
    "## Our Old LLM Analyzer\n",
    "We love our good buddy GPT-4o. Not doing work is super exciting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a352d469-7121-4304-8c00-0a197599388e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Dynamic:**\n",
       "\n",
       "Score: 2/10\n",
       "\n",
       "Verdict: The conversation is highly collaborative.\n",
       "\n",
       "Explanation: The speakers acknowledge each other’s perspectives, build on each other's ideas, and share experiences to collectively explore solutions. They ask each other questions and express agreement, such as speaker_2's agreement with speaker_3 or speaker_1's recognition of both speakers' contributions. Their shared goal appears to be enhancing documentation processes, rather than debating for superiority.\n",
       "\n",
       "**Conclusiveness:**\n",
       "\n",
       "Score: 1/10\n",
       "\n",
       "Verdict: There is a strong consensus among the speakers.\n",
       "\n",
       "Explanation: The participants largely agree on the importance of a mixed approach to documentation, combining automated tools and human input. They share strategies and challenges without any significant divergence in opinion. They frequently affirm each other's points and provide complementary solutions, as seen when they discuss the integration of AI and maintaining documentation quality.\n",
       "\n",
       "**Speaker Identity:**\n",
       "\n",
       "Score: 6/10\n",
       "\n",
       "Verdict: There is a moderate diversity in speaker identity.\n",
       "\n",
       "Explanation: The speakers come from different professional backgrounds: speaker_2 from open source, speaker_3 from a startup, and speaker_1 from a big tech environment. Their methods and focuses differ slightly due to these backgrounds (e.g., speaker_2’s need for community engagement and speaker_3's focus on agility). Despite this, their goals remain similar, and they have overlapping solutions and challenges, like trust in AI tools and embedding feedback into workflow.\n",
       "\n",
       "**Speaker Fluidity:**\n",
       "\n",
       "Score: 8/10\n",
       "\n",
       "Verdict: The speakers maintain consistent perspectives.\n",
       "\n",
       "Explanation: Each speaker holds consistent viewpoints throughout the conversation. Speaker_2 consistently discusses open-source challenges and community engagement, speaker_3 focuses on agility and AI integration in a startup, and speaker_1 emphasizes the need for comprehensive documentation in big tech. Their perspectives remain steady across different topics discussed, from collaboration tools to feedback loops, reflecting continuity in their professional experiences and challenges."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_llm_conversation_review(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75614665-aad3-4af4-99ca-0f31f7411720",
   "metadata": {},
   "source": [
    "## Build a Corpus\n",
    "Now it's time for us to assemble a ConvoKit corpus, which is a common data structure that forms the base of a lot of analysis pipelines in ConvoKit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5c3cbe7-c9ca-4ae4-8306-ad5b65bc2dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "['speaker_2', 'speaker_3', 'speaker_1']\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# filter messages for assistant messages only\n",
    "assistant_messages = [\n",
    "    message for message in conversation if message[\"role\"] == \"assistant\"\n",
    "]\n",
    "\n",
    "speakers = {message[\"name\"]: Speaker(id=message[\"name\"]) for message in assistant_messages}\n",
    "\n",
    "print(len(assistant_messages))\n",
    "print(list(speakers))\n",
    "\n",
    "utterances = [Utterance(\n",
    "    id=str(i),\n",
    "    speaker=speakers[message[\"name\"]],\n",
    "    text=message[\"content\"],\n",
    "    conversation_id=\"conversation_1\",\n",
    "    reply_to= str(i-1) if i != 0 else None\n",
    ") for i, message in enumerate(assistant_messages)]\n",
    "\n",
    "corpus = Corpus(utterances=utterances)\n",
    "\n",
    "print(len(utterances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c721c3a-2bf0-4406-8bc3-ae948dfc69b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3.12: No module named pip\n"
     ]
    }
   ],
   "source": [
    "!python3.12 -m pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9c4cde0-e0a4-4428-930a-eb8cf41b7b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convokit requires a SpaCy model to be installed. Run `python -m spacy download MODEL_NAME` and retry.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyterhub/share/jupyter/venv/python3-12_comm4190/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from convokit import TextParser, PolitenessStrategies\n",
    "\n",
    "# Analyze politeness\n",
    "ps = PolitenessStrategies()\n",
    "parser = TextParser()\n",
    "corpus = parser.transform(corpus)\n",
    "corpus = ps.transform(corpus)\n",
    "\n",
    "# Access politeness features\n",
    "for utt in corpus.iter_utterances():\n",
    "    print(utt.meta['politeness_strategy_features'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
