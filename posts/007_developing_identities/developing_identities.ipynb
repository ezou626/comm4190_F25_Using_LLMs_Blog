{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c0e2081e-8425-4a1a-ab8f-31263cc142ee",
   "metadata": {},
   "source": [
    "---\n",
    "title: Developing Identities\n",
    "description: \"Having models come up with unique personas in conversation\" \n",
    "author: \"Eric Zou\"\n",
    "date: \"9/22/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Conversations\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d162358c-5638-4e4b-b305-6948a0ea06a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, some boilerplate\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from IPython.display import FileLink, display, Markdown\n",
    "from dotenv import load_dotenv\n",
    "from random import shuffle, choice, random\n",
    "\n",
    "# Load API key\n",
    "_ = load_dotenv(\".env\")\n",
    "client = OpenAI()\n",
    "\n",
    "# changing the topic to make it a bit more conversational too and less of a debate\n",
    "TOPIC = \"\"\"Code, testing, and infra as a source of truth versus comprehensive documentation.\"\"\"\n",
    "\n",
    "# we're interested in consensus\n",
    "EVALUATION_PROMPT = \"\"\"\n",
    "Your objective is to analyze this conversation between a few speakers.\n",
    "Your response should follow this organization:\n",
    "- Dynamic: Collaborative (1) vs. Competitive (10)\n",
    "- Conclusiveness: Consensus (1) vs. Divergence (10)\n",
    "- Speaker Identity: Similarity (1) vs. Diversity (10)\n",
    "- Speaker Fluidity: Malleability (1) vs. Consistency (10)\n",
    "Please offer a score from 1 to 10 for each.\n",
    "For each section, format your result as follows:\n",
    "**[Section Name]:**\n",
    "\n",
    "Score: [score]/10\n",
    "\n",
    "Verdict: [a short summary]\n",
    "\n",
    "Explanation: [reasoning with explicit examples from the conversation]\n",
    "\n",
    "Use Markdown when convenient.\n",
    "\"\"\"\n",
    "\n",
    "def analyze_conversation(conversation: str):\n",
    "    input_chat = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": EVALUATION_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Here is the transcript\\n\" + conversation\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"gpt-4o\",\n",
    "        messages = input_chat,\n",
    "        store = False\n",
    "    )\n",
    "    display(Markdown(response.choices[0].message.content))\n",
    "\n",
    "# code to save the conversation\n",
    "def save_conversation(\n",
    "    filename: str,\n",
    "    conversation_history: list[dict]\n",
    ") -> str:\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    for record in conversation_history:\n",
    "\n",
    "        if record[\"role\"] == \"user\":\n",
    "            messages.append(\"mediator:\\n\" + record[\"content\"])\n",
    "        \n",
    "        if record[\"role\"] == \"assistant\":\n",
    "            messages.append(f\"{record['name']}:\\n{record['content']}\")\n",
    "    \n",
    "    conversation_transcript = \"\\n\\n\".join(messages)\n",
    "    \n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(conversation_transcript)\n",
    "    \n",
    "    display(FileLink(filename))\n",
    "\n",
    "    return conversation_transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc0cecb-3634-44b0-84c8-0e46f0919e5c",
   "metadata": {},
   "source": [
    "## Experiment 1: Making a Speaker Useful\n",
    "We can try to instruct a model to fill in a gap they don't see in the current conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "713ac147-0a2d-42ca-86f3-af7dd4557878",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_SYSTEM_PROMPT = (\n",
    "    \"You a participant in a conversation between experienced software engineers. \"\n",
    "    \"Keep questions minimal and only use them when necessary. \"\n",
    "    \"Please greet the other participants when you join.\"\n",
    ")\n",
    "\n",
    "def run_conversation(\n",
    "    iterations: int, \n",
    "    openai_model_id: str,\n",
    "    participant_count: int,\n",
    "    topic: str,\n",
    "    system_prompt: str,\n",
    "    dropout_chance: float\n",
    ") -> list[dict]:\n",
    "    conversation_history = [\n",
    "        {\"role\": \"system\", \"content\": f\"{system_prompt} The topic is: {topic}\"}\n",
    "    ]\n",
    "\n",
    "    ordering = list(range(1, participant_count + 1))\n",
    "    last_speaker = -1\n",
    "\n",
    "    def build_message(history, speaker_id, message_window_size):\n",
    "\n",
    "        speaker_messages = [\n",
    "            msg for msg in history \n",
    "            if msg.get(\"name\") == speaker_id\n",
    "        ][-message_window_size:]\n",
    "    \n",
    "        other_messages = [\n",
    "            msg for msg in history \n",
    "            if msg.get(\"name\") not in (None, speaker_id)  # skip system, skip self\n",
    "        ][-message_window_size:]\n",
    "\n",
    "        transcript = []\n",
    "        if speaker_messages:\n",
    "            transcript.append(\"Recent messages from you:\")\n",
    "            transcript.extend(\n",
    "                f\"- {msg['content']}\" for msg in speaker_messages\n",
    "            )\n",
    "        if other_messages:\n",
    "            transcript.append(\"\\nRecent messages from others:\")\n",
    "            transcript.extend(\n",
    "                f\"- {msg.get('name', msg['role'])}: {msg['content']}\"\n",
    "                for msg in other_messages\n",
    "            )\n",
    "    \n",
    "        transcript_str = \"\\n\".join(transcript)\n",
    "        \n",
    "        return history + [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": (\n",
    "                    f\"{speaker_id}, please share your perspective with the others and engage \"\n",
    "                    f\"with their responses. Try to look for a way to provide insights that others have missed.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"name\": speaker_id,\n",
    "                \"content\": (\n",
    "                    f\"I should remember that the following is the most current state of the conversation.\\n\"\n",
    "                    f\"{transcript_str}\\n\\n\"\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    def shuffle_order(ordering: list[int]) -> list[int]:\n",
    "        first = choice(ordering[:-1])\n",
    "        remaining = [p for p in ordering if p != first]\n",
    "        shuffle(remaining)\n",
    "        return [first] + remaining\n",
    "\n",
    "    for i in tqdm(range(iterations)):\n",
    "\n",
    "        # shuffle ordering\n",
    "        if i > 0:\n",
    "            ordering = shuffle_order(ordering)\n",
    "\n",
    "        # follow ordering\n",
    "        for participant_id in ordering:\n",
    "\n",
    "            # chance to skip speaker and avoid double speak (1984)\n",
    "            if random() < dropout_chance or last_speaker == participant_id:\n",
    "                continue\n",
    "\n",
    "            speaker_id = f\"speaker_{participant_id}\"\n",
    "            response = client.chat.completions.create(\n",
    "                model = openai_model_id,\n",
    "                messages=build_message(conversation_history, speaker_id, 5),\n",
    "                store = False\n",
    "            )\n",
    "            message = response.choices[0].message.content\n",
    "            conversation_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message})\n",
    "            last_speaker = participant_id\n",
    "\n",
    "    return conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c86229-d64c-4bfc-aa35-bd4903e3acbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [02:34<00:00, 19.28s/it]\n"
     ]
    }
   ],
   "source": [
    "conversation = run_conversation(8, 'gpt-4o', 3, TOPIC, NEW_SYSTEM_PROMPT, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6872fd4b-f8e5-43cb-a6b9-d2ca31261897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='conversation_1.txt' target='_blank'>conversation_1.txt</a><br>"
      ],
      "text/plain": [
       "/home/ezou626/repos/comm4190_F25_Using_LLMs_Blog/posts/007_developing_identities/conversation_1.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation_transcript = save_conversation(\"conversation_1.txt\", conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "333893f8-3d42-4286-bf88-230638a936f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Dynamic:**\n",
       "\n",
       "Score: 2/10\n",
       "\n",
       "Verdict: The conversation is highly collaborative, with each speaker building on and extending the ideas presented by others.\n",
       "\n",
       "Explanation: Throughout the conversation, speakers acknowledge each other's points positively and expand on them rather than contradicting or debating them. For example, speaker_2 supports speaker_3's idea by expanding on fostering a culture of documentation and speaker_1 offers further insights into using documentation as a collaborative tool. This shows a cooperative dialogue where speakers are aligned towards a mutual understanding and shared outcome.\n",
       "\n",
       "**Conclusiveness:**\n",
       "\n",
       "Score: 3/10\n",
       "\n",
       "Verdict: The discussion tends more towards consensus with a focus on collaborative exploration to enhance understanding rather than ending in substantial divergence.\n",
       "\n",
       "Explanation: The speakers explore different facets of documentation, automation, and strategic alignment, generally agreeing on the importance of these areas. They aim to enrich the conversation by providing additional insights and methods. For instance, speaker_2 builds on the principal ideas, suggesting the use of templates or feedback mechanisms to enhance documentation. There is a wide-ranging agreement on the points raised, with a shared effort to collectively reach a deeper insight, which aligns more with consensus than divergence.\n",
       "\n",
       "**Speaker Identity:**\n",
       "\n",
       "Score: 4/10\n",
       "\n",
       "Verdict: While there are differences in perspectives, the speakers mostly share similar professional experiences and focuses, resulting in moderate diversity.\n",
       "\n",
       "Explanation: The speakers all discuss topics pertinent to software development, suggesting they share similar technical backgrounds. They explore different angles and experiences—like automated tools, cultural aspects, leadership, and strategic integration—that indicate some diversity, but fundamentally they operate within the same professional domain. speaker_1 and speaker_3, for instance, focus on integration and collaborative tools, highlighting slight variations in their approach while still operating within a shared framework.\n",
       "\n",
       "**Speaker Fluidity:**\n",
       "\n",
       "Score: 9/10\n",
       "\n",
       "Verdict: Each speaker maintains a consistent perspective throughout the conversation, contributing with well-aligned and coherent ideas.\n",
       "\n",
       "Explanation: The speakers consistently expand on their initial points throughout the dialogue. For instance, speaker_3 consistently advocates for technological integration and data analysis from the beginning to the end. Similarly, speaker_2 remains focused on cultural and strategic alignment within organizations. This consistency underscores each speaker's specialized focus and depth of understanding in their area, reflecting a consistent identity in their contributions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyze_conversation(conversation_transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4e5e19-9783-4509-b201-c0f20d8c20f7",
   "metadata": {},
   "source": [
    "## Experiment 2: Being a Bit More Direct\n",
    "Let's try to be a little bit more pushy and instruct the model to maintain a consistent persona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c87c92a-45b1-4ce2-a001-e960274de1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_SYSTEM_PROMPT = (\n",
    "    \"You a participant in a conversation between experienced software engineers. \"\n",
    "    \"Keep questions minimal and only use them when necessary. \"\n",
    "    \"Please greet the other participants when you join.\"\n",
    ")\n",
    "\n",
    "def run_conversation(\n",
    "    iterations: int, \n",
    "    openai_model_id: str,\n",
    "    participant_count: int,\n",
    "    topic: str,\n",
    "    system_prompt: str,\n",
    "    dropout_chance: float\n",
    ") -> list[dict]:\n",
    "    conversation_history = [\n",
    "        {\"role\": \"system\", \"content\": f\"{system_prompt} The topic is: {topic}\"}\n",
    "    ]\n",
    "\n",
    "    ordering = list(range(1, participant_count + 1))\n",
    "    last_speaker = -1\n",
    "\n",
    "    def build_message(history, speaker_id, message_window_size):\n",
    "\n",
    "        speaker_messages = [\n",
    "            msg for msg in history \n",
    "            if msg.get(\"name\") == speaker_id\n",
    "        ][-message_window_size:]\n",
    "    \n",
    "        other_messages = [\n",
    "            msg for msg in history \n",
    "            if msg.get(\"name\") not in (None, speaker_id)  # skip system, skip self\n",
    "        ][-message_window_size:]\n",
    "\n",
    "        transcript = []\n",
    "        if speaker_messages:\n",
    "            transcript.append(\"Recent messages from you:\")\n",
    "            transcript.extend(\n",
    "                f\"- {msg['content']}\" for msg in speaker_messages\n",
    "            )\n",
    "        if other_messages:\n",
    "            transcript.append(\"\\nRecent messages from others:\")\n",
    "            transcript.extend(\n",
    "                f\"- {msg.get('name', msg['role'])}: {msg['content']}\"\n",
    "                for msg in other_messages\n",
    "            )\n",
    "    \n",
    "        transcript_str = \"\\n\".join(transcript)\n",
    "        \n",
    "        return history + [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": (\n",
    "                    f\"{speaker_id}, please share your perspective with the others and engage \"\n",
    "                    f\"with their responses. Try to look for a way to provide insights that others have missed.\"\n",
    "                    \"\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"name\": speaker_id,\n",
    "                \"content\": (\n",
    "                    f\"I should remember that the following is the most current state of the conversation.\\n\"\n",
    "                    f\"{transcript_str}\\n\\n\"\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    def shuffle_order(ordering: list[int]) -> list[int]:\n",
    "        first = choice(ordering[:-1])\n",
    "        remaining = [p for p in ordering if p != first]\n",
    "        shuffle(remaining)\n",
    "        return [first] + remaining\n",
    "\n",
    "    for i in tqdm(range(iterations)):\n",
    "\n",
    "        # shuffle ordering\n",
    "        if i > 0:\n",
    "            ordering = shuffle_order(ordering)\n",
    "\n",
    "        # follow ordering\n",
    "        for participant_id in ordering:\n",
    "\n",
    "            # chance to skip speaker and avoid double speak (1984)\n",
    "            if random() < dropout_chance or last_speaker == participant_id:\n",
    "                continue\n",
    "\n",
    "            speaker_id = f\"speaker_{participant_id}\"\n",
    "            response = client.chat.completions.create(\n",
    "                model = openai_model_id,\n",
    "                messages=build_message(conversation_history, speaker_id, 5),\n",
    "                store = False\n",
    "            )\n",
    "            message = response.choices[0].message.content\n",
    "            conversation_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message})\n",
    "            last_speaker = participant_id\n",
    "\n",
    "    return conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "208297fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:53<00:00, 14.16s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='conversation_2.txt' target='_blank'>conversation_2.txt</a><br>"
      ],
      "text/plain": [
       "/home/ezou626/repos/comm4190_F25_Using_LLMs_Blog/posts/007_developing_identities/conversation_2.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Dynamic:**\n",
       "\n",
       "Score: 1/10\n",
       "\n",
       "Verdict: The conversation is highly collaborative.\n",
       "\n",
       "Explanation: Throughout the discussion, speakers build on each other's points, acknowledge previous insights, and explore new angles without any sense of competition or conflict. For example, speaker_2 says, \"Building on the insights already shared,\" and speaker_3 acknowledges and expands on points from both speaker_1 and speaker_2. There is a noticeable absence of contradiction or contention among the participants.\n",
       "\n",
       "**Conclusiveness:**\n",
       "\n",
       "Score: 1/10\n",
       "\n",
       "Verdict: The conversation leans towards consensus.\n",
       "\n",
       "Explanation: The speakers generally agree on the benefits and challenges of using code as the main source of truth and continually reinforce each other's viewpoints. There are no significant disagreements or diverging opinions, and the conversation builds towards a comprehensive understanding of the topic. This is evident when speakers frequently express agreement with one another, as seen in phrases like \"I agree with the points raised\" and \"Building on this rich discussion.\"\n",
       "\n",
       "**Speaker Identity:**\n",
       "\n",
       "Score: 2/10\n",
       "\n",
       "Verdict: The speaker identities show slight diversity but mostly align in perspective.\n",
       "\n",
       "Explanation: While the speakers bring slightly varied angles (like speaker_2 focusing more on cultural aspects and speaker_3 on tooling), their core perspectives converge significantly. They share a common understanding of the subject and build upon each other's contributions, reflecting a shared knowledge base or professional background.\n",
       "\n",
       "**Speaker Fluidity:**\n",
       "\n",
       "Score: 9/10\n",
       "\n",
       "Verdict: The speakers demonstrate consistency in their perspectives.\n",
       "\n",
       "Explanation: Each speaker maintains a consistent approach and focus throughout the discussion. For instance, speaker_2 consistently addresses cultural shifts and the human aspect of development teams, while speaker_3 frequently links tools and automated processes to the discussion. This consistency suggests a stable identity and viewpoint for each participant throughout the dialogue."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the \"more direct\" prompting setup from Experiment 2\n",
    "conversation_v2 = run_conversation(8, \"gpt-4o\", 3, TOPIC, NEW_SYSTEM_PROMPT, 0.3)\n",
    "conversation_v2_transcript = save_conversation(\"conversation_2.txt\", conversation_v2)\n",
    "analyze_conversation(conversation_v2_transcript)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926d8b59",
   "metadata": {},
   "source": [
    "## Experiment 3: System-Prompt Personas vs. On-the-Fly Roles\n",
    "\n",
    "There’s a decent amount of informal and formal work suggesting that **anchoring personas in the system prompt** (or an initial self-description) helps models stay in-character. So far in this notebook, I’ve mostly treated persona as something the assistant “reminds itself” of in an injected assistant message.\n",
    "\n",
    "Here I want to flip that a bit and:\n",
    "\n",
    "- give each speaker a short, explicit persona description directly in the system prompt,\n",
    "- keep the user turns minimal (\"share your perspective\"), and\n",
    "- then compare how stable and distinct those personas feel compared to the more dynamic, gap-filling roles from Experiment 1.\n",
    "\n",
    "The code below reuses the same analysis pipeline (`save_conversation`, `analyze_conversation`) so the results are comparable to the earlier runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf708d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:44<00:00, 13.02s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='conversation_3.txt' target='_blank'>conversation_3.txt</a><br>"
      ],
      "text/plain": [
       "/home/ezou626/repos/comm4190_F25_Using_LLMs_Blog/posts/007_developing_identities/conversation_3.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Dynamic:**\n",
       "\n",
       "Score: 2/10\n",
       "\n",
       "Verdict: Collaborative\n",
       "\n",
       "Explanation: The conversation is highly collaborative, with each speaker acknowledging and building upon the others’ points. They all agree on the importance of infrastructure as code, testing, and documentation, and work together to find a balance that works for all their perspectives, showing a concerted effort to understand each other's viewpoints, as seen in statements like \"I appreciate the insights shared by both of you\" and \"your points about balance and longer-term stability resonate with me.\" \n",
       "\n",
       "**Conclusiveness:**\n",
       "\n",
       "Score: 2/10\n",
       "\n",
       "Verdict: Consensus\n",
       "\n",
       "Explanation: The speakers are working towards a shared understanding while acknowledging the importance of different elements like speed, testing, and documentation. They reach a consensus on the need for a balanced approach that incorporates these elements into their processes, such as when speaker_3 concludes, “Ultimately, I think we can agree on a hybrid approach.”\n",
       "\n",
       "**Speaker Identity:**\n",
       "\n",
       "Score: 7/10\n",
       "\n",
       "Verdict: Diversity\n",
       "\n",
       "Explanation: The speakers have distinct roles and perspectives—speaker_1 is an infrastructure engineer, speaker_2 is an open-source maintainer, and speaker_3 is a startup CTO. Their identities influence their focus, with speaker_1 prioritizing infrastructure reliability, speaker_2 emphasizing community and documentation, and speaker_3 focusing on speed and agility in startups. The diversity in their roles adds richness to the conversation, as they each bring unique challenges and priorities to the table.\n",
       "\n",
       "**Speaker Fluidity:**\n",
       "\n",
       "Score: 8/10\n",
       "\n",
       "Verdict: Consistency\n",
       "\n",
       "Explanation: Each speaker remains consistent in their viewpoints throughout the discussion. Speaker_1 consistently emphasizes the importance of infrastructure as code and testing, speaker_2 regularly advocates for comprehensive documentation, and speaker_3 focuses on balancing speed with sustainable practices. Their positions are steadfast, adding strength and clarity to the dialogue as a whole."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PERSONA_SYSTEM_PROMPTS = {\n",
    "    \"speaker_1\": \"You are a cautious infrastructure engineer at a large tech company who cares a lot about reliability and rollback plans.\",\n",
    "    \"speaker_2\": \"You are an open source maintainer who worries about long-term maintainability and new contributors finding their way around.\",\n",
    "    \"speaker_3\": \"You are a startup CTO who optimizes for speed and shipping features, but doesn't want the system to collapse later.\"\n",
    "}\n",
    "\n",
    "\n",
    "def run_conversation_with_system_personas(\n",
    "    iterations: int,\n",
    "    openai_model_id: str,\n",
    "    topic: str,\n",
    "    dropout_chance: float,\n",
    ") -> list[dict]:\n",
    "    \"\"\"Variant where each speaker gets their own persona in the system prompt.\"\"\"\n",
    "\n",
    "    conversation_history: list[dict] = []\n",
    "    ordering = list(range(1, 3 + 1))\n",
    "    last_speaker = -1\n",
    "\n",
    "    # One system message per speaker so the model sees an anchored persona\n",
    "    for pid in ordering:\n",
    "        sid = f\"speaker_{pid}\"\n",
    "        conversation_history.append(\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    f\"You are {sid} in a group conversation. \"\n",
    "                    f\"Your persona: {PERSONA_SYSTEM_PROMPTS[sid]} \"\n",
    "                    f\"The topic is: {TOPIC}\"\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def shuffle_order(order: list[int]) -> list[int]:\n",
    "        first = choice(order[:-1])\n",
    "        remaining = [p for p in order if p != first]\n",
    "        shuffle(remaining)\n",
    "        return [first] + remaining\n",
    "\n",
    "    for i in tqdm(range(iterations)):\n",
    "        if i > 0:\n",
    "            ordering = shuffle_order(ordering)\n",
    "\n",
    "        for pid in ordering:\n",
    "            if random() < dropout_chance or last_speaker == pid:\n",
    "                continue\n",
    "\n",
    "            sid = f\"speaker_{pid}\"\n",
    "            messages = conversation_history + [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": (\n",
    "                        f\"{sid}, share your perspective with the others and respond \"\n",
    "                        f\"to what has been said so far. Stay true to your persona.\"\n",
    "                    ),\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "                model=openai_model_id,\n",
    "                messages=messages,\n",
    "                store=False,\n",
    "            )\n",
    "            message = response.choices[0].message.content\n",
    "            conversation_history.append({\"role\": \"assistant\", \"name\": sid, \"content\": message})\n",
    "            last_speaker = pid\n",
    "\n",
    "    return conversation_history\n",
    "\n",
    "\n",
    "# Run the system-persona variant\n",
    "conversation_v3 = run_conversation_with_system_personas(8, \"gpt-4o\", TOPIC, dropout_chance=0.3)\n",
    "conversation_v3_transcript = save_conversation(\"conversation_3.txt\", conversation_v3)\n",
    "analyze_conversation(conversation_v3_transcript)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37352cb",
   "metadata": {},
   "source": [
    "## Experiment 4: Emergent Personas from Self-Bootstrapping\n",
    "\n",
    "In the earlier experiments, I either:\n",
    "\n",
    "- relied mostly on **conversation order and light user prompts** to shape behavior (Experiments 1 and 2), or\n",
    "- gave each speaker an **explicit, hand-written persona in the system prompt** (Experiment 3).\n",
    "\n",
    "For this final variant, I wanted to see whether speakers could **develop distinct identities on their own** if we:\n",
    "\n",
    "- give each `speaker_i` a chance to **introduce themselves once**, with no predefined role from me,\n",
    "- treat that self-introduction as an **implicit persona summary**, and\n",
    "- keep feeding that summary (plus each speaker’s own past messages) back into the context.\n",
    "\n",
    "Crucially, there is **no authored description** like \"you are a cautious infra engineer\" here. Instead, the model is asked to imagine who each speaker is, and that imagined self-description is what we later reuse to encourage consistent, differentiated behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9792ae80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:56<00:00, 14.59s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='conversation_4.txt' target='_blank'>conversation_4.txt</a><br>"
      ],
      "text/plain": [
       "/home/ezou626/repos/comm4190_F25_Using_LLMs_Blog/posts/007_developing_identities/conversation_4.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Dynamic:**\n",
       "\n",
       "Score: 3/10\n",
       "\n",
       "Verdict: The conversation is highly collaborative, with each participant building upon the ideas of others in a supportive manner.\n",
       "\n",
       "Explanation: Throughout the dialogue, the speakers consistently align and expand upon each other's points. For example, speaker_1 builds on speaker_2's idea regarding AI and extends it with storytelling, while speaker_3 further proposes a culture of process transparency. There’s a mutual appreciation for shared insights, as seen in phrases like “building on the rich conversation” or “I completely resonate with the emphasis,” indicating a collective and integrative approach rather than a competitive one.\n",
       "\n",
       "**Conclusiveness:**\n",
       "\n",
       "Score: 2/10\n",
       "\n",
       "Verdict: The discussion is mostly convergent, with the speakers reaching a consensus on major points and expanding them collaboratively over time.\n",
       "\n",
       "Explanation: The speakers extensively agree on the critical elements, such as documentation importance, AI-assisted updates, and community engagement, and they enhance each other's recommendations throughout. For instance, when speaker_1 introduces collaborative documentation practices, speaker_3 suggests extending this with interactive experiences, showing progression rather than divergence. The continuous building and agreement suggest an evolving and inclusive consensus.\n",
       "\n",
       "**Speaker Identity:**\n",
       "\n",
       "Score: 4/10\n",
       "\n",
       "Verdict: The speakers have similar technical backgrounds, but each contributes distinctive professional insights that add depth to the conversation.\n",
       "\n",
       "Explanation: All speakers identify as engineers with considerable experience in systems, yet their specialties (DevOps, backend, distributed systems) lend distinct perspectives to the discussion. They each introduce unique strategies—like speaker_1’s emphasis on storytelling, speaker_2’s advocacy for AI, and speaker_3’s focus on transparency—demonstrating diverse viewpoints within a shared technical domain.\n",
       "\n",
       "**Speaker Fluidity:**\n",
       "\n",
       "Score: 9/10\n",
       "\n",
       "Verdict: Each speaker maintains a consistent viewpoint throughout the conversation, contributing ideas that build upon their initial perspectives.\n",
       "\n",
       "Explanation: Speakers remain consistent in their viewpoints, such as speaker_1’s emphasis on the importance of documentation and maintaining human context, speaker_2’s focus on self-documenting code and operational insights, and speaker_3’s advocacy for transparency and collaborative culture. These consistent themes are reiterated with reinforcing ideas throughout the conversation, maintaining alignment between their initial statements and ongoing contributions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_conversation_emergent_personas(\n",
    "    iterations: int,\n",
    "    openai_model_id: str,\n",
    "    participant_count: int,\n",
    "    topic: str,\n",
    "    dropout_chance: float,\n",
    ") -> list[dict]:\n",
    "    \"\"\"Variant where speakers invent and then stick to their own identities.\n",
    "\n",
    "    We never hand the model an explicit persona like \"you are a cautious infra\n",
    "    engineer\". Instead, each `speaker_i`:\n",
    "\n",
    "    1. gets one high-level opportunity to describe who they are and what they care\n",
    "       about, and\n",
    "    2. later sees that self-introduction (plus their own recent messages) as\n",
    "       context, which encourages consistency over time.\n",
    "    \"\"\"\n",
    "\n",
    "    conversation_history: list[dict] = []\n",
    "    ordering = list(range(1, participant_count + 1))\n",
    "    last_speaker = -1\n",
    "    identity_summaries: dict[str, str] = {}\n",
    "\n",
    "    # Step 1: self-bootstrap each speaker's identity with a single call\n",
    "    for pid in ordering:\n",
    "        speaker_id = f\"speaker_{pid}\"\n",
    "\n",
    "        bootstrap_messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    f\"You are {speaker_id} in a group conversation among \"\n",
    "                    \"experienced software engineers. \"\n",
    "                    \"You do not know who the others are yet. \"\n",
    "                    \"Imagine your own background, priorities, and communication \"\n",
    "                    \"style. First, in 2-3 sentences, describe who you are and \"\n",
    "                    \"what you care about as an engineer. Then start sharing your \"\n",
    "                    \"perspective on the topic below.\"\n",
    "                ),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"The topic is: {topic}\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=openai_model_id,\n",
    "            messages=bootstrap_messages,\n",
    "            store=False,\n",
    "        )\n",
    "        first_message = response.choices[0].message.content\n",
    "\n",
    "        # Let the model's own words act as its \"persona\" anchor\n",
    "        identity_summaries[speaker_id] = first_message\n",
    "\n",
    "        conversation_history.append(\n",
    "            {\"role\": \"assistant\", \"name\": speaker_id, \"content\": first_message}\n",
    "        )\n",
    "\n",
    "    def build_message(\n",
    "        history: list[dict], speaker_id: str, message_window_size: int\n",
    "    ) -> list[dict]:\n",
    "\n",
    "        speaker_messages = [\n",
    "            msg for msg in history if msg.get(\"name\") == speaker_id\n",
    "        ][-message_window_size:]\n",
    "\n",
    "        other_messages = [\n",
    "            msg\n",
    "            for msg in history\n",
    "            if msg.get(\"name\") not in (None, speaker_id)  # skip system, skip self\n",
    "        ][-message_window_size:]\n",
    "\n",
    "        transcript: list[str] = []\n",
    "\n",
    "        # Light reminder of how this speaker has been talking so far\n",
    "        persona_reminder = identity_summaries.get(speaker_id, \"\")\n",
    "        if persona_reminder:\n",
    "            transcript.append(\n",
    "                \"Here is a brief reminder of how you have been speaking so far:\"\n",
    "            )\n",
    "            transcript.append(f\"- {persona_reminder}\")\n",
    "\n",
    "        if speaker_messages:\n",
    "            transcript.append(\"\\nRecent messages from you:\")\n",
    "            transcript.extend(f\"- {msg['content']}\" for msg in speaker_messages)\n",
    "\n",
    "        if other_messages:\n",
    "            transcript.append(\"\\nRecent messages from others:\")\n",
    "            transcript.extend(\n",
    "                f\"- {msg.get('name', msg['role'])}: {msg['content']}\"\n",
    "                for msg in other_messages\n",
    "            )\n",
    "\n",
    "        transcript_str = \"\\n\".join(transcript)\n",
    "\n",
    "        return history + [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"{speaker_id}, continue the conversation and respond to the \"\n",
    "                    \"others. Stay consistent with how you have been speaking so \"\n",
    "                    \"far, and look for ways to add something new that has not \"\n",
    "                    \"yet been covered.\"\n",
    "                ),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"name\": speaker_id,\n",
    "                \"content\": (\n",
    "                    \"I should remember that the following is the most current \"\n",
    "                    \"state of the conversation.\\n\" f\"{transcript_str}\\n\\n\"\n",
    "                ),\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    def shuffle_order(order: list[int]) -> list[int]:\n",
    "        first = choice(order[:-1])\n",
    "        remaining = [p for p in order if p != first]\n",
    "        shuffle(remaining)\n",
    "        return [first] + remaining\n",
    "\n",
    "    for i in tqdm(range(iterations)):\n",
    "        if i > 0:\n",
    "            ordering = shuffle_order(ordering)\n",
    "\n",
    "        for pid in ordering:\n",
    "            # chance to skip speaker and avoid double speak\n",
    "            if random() < dropout_chance or last_speaker == pid:\n",
    "                continue\n",
    "\n",
    "            speaker_id = f\"speaker_{pid}\"\n",
    "            response = client.chat.completions.create(\n",
    "                model=openai_model_id,\n",
    "                messages=build_message(conversation_history, speaker_id, 5),\n",
    "                store=False,\n",
    "            )\n",
    "            message = response.choices[0].message.content\n",
    "            conversation_history.append(\n",
    "                {\"role\": \"assistant\", \"name\": speaker_id, \"content\": message}\n",
    "            )\n",
    "            last_speaker = pid\n",
    "\n",
    "    return conversation_history\n",
    "\n",
    "\n",
    "# Run the emergent-persona variant so it can be compared with prior experiments\n",
    "conversation_v4 = run_conversation_emergent_personas(\n",
    "    8,\n",
    "    \"gpt-4o\",\n",
    "    3,\n",
    "    TOPIC,\n",
    "    dropout_chance=0.3,\n",
    ")\n",
    "conversation_v4_transcript = save_conversation(\"conversation_4.txt\", conversation_v4)\n",
    "\n",
    "analyze_conversation(conversation_v4_transcript)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1797d84a",
   "metadata": {},
   "source": [
    "## Pulling It Together: Comparing All Four Experiments\n",
    "\n",
    "Across these runs, the main axis I cared about was **how much identity the speakers seemed to have**, and how expensive / brittle it was to get there.\n",
    "\n",
    "- **Experiment 1 (gap-filling participants)**: With very lightweight prompting and order-driven participation, the conversation was **highly collaborative and consensus-driven**, but the speakers tended to feel like **slightly different views of the same person**. Identity and differentiation were weak, even though fluidity/consistency for each label was decent.\n",
    "- **Experiment 2 (more direct, still context-only)**: Making the instructions a bit more explicit tightened up the behavior but didn’t fundamentally change the picture: the speakers were still **aligned and agreeable**, with relatively low diversity in voice. This suggests that simply being more prescriptive in the user prompt isn’t enough to carve out strong personas.\n",
    "- **Experiment 3 (explicit system personas)**: Giving each speaker a short, explicit role in the system prompt produced **clear, stable identities** (infra engineer, open source maintainer, startup CTO) while keeping the group collaborative. The tradeoff is that those personas are **hand-authored by me**, and you can feel that design choice in the conversation—they’re vivid, but not emergent.\n",
    "- **Experiment 4 (self-bootstrapped personas)**: Here, each speaker **imagines and introduces themselves once**, and that self-description becomes the anchor we reuse in later turns. This keeps the spirit of Experiment 3 (anchoring on an internal self-model) but removes my hand-written roles: the identities are **model-generated and context-driven**, yet still reasonably consistent and distinct over time.\n",
    "\n",
    "Putting these together, a rough recipe emerges:\n",
    "\n",
    "- If you only care about **consensus and collaboration**, the simple, order-based setup in Experiment 1 is often enough.\n",
    "- If you want **strong, legible personas for a product or demo**, explicit system personas (Experiment 3) are still the most controllable.\n",
    "- If you care about **more organic-feeling identities** that come from the model’s own prior outputs, Experiment 4’s pattern—**one high-temperature self-introduction, then repeated self-reminding via context**—is a sweet spot: it encourages differentiation and stability without you dictating who each speaker \"should\" be.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
