{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Book Review 3\"\n",
        "description: \"Section 3\"\n",
        "author: \"Eric Zou\"\n",
        "date: \"10/5/2025\"\n",
        "categories:\n",
        "  - Review\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What It's About\n",
        "\n",
        "This section of the book touches on one of the core debates about modern LLMs, whether or not they are actually thinking. It covers the stochastic parrot argument, that LLMs are just next word predictors, counterexamples to LLM brilliance, and fast and slow thinking.\n",
        "\n",
        "## What I Thought\n",
        "\n",
        "I think this is a super interesting discussion that people will continue to have over the next few years as the technology continues to mature. To me, it seems that while LLMs are capable of demonstrating logic and reasoning in their outputs, ultimately it's tough to class the LLM itself as a true thinking being due to its inflexibility and the paradigm that modern systems use to give them this thinking ability (namely, generating a stream of tokens). There are evidently strategies that help LLMs achieve better performance in certain kinds of tasks, but from recent benchmark fatigue observed in new model releases, it seems the tradeoff of the power and fragility of new methods is growing more balanced, so I don't think we'll be able to scale LLMs to some sort of AGI. \n",
        "\n",
        "## Reflecting\n",
        "\n",
        "I think when I work with LLMs in this blog, I think it's my job to go back to understanding the true problem/situation we want to place an agent in to imitate intelligence, and this lends itself to strategies in how to use the LLM to achieve this purpose. This might involve some specific way of structuring the prompt/context, parsing the output, or which model I select. Ultimately though, these methods are likely not transferable across different tasks, and combining them into a system may not be feasible. Even if it was, this amalgam is also not guaranteed to generalize in the way that we might want to other tasks. "
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
