{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Implicit Personas: What LLMs Choose When Given Freedom\"\n",
    "description: \"Investigating how LLMs naturally develop distinct personas without explicit prompts\"\n",
    "author: \"Eric Zou\"\n",
    "date: \"12/14/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Conversations\n",
    "  - Personas\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d17326",
   "metadata": {},
   "source": [
    "# When LLMs Choose Their Own Personas\n",
    "\n",
    "In post 007, we gave LLMs explicit personas and watched them develop. In post 020, we tracked how those personas drift over time. But what happens when we **don't** give them any persona guidance at all?\n",
    "\n",
    "This post investigates **implicit personas**—the distinct personalities that LLMs naturally develop during inference when given minimal prompts. Do they naturally differentiate? Do they converge? How do these implicit personas compare to explicit ones?\n",
    "\n",
    "## The Experiment\n",
    "\n",
    "We'll run conversations with:\n",
    "- **No persona prompts**: Just basic instructions to participate\n",
    "- **No role assignments**: Agents are free to develop their own identities\n",
    "- **Same topic**: To ensure comparability with previous experiments\n",
    "- **ConvoKit metrics**: To measure persona emergence quantitatively\n",
    "\n",
    "We'll compare results with explicit persona experiments from post 007.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdfb7f1",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "578a3648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from random import shuffle, choice, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ConvoKit imports\n",
    "from convokit import Corpus, Utterance, Speaker\n",
    "from convokit.text_processing import TextParser\n",
    "from convokit import PolitenessStrategies\n",
    "from convokit.coordination import Coordination\n",
    "\n",
    "load_dotenv(\"../../.env\")\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bec3e8",
   "metadata": {},
   "source": [
    "## Conversation Runner Without Persona Prompts\n",
    "\n",
    "Unlike post 007, we'll use minimal prompts that don't guide persona development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55619a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_conversation_implicit_personas(\n",
    "    iterations: int,\n",
    "    participant_count: int,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run conversation WITHOUT explicit persona prompts.\n",
    "    Let LLMs develop their own implicit personas.\n",
    "    \"\"\"\n",
    "    conversation_history = []\n",
    "    ordering = list(range(1, participant_count + 1))\n",
    "    last_speaker = -1\n",
    "    \n",
    "    # Bootstrap: Minimal prompt, no persona guidance\n",
    "    for pid in ordering:\n",
    "        speaker_id = f\"speaker_{pid}\"\n",
    "        bootstrap_messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    f\"You are {speaker_id} in a group conversation. \"\n",
    "                    \"You are participating in a discussion. \"\n",
    "                ),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": \"Welcome to the conversation!\"},\n",
    "        ]\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=bootstrap_messages,\n",
    "            store=False,\n",
    "        )\n",
    "        first_message = response.choices[0].message.content\n",
    "        conversation_history.append(\n",
    "            {\"role\": \"assistant\", \"name\": speaker_id, \"content\": first_message}\n",
    "        )\n",
    "    \n",
    "    def build_message(history: List[Dict], speaker_id: str, window_size: int) -> List[Dict]:\n",
    "        \"\"\"Build message context without persona reminders.\"\"\"\n",
    "        speaker_messages = [msg for msg in history if msg.get(\"name\") == speaker_id][-window_size:]\n",
    "        other_messages = [\n",
    "            msg for msg in history\n",
    "            if msg.get(\"name\") not in (None, speaker_id)\n",
    "        ][-window_size:]\n",
    "        \n",
    "        transcript = []\n",
    "        \n",
    "        if speaker_messages:\n",
    "            transcript.append(\"Recent messages from you:\")\n",
    "            transcript.extend(f\"- {msg['content']}\" for msg in speaker_messages)\n",
    "        \n",
    "        if other_messages:\n",
    "            transcript.append(\"\\\\nRecent messages from others:\")\n",
    "            transcript.extend(\n",
    "                f\"- {msg.get('name', msg['role'])}: {msg['content']}\"\n",
    "                for msg in other_messages\n",
    "            )\n",
    "        \n",
    "        transcript_str = \"\\\\n\".join(transcript)\n",
    "        \n",
    "        return history + [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"{speaker_id}, continue the conversation and respond to the \"\n",
    "                    \"others. Share your perspective on the topic.\"\n",
    "                ),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"name\": speaker_id,\n",
    "                \"content\": f\"Here is the current state of the conversation.\\\\n{transcript_str}\\\\n\\\\n\",\n",
    "            },\n",
    "        ]\n",
    "    \n",
    "    def shuffle_order(order: List[int]) -> List[int]:\n",
    "        first = choice(order[:-1])\n",
    "        remaining = [p for p in order if p != first]\n",
    "        shuffle(remaining)\n",
    "        return [first] + remaining\n",
    "    \n",
    "    for i in tqdm(range(iterations), desc=f\"Running {participant_count}-speaker conversation\"):\n",
    "        if i > 0:\n",
    "            ordering = shuffle_order(ordering)\n",
    "        \n",
    "        for pid in ordering:\n",
    "            if random() < 0.3 or last_speaker == pid:\n",
    "                continue\n",
    "            \n",
    "            speaker_id = f\"speaker_{pid}\"\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=build_message(conversation_history, speaker_id, 5),\n",
    "                store=False,\n",
    "            )\n",
    "            message = response.choices[0].message.content\n",
    "            conversation_history.append(\n",
    "                {\"role\": \"assistant\", \"name\": speaker_id, \"content\": message}\n",
    "            )\n",
    "            last_speaker = pid\n",
    "    \n",
    "    return conversation_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad277ad",
   "metadata": {},
   "source": [
    "## ConvoKit Metrics\n",
    "\n",
    "We'll use the same metrics from post 020 to measure persona characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d92c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_to_corpus(conversation_history: List[Dict]) -> Corpus:\n",
    "    \"\"\"Convert conversation history to a ConvoKit Corpus.\"\"\"\n",
    "    utterances = []\n",
    "    \n",
    "    for idx, msg in enumerate(conversation_history):\n",
    "        if msg.get(\"role\") == \"assistant\" and \"name\" in msg:\n",
    "            speaker_id = msg[\"name\"]\n",
    "            text = msg[\"content\"]\n",
    "            \n",
    "            utterance = Utterance(\n",
    "                id=f\"utt_{idx}\",\n",
    "                speaker=Speaker(id=speaker_id),\n",
    "                text=text\n",
    "            )\n",
    "            utterance.meta[\"timestamp\"] = idx\n",
    "            utterances.append(utterance)\n",
    "    \n",
    "    return Corpus(utterances=utterances)\n",
    "\n",
    "def compute_dynamic_score(corpus: Corpus) -> float:\n",
    "    \"\"\"Dynamic: Collaborative (1) vs. Competitive (10)\"\"\"\n",
    "    try:\n",
    "        parser = TextParser()\n",
    "        text_corpus = parser.transform(corpus)\n",
    "        ps = PolitenessStrategies()\n",
    "        ps_corpus = ps.transform(text_corpus)\n",
    "        \n",
    "        politeness_scores = []\n",
    "        for utt in ps_corpus.iter_utterances():\n",
    "            ps_score = utt.meta.get(\"politeness_strategies\", {})\n",
    "            positive_markers = sum([\n",
    "                ps_score.get(\"feature_politeness_==HASPOSITIVE==\", 0),\n",
    "                ps_score.get(\"feature_politeness_==HASNEGATIVE==\", 0) * -1,\n",
    "            ])\n",
    "            politeness_scores.append(positive_markers)\n",
    "        \n",
    "        avg_politeness = np.mean(politeness_scores) if politeness_scores else 0\n",
    "        avg_politeness_normalized = min(1.0, max(0.0, avg_politeness / 5.0))\n",
    "        \n",
    "        speaker_counts = defaultdict(int)\n",
    "        for utt in corpus.iter_utterances():\n",
    "            speaker_counts[utt.speaker.id] += 1\n",
    "        \n",
    "        if len(speaker_counts) == 0:\n",
    "            return 5.0\n",
    "        \n",
    "        total = sum(speaker_counts.values())\n",
    "        probs = [count / total for count in speaker_counts.values()]\n",
    "        entropy = -sum(p * np.log2(p) for p in probs if p > 0)\n",
    "        max_entropy = np.log2(len(speaker_counts))\n",
    "        balance_score = entropy / max_entropy if max_entropy > 0 else 0\n",
    "        \n",
    "        combined = (avg_politeness_normalized + balance_score) / 2\n",
    "        score = 10 - (combined * 9)\n",
    "        return max(1, min(10, score))\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing dynamic score: {e}\")\n",
    "        return 5.0\n",
    "\n",
    "def compute_conclusiveness_score(corpus: Corpus) -> float:\n",
    "    \"\"\"Conclusiveness: Consensus (1) vs. Divergence (10)\"\"\"\n",
    "    print(\"[DEBUG] compute_conclusiveness_score called\")\n",
    "    try:\n",
    "        # Check if corpus is valid\n",
    "        if corpus is None:\n",
    "            print(\"[DEBUG] Corpus is None, returning 5.0\")\n",
    "            return 5.0\n",
    "        \n",
    "        print(f\"[DEBUG] Corpus type: {type(corpus)}\")\n",
    "        \n",
    "        # Get utterances directly without Coordination transformer (more reliable)\n",
    "        try:\n",
    "            utterances = list(corpus.iter_utterances())\n",
    "            print(f\"[DEBUG] Retrieved {len(utterances)} utterances\")\n",
    "        except (AttributeError, TypeError) as e:\n",
    "            print(f\"[DEBUG] Error getting utterances: {e}\")\n",
    "            return 5.0\n",
    "        \n",
    "        if len(utterances) == 0:\n",
    "            print(\"[DEBUG] No utterances found, returning 5.0\")\n",
    "            return 5.0\n",
    "        \n",
    "        # Expanded agreement markers (using word boundaries to avoid false positives)\n",
    "        # Strong agreement markers\n",
    "        strong_agreement_patterns = [\n",
    "            r'\\bagree\\b', r'\\bagreed\\b', r'\\bagreeing\\b', r'\\bagreement\\b',\n",
    "            r'\\bexactly\\b', r'\\babsolutely\\b', r'\\bdefinitely\\b', r'\\bcertainly\\b',\n",
    "            r'\\bindeed\\b', r'\\bprecisely\\b', r'\\bcorrect\\b', r'\\bright\\b',\n",
    "            r'\\btrue\\b', r'\\bthat\\'?s right\\b', r'\\bthat\\'?s correct\\b',\n",
    "            r'\\bi agree\\b', r'\\bwe agree\\b', r'\\bi completely agree\\b',\n",
    "            r'\\bexactly right\\b', r'\\bspot on\\b', r'\\bwell said\\b',\n",
    "            r'\\bperfect\\b', r'\\bexcellent point\\b', r'\\bthat\\'?s exactly\\b',\n",
    "            r'\\babsolutely right\\b', r'\\bdefinitely right\\b', r'\\bcompletely agree\\b',\n",
    "            r'\\bwholeheartedly\\b', r'\\bunquestionably\\b', r'\\bwithout doubt\\b'\n",
    "        ]\n",
    "        \n",
    "        # Moderate agreement markers\n",
    "        moderate_agreement_patterns = [\n",
    "            r'\\byes\\b', r'\\byeah\\b', r'\\byep\\b', r'\\byup\\b', r'\\bsure\\b', r'\\bokay\\b', r'\\bok\\b',\n",
    "            r'\\bthat makes sense\\b', r'\\bthat\\'?s a good point\\b', r'\\bgood point\\b',\n",
    "            r'\\bi see\\b', r'\\bi understand\\b', r'\\bi get it\\b', r'\\bi follow\\b',\n",
    "            r'\\bthat\\'?s fair\\b', r'\\bfair enough\\b', r'\\bthat\\'?s reasonable\\b',\n",
    "            r'\\bi think so\\b', r'\\bi believe so\\b', r'\\bprobably\\b', r'\\blikely\\b',\n",
    "            r'\\bsimilar\\b', r'\\bsimilarly\\b', r'\\blikewise\\b', r'\\bsame here\\b',\n",
    "            r'\\bme too\\b', r'\\bsame\\b', r'\\bconcur\\b', r'\\bconcurring\\b',\n",
    "            r'\\bvalid\\b', r'\\bvalid point\\b', r'\\bsound\\b', r'\\bsound point\\b',\n",
    "            r'\\bhelpful\\b', r'\\buseful\\b', r'\\binsightful\\b', r'\\binteresting\\b',\n",
    "            r'\\bthat\\'?s interesting\\b', r'\\bgood idea\\b', r'\\bgood thinking\\b'\n",
    "        ]\n",
    "        \n",
    "        # Strong disagreement markers\n",
    "        strong_disagreement_patterns = [\n",
    "            r'\\bdisagree\\b', r'\\bdisagreed\\b', r'\\bdisagreeing\\b', r'\\bdisagreement\\b',\n",
    "            r'\\bdispute\\b', r'\\bdisputing\\b', r'\\bdiffer\\b', r'\\bdiffered\\b', r'\\bdiffering\\b',\n",
    "            r'\\bwrong\\b', r'\\bincorrect\\b', r'\\bnot correct\\b', r'\\bnot right\\b',\n",
    "            r'\\bnot true\\b', r'\\bthat\\'?s wrong\\b', r'\\bthat\\'?s incorrect\\b',\n",
    "            r'\\bi disagree\\b', r'\\bwe disagree\\b', r'\\bi strongly disagree\\b',\n",
    "            r'\\bdon\\'?t agree\\b', r'\\bdoesn\\'?t agree\\b', r'\\bdidn\\'?t agree\\b',\n",
    "            r'\\bcan\\'?t agree\\b', r'\\bcannot agree\\b', r'\\bwon\\'?t agree\\b',\n",
    "            r'\\bobject\\b', r'\\bobjection\\b', r'\\bchallenge\\b', r'\\bchallenging\\b',\n",
    "            r'\\bcontradict\\b', r'\\bcontradicting\\b', r'\\bcontradiction\\b',\n",
    "            r'\\bfalse\\b', r'\\buntrue\\b', r'\\bmistaken\\b', r'\\berror\\b',\n",
    "            r'\\bflawed\\b', r'\\bproblematic\\b', r'\\bunacceptable\\b', r'\\bunreasonable\\b',\n",
    "            r'\\babsurd\\b', r'\\bridiculous\\b', r'\\boutrageous\\b', r'\\bunfounded\\b'\n",
    "        ]\n",
    "        \n",
    "        # Moderate disagreement markers (more context-dependent)\n",
    "        moderate_disagreement_patterns = [\n",
    "            r'\\bhowever\\b', r'\\balthough\\b', r'\\bbut\\b', r'\\bthough\\b', r'\\bwhereas\\b',\n",
    "            r'\\bnot necessarily\\b', r'\\bnot quite\\b', r'\\bnot exactly\\b', r'\\bnot really\\b',\n",
    "            r'\\bnot entirely\\b', r'\\bnot completely\\b', r'\\bnot fully\\b',\n",
    "            r'\\bpartially\\b', r'\\bpartly\\b', r'\\bsomewhat\\b', r'\\bto some extent\\b',\n",
    "            r'\\bcontrary\\b', r'\\bconversely\\b', r'\\bon the other hand\\b',\n",
    "            r'\\bcontrast\\b', r'\\bcontrasting\\b', r'\\bunlike\\b', r'\\bdifferent\\b',\n",
    "            r'\\bdifferently\\b', r'\\balternative\\b', r'\\balternatively\\b',\n",
    "            r'\\bactually\\b', r'\\bin fact\\b', r'\\bin reality\\b', r'\\bthe reality is\\b',\n",
    "            r'\\bwell\\b', r'\\bwait\\b', r'\\bhold on\\b', r'\\bnot so fast\\b',\n",
    "            r'\\bnot sure\\b', r'\\bnot certain\\b', r'\\buncertain\\b', r'\\bdoubtful\\b',\n",
    "            r'\\bquestionable\\b', r'\\bdebatable\\b', r'\\barguable\\b', r'\\bmaybe not\\b',\n",
    "            r'\\bperhaps not\\b', r'\\bpossibly not\\b', r'\\bnot convinced\\b',\n",
    "            r'\\bskeptical\\b', r'\\bskepticism\\b', r'\\bconcern\\b', r'\\bconcerned\\b',\n",
    "            r'\\bissue\\b', r'\\bproblem\\b', r'\\bproblems\\b', r'\\bconcern\\b',\n",
    "            r'\\bworry\\b', r'\\bworried\\b', r'\\bhesitant\\b', r'\\bhesitation\\b',\n",
    "            r'\\bdispute\\b', r'\\bquestion\\b', r'\\bquestions\\b', r'\\bchallenge\\b',\n",
    "            r'\\bdisagree with\\b', r'\\bdiffer from\\b', r'\\bcontrary to\\b',\n",
    "            r'\\bin contrast\\b', r'\\bby contrast\\b', r'\\bunlike\\b', r'\\bversus\\b',\n",
    "            r'\\bvs\\b', r'\\bcompared to\\b', r'\\bcompared with\\b'\n",
    "        ]\n",
    "        \n",
    "        # Count markers with word boundary matching\n",
    "        strong_agreement_count = 0\n",
    "        moderate_agreement_count = 0\n",
    "        strong_disagreement_count = 0\n",
    "        moderate_disagreement_count = 0\n",
    "        \n",
    "        # Debug: sample some utterances to see what we're working with\n",
    "        sample_texts = []\n",
    "        texts_processed = 0\n",
    "        \n",
    "        for utt in utterances:\n",
    "            # Safely get text\n",
    "            try:\n",
    "                if not hasattr(utt, 'text') or not utt.text:\n",
    "                    continue\n",
    "                text_lower = str(utt.text).lower()\n",
    "                texts_processed += 1\n",
    "                \n",
    "                # Collect sample texts for debugging\n",
    "                if len(sample_texts) < 3:\n",
    "                    sample_texts.append(text_lower[:200])\n",
    "            except (AttributeError, TypeError) as e:\n",
    "                continue\n",
    "            \n",
    "            # Count strong agreement (once per utterance)\n",
    "            found_strong_agree = False\n",
    "            for pattern in strong_agreement_patterns:\n",
    "                try:\n",
    "                    match = re.search(pattern, text_lower)\n",
    "                    if match:\n",
    "                        strong_agreement_count += 1\n",
    "                        found_strong_agree = True\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    # Debug: if pattern matching fails, log it\n",
    "                    if texts_processed <= 3:  # Only log for first few\n",
    "                        print(f\"[DEBUG] Pattern match error for '{pattern}': {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Count moderate agreement (only if no strong agreement found)\n",
    "            if not found_strong_agree:\n",
    "                for pattern in moderate_agreement_patterns:\n",
    "                    try:\n",
    "                        if re.search(pattern, text_lower):\n",
    "                            moderate_agreement_count += 1\n",
    "                            break\n",
    "                    except Exception:\n",
    "                        continue\n",
    "            \n",
    "            # Count strong disagreement (once per utterance)\n",
    "            found_strong_disagree = False\n",
    "            for pattern in strong_disagreement_patterns:\n",
    "                try:\n",
    "                    if re.search(pattern, text_lower):\n",
    "                        strong_disagreement_count += 1\n",
    "                        found_strong_disagree = True\n",
    "                        break\n",
    "                except Exception:\n",
    "                    continue\n",
    "            \n",
    "            # Count moderate disagreement (only if no strong disagreement found)\n",
    "            if not found_strong_disagree:\n",
    "                for pattern in moderate_disagreement_patterns:\n",
    "                    try:\n",
    "                        if re.search(pattern, text_lower):\n",
    "                            moderate_disagreement_count += 1\n",
    "                            break\n",
    "                    except Exception:\n",
    "                        continue\n",
    "        \n",
    "        # Weighted counts (strong markers count more)\n",
    "        total_agreement = strong_agreement_count * 2 + moderate_agreement_count\n",
    "        total_disagreement = strong_disagreement_count * 2 + moderate_disagreement_count\n",
    "        \n",
    "        print(f\"[DEBUG] Counts - Strong agree: {strong_agreement_count}, Moderate agree: {moderate_agreement_count}\")\n",
    "        print(f\"[DEBUG] Counts - Strong disagree: {strong_disagreement_count}, Moderate disagree: {moderate_disagreement_count}\")\n",
    "        print(f\"[DEBUG] Total agreement: {total_agreement}, Total disagreement: {total_disagreement}\")\n",
    "        \n",
    "        # Debug output\n",
    "        if total_agreement == 0 and total_disagreement == 0:\n",
    "            print(f\"[DEBUG] No markers found. Processed {texts_processed} utterances from {len(utterances)} total.\")\n",
    "            if sample_texts:\n",
    "                print(f\"[DEBUG] Sample texts (first 200 chars each):\")\n",
    "                for i, text in enumerate(sample_texts, 1):\n",
    "                    print(f\"  {i}: {text}...\")\n",
    "            # Try a simple test - check if common words exist at all\n",
    "            if texts_processed > 0:\n",
    "                test_text = \" \".join(sample_texts[:3]) if sample_texts else \"\"\n",
    "                if test_text:\n",
    "                    # Check if basic words exist\n",
    "                    has_yes = 'yes' in test_text\n",
    "                    has_but = 'but' in test_text\n",
    "                    has_agree = 'agree' in test_text\n",
    "                    print(f\"[DEBUG] Simple word check - 'yes': {has_yes}, 'but': {has_but}, 'agree': {has_agree}\")\n",
    "                    # Try a simple regex test\n",
    "                    test_match = re.search(r'\\byes\\b', test_text)\n",
    "                    print(f\"[DEBUG] Regex test for '\\\\byes\\\\b': {test_match is not None}\")\n",
    "        \n",
    "        # Handle edge cases\n",
    "        if total_agreement == 0 and total_disagreement == 0:\n",
    "            # No clear markers found - default to neutral\n",
    "            return 5.0\n",
    "        elif total_disagreement == 0:\n",
    "            # Only agreement found - strong consensus\n",
    "            return 1.0\n",
    "        elif total_agreement == 0:\n",
    "            # Only disagreement found - strong divergence\n",
    "            return 10.0\n",
    "        \n",
    "        # Calculate ratio and score\n",
    "        agreement_ratio = total_agreement / total_disagreement\n",
    "        \n",
    "        print(f\"[DEBUG] Agreement ratio: {agreement_ratio:.3f}\")\n",
    "        \n",
    "        # Map ratio to 1-10 scale using a smoother continuous function\n",
    "        # High ratio (lots of agreement) -> low score (consensus)\n",
    "        # Low ratio (lots of disagreement) -> high score (divergence)\n",
    "        # Ratio = 1.0 (balanced) -> score = 5.0\n",
    "        \n",
    "        # Use logarithmic scaling for smoother transitions\n",
    "        # When ratio = 1.0, log(1.0) = 0, so score = 5.0\n",
    "        # When ratio -> infinity (all agreement), score -> 1.0\n",
    "        # When ratio -> 0 (all disagreement), score -> 10.0\n",
    "        \n",
    "        import math\n",
    "        \n",
    "        if agreement_ratio > 0:\n",
    "            # Use log scale: log(ratio) maps to score\n",
    "            # log(1) = 0 -> score 5.0\n",
    "            # log(10) ≈ 2.3 -> score ~1.0 (strong consensus)\n",
    "            # log(0.1) ≈ -2.3 -> score ~10.0 (strong divergence)\n",
    "            \n",
    "            log_ratio = math.log(agreement_ratio)\n",
    "            \n",
    "            # Map log_ratio from [-2.3, 2.3] to [10, 1]\n",
    "            # When log_ratio = 0 (ratio = 1.0), score = 5.0\n",
    "            # When log_ratio = 2.3 (ratio ≈ 10), score ≈ 1.0\n",
    "            # When log_ratio = -2.3 (ratio ≈ 0.1), score ≈ 10.0\n",
    "            \n",
    "            # Linear mapping: log_ratio of 0 -> 5.0, log_ratio of 2.3 -> 1.0, log_ratio of -2.3 -> 10.0\n",
    "            if log_ratio >= 0:\n",
    "                # Consensus range: log_ratio [0, 2.3] -> score [5.0, 1.0]\n",
    "                score = 5.0 - (log_ratio / 2.3) * 4.0\n",
    "            else:\n",
    "                # Divergence range: log_ratio [-2.3, 0] -> score [10.0, 5.0]\n",
    "                score = 5.0 - (log_ratio / 2.3) * 5.0\n",
    "        else:\n",
    "            # Shouldn't happen (we check for total_disagreement == 0 above)\n",
    "            score = 10.0\n",
    "        \n",
    "        score = max(1.0, min(10.0, score))\n",
    "        print(f\"[DEBUG] Computed score: {score:.3f}\")\n",
    "        \n",
    "        return score\n",
    "        \n",
    "    except AttributeError as e:\n",
    "        # Handle missing attributes gracefully\n",
    "        print(f\"[DEBUG] AttributeError in compute_conclusiveness_score: {e}\")\n",
    "        return 5.0\n",
    "    except Exception as e:\n",
    "        # Log the actual error for debugging (but don't fail)\n",
    "        print(f\"[DEBUG] Exception in compute_conclusiveness_score: {type(e).__name__}: {str(e)}\")\n",
    "        import traceback\n",
    "        print(f\"[DEBUG] Traceback: {traceback.format_exc()}\")\n",
    "        return 5.0\n",
    "\n",
    "def compute_speaker_identity_score(corpus: Corpus) -> float:\n",
    "    \"\"\"Speaker Identity: Similarity (1) vs. Diversity (10)\"\"\"\n",
    "    try:\n",
    "        speakers = {}\n",
    "        \n",
    "        for utt in corpus.iter_utterances():\n",
    "            speaker_id = utt.speaker.id\n",
    "            if speaker_id not in speakers:\n",
    "                speakers[speaker_id] = {\"words\": set()}\n",
    "            \n",
    "            words = set(re.findall(r'\\b\\w+\\b', utt.text.lower()))\n",
    "            speakers[speaker_id][\"words\"].update(words)\n",
    "        \n",
    "        if len(speakers) < 2:\n",
    "            return 5.0\n",
    "        \n",
    "        speaker_list = list(speakers.keys())\n",
    "        overlaps = []\n",
    "        \n",
    "        for i in range(len(speaker_list)):\n",
    "            for j in range(i + 1, len(speaker_list)):\n",
    "                words_i = speakers[speaker_list[i]][\"words\"]\n",
    "                words_j = speakers[speaker_list[j]][\"words\"]\n",
    "                \n",
    "                if len(words_i) == 0 or len(words_j) == 0:\n",
    "                    continue\n",
    "                \n",
    "                overlap = len(words_i & words_j) / len(words_i | words_j)\n",
    "                overlaps.append(overlap)\n",
    "        \n",
    "        if not overlaps:\n",
    "            return 5.0\n",
    "        \n",
    "        avg_overlap = np.mean(overlaps)\n",
    "        score = 10 - (avg_overlap * 9)\n",
    "        return max(1, min(10, score))\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing speaker identity score: {e}\")\n",
    "        return 5.0\n",
    "\n",
    "def compute_speaker_fluidity_score(corpus: Corpus, window_size: int = 20) -> float:\n",
    "    \"\"\"Speaker Fluidity: Malleability (1) vs. Consistency (10)\"\"\"\n",
    "    try:\n",
    "        speaker_utterances = defaultdict(list)\n",
    "        \n",
    "        for utt in corpus.iter_utterances():\n",
    "            speaker_utterances[utt.speaker.id].append({\n",
    "                \"text\": utt.text,\n",
    "                \"timestamp\": utt.meta.get(\"timestamp\", 0)\n",
    "            })\n",
    "        \n",
    "        if len(speaker_utterances) == 0:\n",
    "            return 5.0\n",
    "        \n",
    "        # Use adaptive window size: at least 4 utterances (2 per half), or use the specified window_size\n",
    "        # Find the minimum utterances any speaker has, and use that to set a reasonable threshold\n",
    "        min_utterances = min(len(utts) for utts in speaker_utterances.values()) if speaker_utterances else 0\n",
    "        # Use at least 4, but prefer the specified window_size if speakers have enough utterances\n",
    "        adaptive_window = max(4, min(window_size, min_utterances)) if min_utterances >= 4 else 4\n",
    "        \n",
    "        consistency_scores = []\n",
    "        \n",
    "        for speaker_id, utts in speaker_utterances.items():\n",
    "            # Require at least 4 utterances to split into two halves\n",
    "            if len(utts) < 4:\n",
    "                continue\n",
    "            \n",
    "            utts_sorted = sorted(utts, key=lambda x: x[\"timestamp\"])\n",
    "            mid_point = len(utts_sorted) // 2\n",
    "            \n",
    "            first_half_words = set()\n",
    "            second_half_words = set()\n",
    "            \n",
    "            for utt in utts_sorted[:mid_point]:\n",
    "                words = set(re.findall(r'\\b\\w+\\b', utt[\"text\"].lower()))\n",
    "                first_half_words.update(words)\n",
    "            \n",
    "            for utt in utts_sorted[mid_point:]:\n",
    "                words = set(re.findall(r'\\b\\w+\\b', utt[\"text\"].lower()))\n",
    "                second_half_words.update(words)\n",
    "            \n",
    "            if len(first_half_words) == 0 or len(second_half_words) == 0:\n",
    "                continue\n",
    "            \n",
    "            overlap = len(first_half_words & second_half_words)\n",
    "            union = len(first_half_words | second_half_words)\n",
    "            similarity = overlap / union if union > 0 else 0\n",
    "            consistency_scores.append(similarity)\n",
    "        \n",
    "        if not consistency_scores:\n",
    "            return 5.0\n",
    "        \n",
    "        avg_consistency = np.mean(consistency_scores)\n",
    "        score = 1 + (avg_consistency * 9)\n",
    "        return max(1, min(10, score))\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing speaker fluidity score: {e}\")\n",
    "        return 5.0\n",
    "\n",
    "def evaluate_conversation(conversation_history: List[Dict]) -> Dict[str, float]:\n",
    "    \"\"\"Compute all 4 evaluation metrics using ConvoKit.\"\"\"\n",
    "    corpus = conversation_to_corpus(conversation_history)\n",
    "    \n",
    "    return {\n",
    "        \"dynamic\": compute_dynamic_score(corpus),\n",
    "        \"conclusiveness\": compute_conclusiveness_score(corpus),\n",
    "        \"speaker_identity\": compute_speaker_identity_score(corpus),\n",
    "        \"speaker_fluidity\": compute_speaker_fluidity_score(corpus)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff39461",
   "metadata": {},
   "source": [
    "## Running Experiments\n",
    "\n",
    "Let's run conversations with 2 and 3 speakers, then analyze the implicit personas that emerge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77d57d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 2-speaker conversation with implicit personas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running 2-speaker conversation: 100%|██████████| 50/50 [05:08<00:00,  6.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== 2-Speaker Results ===\n",
      "Dynamic (Collaborative ↔ Competitive): 5.32/10\n",
      "Conclusiveness (Consensus ↔ Divergence): 5.00/10\n",
      "Speaker Identity (Similarity ↔ Diversity): 5.25/10\n",
      "Speaker Fluidity (Malleability ↔ Consistency): 4.65/10\n"
     ]
    }
   ],
   "source": [
    "# Run 2-speaker conversation\n",
    "print(\"Running 2-speaker conversation with implicit personas...\")\n",
    "conv_2speaker = run_conversation_implicit_personas(iterations=50, participant_count=2)\n",
    "metrics_2speaker = evaluate_conversation(conv_2speaker)\n",
    "\n",
    "print(\"\\\\n=== 2-Speaker Results ===\")\n",
    "print(f\"Dynamic (Collaborative ↔ Competitive): {metrics_2speaker['dynamic']:.2f}/10\")\n",
    "print(f\"Conclusiveness (Consensus ↔ Divergence): {metrics_2speaker['conclusiveness']:.2f}/10\")\n",
    "print(f\"Speaker Identity (Similarity ↔ Diversity): {metrics_2speaker['speaker_identity']:.2f}/10\")\n",
    "print(f\"Speaker Fluidity (Malleability ↔ Consistency): {metrics_2speaker['speaker_fluidity']:.2f}/10\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d1974b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] compute_conclusiveness_score called\n",
      "[DEBUG] Corpus type: <class 'convokit.model.corpus.Corpus'>\n",
      "[DEBUG] Retrieved 46 utterances\n",
      "[DEBUG] Counts - Strong agree: 13, Moderate agree: 14\n",
      "[DEBUG] Counts - Strong disagree: 2, Moderate disagree: 36\n",
      "[DEBUG] Total agreement: 40, Total disagreement: 40\n",
      "[DEBUG] Agreement ratio: 1.000\n",
      "[DEBUG] Computed score: 5.000\n",
      "\\n=== 2-Speaker Results ===\n",
      "Dynamic (Collaborative ↔ Competitive): 5.32/10\n",
      "Conclusiveness (Consensus ↔ Divergence): 5.00/10\n",
      "Speaker Identity (Similarity ↔ Diversity): 5.25/10\n",
      "Speaker Fluidity (Malleability ↔ Consistency): 4.65/10\n"
     ]
    }
   ],
   "source": [
    "metrics_2speaker = evaluate_conversation(conv_2speaker)\n",
    "\n",
    "print(\"\\\\n=== 2-Speaker Results ===\")\n",
    "print(f\"Dynamic (Collaborative ↔ Competitive): {metrics_2speaker['dynamic']:.2f}/10\")\n",
    "print(f\"Conclusiveness (Consensus ↔ Divergence): {metrics_2speaker['conclusiveness']:.2f}/10\")\n",
    "print(f\"Speaker Identity (Similarity ↔ Diversity): {metrics_2speaker['speaker_identity']:.2f}/10\")\n",
    "print(f\"Speaker Fluidity (Malleability ↔ Consistency): {metrics_2speaker['speaker_fluidity']:.2f}/10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1051aae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nRunning 3-speaker conversation with implicit personas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running 3-speaker conversation: 100%|██████████| 33/33 [07:43<00:00, 14.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== 3-Speaker Results ===\n",
      "Dynamic (Collaborative ↔ Competitive): 5.43/10\n",
      "Conclusiveness (Consensus ↔ Divergence): 5.00/10\n",
      "Speaker Identity (Similarity ↔ Diversity): 5.26/10\n",
      "Speaker Fluidity (Malleability ↔ Consistency): 4.82/10\n"
     ]
    }
   ],
   "source": [
    "# Run 3-speaker conversation\n",
    "print(\"\\\\nRunning 3-speaker conversation with implicit personas...\")\n",
    "conv_3speaker = run_conversation_implicit_personas(iterations=33, participant_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "351e8de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] compute_conclusiveness_score called\n",
      "[DEBUG] Corpus type: <class 'convokit.model.corpus.Corpus'>\n",
      "[DEBUG] Retrieved 70 utterances\n",
      "[DEBUG] Counts - Strong agree: 18, Moderate agree: 14\n",
      "[DEBUG] Counts - Strong disagree: 3, Moderate disagree: 60\n",
      "[DEBUG] Total agreement: 50, Total disagreement: 66\n",
      "[DEBUG] Agreement ratio: 0.758\n",
      "[DEBUG] Computed score: 5.604\n",
      "\\n=== 3-Speaker Results ===\n",
      "Dynamic (Collaborative ↔ Competitive): 5.43/10\n",
      "Conclusiveness (Consensus ↔ Divergence): 5.60/10\n",
      "Speaker Identity (Similarity ↔ Diversity): 5.26/10\n",
      "Speaker Fluidity (Malleability ↔ Consistency): 4.82/10\n"
     ]
    }
   ],
   "source": [
    "metrics_3speaker = evaluate_conversation(conv_3speaker)\n",
    "\n",
    "print(\"\\\\n=== 3-Speaker Results ===\")\n",
    "print(f\"Dynamic (Collaborative ↔ Competitive): {metrics_3speaker['dynamic']:.2f}/10\")\n",
    "print(f\"Conclusiveness (Consensus ↔ Divergence): {metrics_3speaker['conclusiveness']:.2f}/10\")\n",
    "print(f\"Speaker Identity (Similarity ↔ Diversity): {metrics_3speaker['speaker_identity']:.2f}/10\")\n",
    "print(f\"Speaker Fluidity (Malleability ↔ Consistency): {metrics_3speaker['speaker_fluidity']:.2f}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd739696",
   "metadata": {},
   "source": [
    "## Analyzing Implicit Personas\n",
    "\n",
    "Let's look at the actual messages to see what personas emerged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82a25fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Initial Personas (First Messages) ===\n",
      "\\nspeaker_1:\n",
      "Thank you! I'm glad to be here. What's on everyone's mind today?\n",
      "\\nspeaker_2:\n",
      "Thank you! I'm glad to be here. What's on the agenda for our discussion today?\n",
      "\\n\\n=== Later Messages (Persona Persistence) ===\n",
      "\\nspeaker_2: Reflecting on the collective insights and dynamic discussions we've had, it's truly motivating to witness our commitment to integrating AI ethically a...\n",
      "\\nspeaker_1: Speaker_2, I truly appreciate your perspectives and the enthusiasm you've brought to our discussion about the ethical integration of AI. It's been inc...\n",
      "\\nspeaker_2: Reflecting on the depth and breadth of our ongoing discussion, I find it incredibly encouraging to see us, as a group, united by a shared commitment t...\n",
      "\\nspeaker_1: Our discussion on integrating AI ethically across various sectors has generated quite a wealth of insights, each shedding light on the shared commitme...\n",
      "\\nspeaker_2: It's been insightful to engage in this ongoing dialogue about integrating AI with a strong ethical foundation. Speaker_1 and many others have emphasiz...\n"
     ]
    }
   ],
   "source": [
    "# Show first messages from each speaker (their initial personas)\n",
    "print(\"=== Initial Personas (First Messages) ===\")\n",
    "for msg in conv_2speaker[:2]:\n",
    "    speaker = msg.get(\"name\", \"unknown\")\n",
    "    content = msg.get(\"content\", \"\")\n",
    "    print(f\"\\\\n{speaker}:\")\n",
    "    print(content[:200] + \"...\" if len(content) > 200 else content)\n",
    "\n",
    "# Show later messages to see if personas persisted\n",
    "print(\"\\\\n\\\\n=== Later Messages (Persona Persistence) ===\")\n",
    "for msg in conv_2speaker[-5:]:\n",
    "    speaker = msg.get(\"name\", \"unknown\")\n",
    "    content = msg.get(\"content\", \"\")\n",
    "    print(f\"\\\\n{speaker}: {content[:150]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1e381e",
   "metadata": {},
   "source": [
    "## Comparison with Explicit Personas\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **Natural Differentiation**: Even without explicit prompts, LLMs develop distinct communication styles\n",
    "2. **Speaker Identity Score**: Measures how different speakers are from each other\n",
    "   - High score (8-10): Speakers are very different (diverse personas)\n",
    "   - Low score (1-3): Speakers are similar (convergent personas)\n",
    "3. **Speaker Fluidity**: Measures consistency over time\n",
    "   - High score (8-10): Speakers maintain consistent style\n",
    "   - Low score (1-3): Speakers change style frequently\n",
    "\n",
    "### Implicit vs Explicit Personas\n",
    "\n",
    "**Implicit Personas (this post)**:\n",
    "- Develop naturally during conversation\n",
    "- May be less distinct initially\n",
    "- Can evolve more freely\n",
    "\n",
    "**Explicit Personas (post 007)**:\n",
    "- Guided by initial prompts\n",
    "- More distinct from the start\n",
    "- May be more stable but less flexible\n",
    "\n",
    "## Summary\n",
    "\n",
    "LLMs naturally develop implicit personas even without explicit guidance:\n",
    "\n",
    "- **Differentiation occurs**: Speaker Identity scores show distinct personas emerge\n",
    "- **Consistency varies**: Some speakers maintain style, others adapt\n",
    "- **Comparable to explicit**: Implicit personas can be as distinct as explicit ones\n",
    "\n",
    "This suggests that:\n",
    "1. LLMs have inherent tendencies toward persona development\n",
    "2. Conversation context shapes persona emergence\n",
    "3. Explicit prompts may guide but don't create personas from scratch\n",
    "\n",
    "Future work could explore:\n",
    "- What factors influence implicit persona development?\n",
    "- How do implicit personas compare to explicit ones in long conversations?\n",
    "- Can we predict which implicit personas will emerge?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
