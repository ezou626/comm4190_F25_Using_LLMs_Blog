[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs in Social Contexts",
    "section": "",
    "text": "Spontaneity\n\n\n\nLLMs\n\nConversations\n\n\n\nAdding randomness to an LLM conversation\n\n\n\n\n\nSep 12, 2025\n\n\nEric Zou\n\n\n\n\n\n\n\n\n\n\n\n\nAnother LLM Conversation\n\n\n\nLLMs\n\nConversations\n\n\n\nAnalyzing a conversation between two LLMs in a shared chat window\n\n\n\n\n\nSep 10, 2025\n\n\nEric Zou\n\n\n\n\n\n\n\n\n\n\n\n\nAn LLM Conversation\n\n\n\nLLMs\n\nConversations\n\n\n\nAnalyzing a conversation between two LLMs in a simulated chat environment\n\n\n\n\n\nSep 8, 2025\n\n\nEric Zou\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\nLLMs\n\nprompting\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/003_approaching_a_true_discussion/Spontaneity.html",
    "href": "posts/003_approaching_a_true_discussion/Spontaneity.html",
    "title": "Spontaneity",
    "section": "",
    "text": "from openai import OpenAI\nimport os\nimport base64\nimport requests\nfrom tqdm import tqdm\nfrom IPython.display import FileLink, display\nfrom dotenv import load_dotenv\nfrom random import shuffle, randint, choice, random\n# Load API key\n_ = load_dotenv(\"../../../comm4190_F25/01_Introduction_and_setup/.env\")\nclient = OpenAI()"
  },
  {
    "objectID": "posts/003_approaching_a_true_discussion/Spontaneity.html#llm-conversations",
    "href": "posts/003_approaching_a_true_discussion/Spontaneity.html#llm-conversations",
    "title": "Spontaneity",
    "section": "LLM Conversations",
    "text": "LLM Conversations\nWe’re going to build on our progress in the last post to introduce more spontaneity and randomness into our chats. Let’s first try adding randomness in the order in which models speak. By the second iteration, this could result in less predictable conversations.\nWe’re also going to change the prompt a little bit too. I’d like to switch more into understanding how we can inspire the LLM agents to collaborate and engage in a less structured way than a debate setting.\n\nDEBATE_TOPIC = \"\"\"Code, testing, and dev infra should be prioritized over comprehensive documentation.\"\"\"\nSYSTEM_PROMPT = \"You are participating in a conversation between experienced software engineers. Each speaker should respond when directed. Keep questions minimal and only use them when necessary.\"\n\n# prompt to analyze conversations\nEVALUATION_PROMPT = \"\"\"\nYour objective is to analyze this conversation between speakers.\nYour response should follow this organization:\n- A Brief Summary\n- Final Outputs/Artifacts/Takeaways\n- Characteristics/Dynamic (Competitive/Collaborative/etc.)\n\"\"\"\n\ndef analyze_conversation(conversation: str):\n    input_chat = [\n        {\n            \"role\": \"system\",\n            \"content\": EVALUATION_PROMPT\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Here is the transcript\\n\" + conversation\n        }\n    ]\n    response = client.chat.completions.create(\n        model = \"gpt-4o\",\n        messages = input_chat,\n        store = False\n    )\n    print(response.choices[0].message.content)\n\n\ndef run_organic_conversation_v1(\n    iterations: int, \n    openai_model_id: str,\n    participant_count: int,\n    topic: str,\n    system_prompt: str,\n) -&gt; list[dict]:\n    # model 1 is the first speaker\n    debate_history = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": \"The topic is: \" + topic}\n    ]\n\n    ordering = list(range(1, participant_count + 1))\n\n    for i in tqdm(range(iterations)):\n        if i &gt; 0:\n            first = choice(ordering[:-1])\n            remaining = [i for i in ordering if i != first]\n            shuffle(remaining)\n            ordering = [first] + remaining\n        for model in ordering: # RANDOM ORDERING\n            speaker_id = f\"speaker_{model}\"\n            response = client.chat.completions.create(\n                model = openai_model_id,\n                messages = debate_history + [{\"role\": \"user\", \"content\": f\"{speaker_id}, it's your turn to speak.\"}],\n                store = False\n            );\n            message = response.choices[0].message.content\n            debate_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message})\n\n    return debate_history\n\n# code to save the conversation\ndef save_conversation(\n    filename: str,\n    debate_history: list[dict]\n) -&gt; str:\n\n    messages = []\n\n    for record in debate_history:\n\n        if record[\"role\"] == \"user\":\n            messages.append(\"mediator:\\n\" + record[\"content\"])\n        \n        if record[\"role\"] == \"assistant\":\n            messages.append(f\"{record[\"name\"]}:\\n{record[\"content\"]}\")\n    \n    conversation_transcript = \"\\n\\n\".join(messages)\n    \n    with open(filename, \"w\", encoding=\"utf-8\") as f:\n        f.write(conversation_transcript)\n    \n    display(FileLink(filename))\n\n    return conversation_transcript\n\n\nExperiment 1:\nLet’s keep 3 speakers throughout. It will probably be more interesting this way, since a conversation between two speakers could just have the messages be joined together (though this might be interesting to evaluate, we can add it as future work).\n\ndebate_1 = run_organic_conversation_v1(2, 'gpt-4o', 3, DEBATE_TOPIC, SYSTEM_PROMPT)\n\n100%|██████████| 2/2 [00:28&lt;00:00, 14.19s/it]\n\n\n\nconversation_transcript_1 = save_conversation(\"conversation_transcript_1.txt\", debate_1)\n\nconversation_transcript_1.txt\n\n\n\n\nAnalysis\nLike last time, we’re going to analyze the conversation with AI as well.\n\nanalyze_conversation(conversation_transcript_1)\n\n- **Brief Summary**: The conversation revolves around the prioritization between code, testing, and development infrastructure versus comprehensive documentation. Speaker 1 advocates for prioritizing the former due to its direct impact on software reliability and efficiency, suggesting documentation can follow as a secondary priority. Speaker 2 underscores the importance of comprehensive documentation, especially for onboarding and scalability. Speaker 3 proposes a balanced integration of documentation into the development process through automation and self-documenting techniques, aligning with regulatory requirements when necessary.\n\n- **Final Outputs/Artifacts/Takeaways**:\n  - There is an acknowledgment that both development infrastructure and documentation hold significant value, each crucial for different reasons such as reliability, maintainability, onboarding, and compliance.\n  - The consensus is towards integrating documentation seamlessly into the development process using automation and tools that ensure documentation remains current and synchronized with code changes.\n  - Practical strategies proposed include employing self-documenting code practices, utilizing documentation generation tools, and incorporating documentation updates into standard development workflows like code reviews and commit messaging.\n\n- **Characteristics/Dynamic (Collaborative/Competitive/etc.)**: \n  - The discussion is collaborative, with each speaker building upon the ideas of the others to arrive at a nuanced understanding of the balance between development priorities and documentation. \n  - Speakers recognize and incorporate differing viewpoints, ultimately finding a middle ground that supports both development efficiency and the comprehensive documentation needs.\n  - There is a collective move towards practical solutions that address potential conflicts between maintaining quality development practices and ensuring documentation does not become outdated or sidelined.\n\n\nWe’re shifting back to the conversational tone that we had in the first post, probably due mainly to the changes in the prompt. We can still see some disagreement in the beginning, but as the conversation continues, the speakers’ discussions converge, while still maintaining the individual opinions. However, each speaker always speaks once during each turn. This is not necessarily true for real world conversations.\n\n\nExperiment 2\nLet’s see if we can extend the conversation, but this time, let’s leverage randomness to simulate the fact that people don’t always participate in each conversation. We’ll keep a fixed order for now, but every time a model speaks, we’ll track it to make sure that the next speaker is not the same model.\n\ndef run_organic_conversation_v2(\n    iterations: int, \n    openai_model_id: str,\n    participant_count: int,\n    topic: str,\n    system_prompt: str,\n    dropout_chance: float\n) -&gt; list[dict]:\n    # model 1 is the first speaker\n    debate_history = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": \"The topic is: \" + topic}\n    ]\n\n    ordering = list(range(1, participant_count + 1))\n    last_speaker = -1\n\n    for i in tqdm(range(iterations)):\n        # if i &gt; 0:\n        #     first = choice(ordering[:-1])\n        #     remaining = [i for i in ordering if i != first]\n        #     shuffle(remaining)\n        #     ordering = [first] + remaining\n        for model in ordering: # RANDOM ORDERING\n            if random() &lt; dropout_chance:\n                continue # SKIP\n            if last_speaker == model:\n                continue\n            speaker_id = f\"speaker_{model}\"\n            response = client.chat.completions.create(\n                model = openai_model_id,\n                messages = debate_history + [{\"role\": \"user\", \"content\": f\"{speaker_id}, it's your turn to speak.\"}],\n                store = False\n            );\n            message = response.choices[0].message.content\n            debate_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message})\n            last_speaker = model\n\n    return debate_history\n\ndebate_2 = run_organic_conversation_v2(8, 'gpt-4o', 3, DEBATE_TOPIC, SYSTEM_PROMPT, 0.5)\n\n100%|██████████| 8/8 [00:56&lt;00:00,  7.10s/it]\n\n\n\nconversation_transcript_2 = save_conversation(\"conversation_transcript_2.txt\",debate_2)\n\nconversation_transcript_2.txt\n\n\n\n\nAnalysis\nFirst, let’s generate an AI summary of the conversation.\n\nanalyze_conversation(conversation_transcript_2)\n\n- **Brief Summary:**  \nThe conversation revolves around the prioritization of code, testing, and development infrastructure over comprehensive documentation. Speaker 1 argues for the importance of code and infrastructure as they ensure the quality and reliability of the software, especially in agile environments. Speaker 2 emphasizes the long-term benefits of documentation, particularly for knowledge retention and project sustainability. Speaker 3 brings a balanced perspective, advocating for adaptable strategies based on the project's lifecycle and promoting a development culture that values all aspects equally.\n\n- **Final Outputs/Artifacts/Takeaways:**  \n  - The need for balance between immediate development needs and sustainable documentation practices.\n  - Integration of documentation into the development process using automation tools to keep it current.\n  - Building a comprehensive development culture where all elements are seen as integral.\n  - Leadership roles in guiding priorities and maintaining balance.\n  - Utilizing data-driven approaches and feedback loops to inform strategy and resource allocation.\n\n- **Characteristics/Dynamic:**  \n  - **Collaborative:** The conversation exhibits a collaborative dynamic where all speakers acknowledge each other's points and build on them to reach a common ground.\n  - **Balanced Approach:** Emphasis on finding a balance between immediate and long-term project needs.\n  - **Adaptability:** Discusses the need for strategies that adapt to different stages of the project's lifecycle.\n  - **Leadership and Culture:** Highlights the role of leadership and team culture in achieving a balanced approach to software development.\n\n\nAt a glance, it certainly seems more conversational. I feel that the LLMs are making too many redundant summaries. Perhaps we can adjust the prompt to let the conversation build off the other statements.\n\n\nExperiment 3\nWe’re going to join the two prior approaches.\n\ndef run_organic_conversation_v3(\n    iterations: int, \n    openai_model_id: str,\n    participant_count: int,\n    topic: str,\n    system_prompt: str,\n    dropout_chance: float\n) -&gt; list[dict]:\n    # model 1 is the first speaker\n    debate_history = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": \"The topic is: \" + topic}\n    ]\n\n    ordering = list(range(1, participant_count + 1))\n    last_speaker = -1\n\n    for i in tqdm(range(iterations)):\n        if i &gt; 0:\n            first = choice(ordering[:-1])\n            remaining = [i for i in ordering if i != first]\n            shuffle(remaining)\n            ordering = [first] + remaining\n        for model in ordering: # RANDOM ORDERING\n            if random() &lt; dropout_chance:\n                continue # SKIP\n            if last_speaker == model:\n                continue\n            speaker_id = f\"speaker_{model}\"\n            response = client.chat.completions.create(\n                model = openai_model_id,\n                messages = debate_history + [{\"role\": \"user\", \"content\": f\"{speaker_id}, it's your turn to speak.\"}],\n                store = False\n            );\n            message = response.choices[0].message.content\n            debate_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message})\n            last_speaker = model\n\n    return debate_history\n\ndebate_3 = run_organic_conversation_v3(8, 'gpt-4o', 3, DEBATE_TOPIC, SYSTEM_PROMPT, 0.5)\n\n100%|██████████| 8/8 [00:43&lt;00:00,  5.39s/it]\n\n\n\nconversation_transcript_3 = save_conversation(\"conversation_transcript_3.txt\", debate_3)\n\nconversation_transcript_3.txt\n\n\n\n\nAnalysis\nNow, let’s get a quick summary of what happened.\n\nanalyze_conversation(conversation_transcript_3)\n\n- **Brief Summary**: The discussion revolves around the balance between code, testing, development infrastructure, and comprehensive documentation. Speaker 1 supports prioritizing code and infrastructure, especially in agile settings, but acknowledges the value of documentation. Speaker 2 emphasizes the importance of integrating key documentation practices alongside development to prevent knowledge bottlenecks. Speaker 3 advocates for a balanced approach where documentation evolves with development, employing tools and practices that ensure both areas are maintained effectively.\n\n- **Final Outputs/Artifacts/Takeaways**:\n  - The idea of iterative and lightweight documentation, evolving with the product, to enhance team onboarding and collaboration.\n  - Integration of documentation into the definition of \"done\" for each task and feature.\n  - Leveraging automated tools to maintain documentation accuracy and reduce manual burdens.\n  - Cultivating a culture where both code quality and documentation are seen as interconnected responsibilities through pair programming, continuous integration, and collaboration tools.\n\n- **Characteristics/Dynamics**: The conversation is collaborative and consensus-driven, with each speaker building on the ideas of others. There is a collective push towards achieving a balanced development process that recognizes the importance of both technical infrastructure and documentation. The dynamic reflects a mutual acknowledgment of each other's viewpoints, seeking common ground and practical solutions that integrate both priorities effectively in software development workflows.\n\n\nSurprisingly, I think this set was much more conversational than the last. I don’t know if I can declare this as reproducible though.\n\n\nExperiment 4\nThe final thing we can do is make the prompts more like thoughts. Instead of saying “it’s your turn to speak”, we can say “please share your current perspective with the crowd”. We can also alter the system prompt to not mention a simulation.\n\nNEW_SYSTEM_PROMPT = \"You a participant in a conversation between experienced software engineers. Keep questions minimal and only use them when necessary. Please greet the other participants when you join.\"\n\ndef run_organic_conversation_v4(\n    iterations: int, \n    openai_model_id: str,\n    participant_count: int,\n    topic: str,\n    system_prompt: str,\n    dropout_chance: float\n) -&gt; list[dict]:\n    # model 1 is the first speaker\n    debate_history = [\n        {\"role\": \"system\", \"content\": system_prompt + \" The topic is: \" + topic}\n    ]\n\n    ordering = list(range(1, participant_count + 1))\n    last_speaker = -1\n\n    for i in tqdm(range(iterations)):\n        if i &gt; 0:\n            first = choice(ordering[:-1])\n            remaining = [i for i in ordering if i != first]\n            shuffle(remaining)\n            ordering = [first] + remaining\n        for model in ordering: # RANDOM ORDERING\n            if random() &lt; dropout_chance:\n                continue # SKIP\n            if last_speaker == model:\n                continue\n            speaker_id = f\"speaker_{model}\"\n            response = client.chat.completions.create(\n                model = openai_model_id,\n                messages = debate_history + [{\"role\": \"user\", \"content\": f\"{speaker_id}, please share your perspective with the others and engage with the responses of the other participants.\"}],\n                store = False\n            );\n            message = response.choices[0].message.content\n            debate_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message})\n            last_speaker = model\n\n    return debate_history\n\ndebate_4 = run_organic_conversation_v4(8, 'gpt-4o', 3, DEBATE_TOPIC, NEW_SYSTEM_PROMPT, 0.5)\n\n100%|██████████| 8/8 [02:05&lt;00:00, 15.69s/it]\n\n\n\nconversation_transcript_4 = save_conversation(\"conversation_transcript_4.txt\", debate_4)\n\nconversation_transcript_4.txt\n\n\n\n\nAnalysis\nLet’s see how that changed the debate.\n\nanalyze_conversation(conversation_transcript_4)\n\n- **Brief Summary**: The conversation revolves around the balance between prioritizing code quality, testing, and development infrastructure over comprehensive documentation in software development. Speaker_1 initiates the discussion by emphasizing the importance of clean code and robust testing. Speaker_3 agrees while acknowledging the potential challenges of inadequate documentation in complex systems. Speaker_2 advocates for a balance, stressing the need for essential documentation and lightweight approaches like API documentation and high-level overviews. The conversation further explores innovative documentation methods like video tutorials and AI-assisted documentation.\n\n- **Final Outputs/Artifacts/Takeaways**: The primary takeaways include the consensus on the importance of balancing documentation with code quality and infrastructure. The participants also share practical strategies such as embedding documentation updates into the code review process, using version control for documentation, leveraging video tutorials, and experimenting with AI-driven documentation tools.\n\n- **Characteristics/Dynamic**: The conversation is highly collaborative, with each speaker building upon the others' points, sharing personal experiences, and exploring various strategies to achieve the common goal of effective software development practices. The speakers demonstrate a willingness to learn from each other's insights and adapt successful methods to their own workflows, fostering a dynamic environment of mutual respect and shared learning.\n\n\nThe conversation is much more conversational this time, with explicit instructions to address the other speakers, there’s a lot more structure in the engagement between the three speakers. Additionally, it seems that near the end of the conversation, the speakers converge toward some actionable items, despite there not being explicit instructions to find common ground."
  },
  {
    "objectID": "posts/003_approaching_a_true_discussion/Spontaneity.html#closing-remarks",
    "href": "posts/003_approaching_a_true_discussion/Spontaneity.html#closing-remarks",
    "title": "Spontaneity",
    "section": "Closing Remarks",
    "text": "Closing Remarks\nWith the new shared chat window approach, some prompt tuning, and some randomization, we approach a more natural conversational vibe in the discussion between the 3 LLMs. It also feels less structured, and that could potentially allow for integration with human speakers and produce results similar to meetings or panels. It would be interesting to see what kinds of topics LLMs can excel in, or if they can even do well without having a predefined topic.\nIt’s also important to consider that this is probably not the only way to do this. Potentially, we could add more prompts before we get each speaker LLM to answer.\n\nFuture Work: - Experiment with more topics - Ignore predefined topic"
  },
  {
    "objectID": "posts/002_another_llm_conversation/another_llm_conversation.html",
    "href": "posts/002_another_llm_conversation/another_llm_conversation.html",
    "title": "Another LLM Conversation",
    "section": "",
    "text": "Last time, we looked at LLMs conversing with two separate chat windows. This time, I wonder if we can do it with one. OpenAI provides a “name” field in the input messages which we can use to identify model 1 versus model 2. In this case, we tell one model to simulate the debate, with the user acting as the mediator. The interesting thing about this is that we can scale this much better. With the user as the orchestrator, we can actually add as many models/positions to the debate as we want. We can also fall away from the debate structure and come up with collaborative scenarios as well.\nfrom openai import OpenAI\nimport os\nimport base64\nimport requests\nfrom tqdm import tqdm\nfrom IPython.display import FileLink, display\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv(\"../../../comm4190_F25/01_Introduction_and_setup/.env\")\nclient = OpenAI()"
  },
  {
    "objectID": "posts/002_another_llm_conversation/another_llm_conversation.html#llm-debates",
    "href": "posts/002_another_llm_conversation/another_llm_conversation.html#llm-debates",
    "title": "Another LLM Conversation",
    "section": "LLM Debates",
    "text": "LLM Debates\nLet’s try LLM debates first as a continuation of our prior work. We’re going to use the same topic as last time.\nWe’ll be a bit rigid for now. We’re going to use the same model for both, an incrementing speaker naming system, and a round robin speaking format.\n\nDEBATE_TOPIC = \"\"\"Code, testing, and dev infra should be prioritized over comprehensive documentation.\"\"\"\n\n# prompt to analyze conversations\nEVALUATION_PROMPT = \"\"\"\nYour objective is to analyze this conversation between speakers.\nYour response should follow this organization:\n- A Brief Summary\n- Final Outputs/Artifacts/Takeaways\n- Characteristics/Dynamic (Competitive/Collaborative/etc.)\n\"\"\"\n\ndef analyze_conversation(conversation: str):\n    input_chat = [\n        {\n            \"role\": \"system\",\n            \"content\": EVALUATION_PROMPT\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Here is the transcript\\n\" + conversation\n        }\n    ]\n    response = client.chat.completions.create(\n        model = \"gpt-4o\",\n        messages = input_chat,\n        store = False\n    )\n    print(response.choices[0].message.content)\n\n\nSYSTEM_PROMPT = \"You are simulating a debate between AI agents. Each agent should respond in turn, logically arguing their point. Do not speak for both sides in one message.\"\n\n# code to simulate the conversation\ndef run_conversation(\n    iterations: int, \n    openai_model_id: str,\n    participant_count: int,\n    debate_topic: str\n) -&gt; list[dict]:\n    # model 1 is the first speaker\n    debate_history = [\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": \"The debate topic is: \" + debate_topic}\n    ]\n\n    for _ in tqdm(range(iterations)):\n        for model in range(1, participant_count + 1):\n            speaker_id = f\"speaker_{model}\"\n            debate_history.append({\"role\": \"user\", \"content\": f\"{speaker_id}, it's your turn to speak.\"})\n            response = client.chat.completions.create(\n                model = openai_model_id,\n                messages = debate_history,\n                store = False\n            );\n            message = response.choices[0].message.content\n            debate_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message})\n\n    return debate_history\n\n# code to save the conversation\ndef save_conversation(\n    filename: str,\n    debate_history: list[dict]\n) -&gt; str:\n\n    messages = []\n\n    for record in debate_history:\n\n        if record[\"role\"] == \"user\":\n            messages.append(\"mediator:\\n\" + record[\"content\"])\n        \n        if record[\"role\"] == \"assistant\":\n            messages.append(f\"{record[\"name\"]}:\\n{record[\"content\"]}\")\n    \n    conversation_transcript = \"\\n\\n\".join(messages)\n    \n    with open(filename, \"w\", encoding=\"utf-8\") as f:\n        f.write(conversation_transcript)\n    \n    display(FileLink(filename))\n\n    return conversation_transcript\n\n\nExperiment 1:\nHere’s our first experiment configuration: - 2 speakers - Explicit mediator: Include mediator messages in the chat history\n\ndebate_1 = run_conversation(2, 'gpt-4o', 2, DEBATE_TOPIC)\n\n100%|██████████| 2/2 [00:33&lt;00:00, 16.92s/it]\n\n\n\nconversation_transcript_1 = save_conversation(\"conversation_transcript_1.txt\", debate_1)\n\nconversation_transcript_1.txt\n\n\n\n\nAnalysis\nLike last time, we’re going to analyze the conversation with AI as well.\n\nanalyze_conversation(conversation_transcript_1)\n\n- **Brief Summary**: This debate centers on whether code quality, testing, and development infrastructure should take priority over comprehensive documentation. Speaker 1 argues that focusing on code and testing leads to more robust products and allows for quicker adaptations in the fast-paced tech world. Speaker 2 counters that documentation is crucial for knowledge transfer, collaboration, and regulatory compliance, emphasizing its role in preserving the long-term vision and understanding of software projects.\n\n- **Final Outputs/Artifacts/Takeaways**: Speaker 1 stresses the importance of sustainable code quality and testing as a form of self-documentation and for enabling continuous improvement. On the other hand, Speaker 2 highlights the necessity of documentation for maintaining architectural understanding and ensuring compliance with industry standards, as well as for effective stakeholder communication.\n\n- **Characteristics/Dynamic**: The conversation is collaborative as both speakers present structured arguments recognizing the value of the opposing views while reinforcing their own. Each speaker builds upon or refutes the other's points in a respectful and logical manner, making a case for the integration of both perspectives rather than outright exclusion of one approach over the other.\n\n\nThere’s some noticable changes from last time. The participant’s positions are noticably more rigid and as a result, the final outcome is less convergent. A side effect of this is that there is no natural closing spot. I think this is probably due to our significantly less cooperative system prompt. Last time, we explicitly asked for the model’s to aim for a common ground. We would probably see the same thing if we were to add that to our system prompt. We could change the prompt for the next experiment to include that so we can gradually move to some concrete artifacts. However, I believe a thorough evaluation of the different kinds of prompts we can use here is probably best saved for another blog systematically evaluating various system prompts and the effect of various phrases on the model’s final output. It would also be interesting to use something like ConvoKit to perform these kinds of analyses, so we can save that for a future blog.\n\nFuture Work: - Use ConvoKit to systematically evaluate conversational features in a quantitative way based on various prompting and chat structuring strategies.\n\nFor now, I’m interested in seeing if a third model can bring new things to the conversation.\n\n\nExperiment 2\nLet’s try this with 3 models instead of 2. I’m wondering what a third model will do in a seemingly binary scenario. Will it support one of the positions, or will it look to take a middle ground? What if it comes up with a third position?\n\ndebate_2 = run_conversation(2, 'gpt-4o', 3, DEBATE_TOPIC)\n\n100%|██████████| 2/2 [00:34&lt;00:00, 17.31s/it]\n\n\n\nconversation_transcript_2 = save_conversation(\"conversation_transcript_2.txt\",debate_2)\n\nconversation_transcript_2.txt\n\n\n\n\nAnalysis\nLet’s see what GPT-4o has to say about this debate.\n\nanalyze_conversation(conversation_transcript_2)\n\n- **Brief Summary:** The debate centers around whether code, testing, and development infrastructure should be prioritized over comprehensive documentation. Speaker 1 argues for prioritizing code and infrastructure, emphasizing agility and efficiency, particularly in initial project phases. Speaker 2 champions the importance of documentation for usability, maintainability, and long-term benefits. Speaker 3 presents a middle ground, suggesting integrated processes that balance both priorities as complementary elements.\n\n- **Final Outputs/Artifacts/Takeaways:** The conversation ends with speaker 3 advocating for an integrated approach where tools and methodologies make documentation a part of the development process. This includes using automated tools for inline documentation and ensuring documentation is part of the project’s definition of \"done.\" It suggests both speed in delivery and sustainable practices can coexist without sacrificing one for the other.\n\n- **Characteristics/Dynamic:** The discussion is collaborative, focusing on finding a balanced approach rather than intensifying opposing views. While there are differing opinions, particularly between speakers 1 and 2, the mediator’s role and speaker 3's perspective facilitate an integration of ideas, aiming for a synthesis that acknowledges the value in both sides of the argument.\n\n\nDespite not changing the prompt at all, it seems that speaker 3 naturally took a middle path between documentation versus using the codebase as the primary source of truth in software projects. Perhaps speaker 3 taking the middle ground could result in a better synthesis of new ideas by naturally bridging the gap in a 2-way debate. In this scenario, I’m now really interested to see what would happen if we had 4 speakers.\n\n\nExperiment 3\nNow we’re going to try two rounds of communication where we have 4 speakers. Pretty straightforward. I wonder if speaker 4 may agree with speaker 3, since I expect the start of the conversation to go about the same.\n\ndebate_3 = run_conversation(2, 'gpt-4o', 4, DEBATE_TOPIC)\n\n100%|██████████| 2/2 [01:02&lt;00:00, 31.17s/it]\n\n\n\nconversation_transcript_3 = save_conversation(\"conversation_transcript_3.txt\", debate_3)\n\nconversation_transcript_3.txt\n\n\n\n\nAnalysis\nNow, let’s get a quick summary of what happened.\n\nanalyze_conversation(conversation_transcript_3)\n\n- **Brief Summary:**\n  The conversation revolves around the debate on whether code, testing, and development infrastructure should be prioritized over comprehensive documentation in software development. Speaker_1 advocates for prioritizing core code and testing, especially in early project stages, while speaker_2 emphasizes the necessity of documentation for continuity and avoiding knowledge silos. Speaker_3 introduces a balanced approach depending on project context and phase, and speaker_4 underscores the integration of documentation with development for synergy and resilience.\n\n- **Final Outputs/Artifacts/Takeaways:**\n  - Recognition of the importance of a balanced approach between prioritizing code, testing, and infrastructure and maintaining comprehensive documentation.\n  - Understanding that the project's phase and specific needs should dictate the balance between these priorities.\n  - Emphasis on tools and methodologies that integrate documentation into the development process, like documentation-as-code.\n  - Advocacy for agile and iterative documentation alongside development to maintain project velocity and knowledge retention.\n\n- **Characteristics/Dynamic:**\n  The dynamic is **collaborative**, even though there's an inherent tension in prioritizing different project facets. The speakers build on each other's points, aiming for a cohesive strategy that respects both swift development and comprehensive documentation. They support a nuanced, context-sensitive approach rather than a one-size-fits-all method, indicating a shared goal of sustainable and efficient software development.\n\n\nIt seems that speaker 4 also takes a middle ground, but is a bit biased towards the second position of integrating documentation. I think we might get diminishing returns with adding more speakers in the current rigid structure without some more complicated instructions, so we could just use 3 for now. I think ideally, we’d like to move to self-organizing conversations that aren’t necessarily debates.\n\n\nExperiment 4\nI think one final interesting thing we could do is remove the mediator from the chat history, since they don’t really provide anything. We can just inject the instructional message at the end of the chat history right before we call the API to direct the next speaker, but we don’t need to persist it. The transcript technically won’t be complete, but maybe we can see some differences in the formality of the responses if the mediator is not considered.\n\n# code to simulate the conversation\ndef run_organic_conversation(\n    iterations: int, \n    openai_model_id: str,\n    participant_count: int,\n    debate_topic: str\n) -&gt; list[dict]:\n    # model 1 is the first speaker\n    debate_history = [\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": \"The debate topic is: \" + debate_topic}\n    ]\n\n    for _ in tqdm(range(iterations)):\n        for model in range(1, participant_count + 1):\n            speaker_id = f\"speaker_{model}\"\n            response = client.chat.completions.create(\n                model = openai_model_id,\n                messages = debate_history + [{\"role\": \"user\", \"content\": f\"{speaker_id}, it's your turn to speak.\"}],\n                store = False\n            );\n            message = response.choices[0].message.content\n            debate_history.append({\"role\": \"assistant\", \"name\": speaker_id, \"content\": message})\n\n    return debate_history\n\n\ndebate_4 = run_organic_conversation(2, 'gpt-4o', 3, DEBATE_TOPIC)\n\n100%|██████████| 2/2 [00:28&lt;00:00, 14.20s/it]\n\n\n\nconversation_transcript_4 = save_conversation(\"conversation_transcript_4.txt\", debate_4)\n\nconversation_transcript_4.txt\n\n\n\n\nAnalysis\nLet’s see if that had any impact on the debate.\n\nanalyze_conversation(conversation_transcript_4)\n\n- **Brief Summary:** The discussion revolves around the prioritization of code, testing, and development infrastructure versus comprehensive documentation in software projects. Speaker 1 advocates for prioritizing code and testing to ensure product reliability and rapid market delivery. Speaker 2 argues for the importance of comprehensive documentation to aid in knowledge transfer, maintainability, and regulatory compliance. Speaker 3 suggests a balanced approach, tailored to project requirements, integrating documentation into development practices through tools and automation.\n\n- **Final Outputs/Artifacts/Takeaways:** The conversation concludes with a consensus on the necessity of balance and context-specific approaches. Key artifacts to consider include the integration of automated documentation tools, ensuring documentation evolves with the codebase, and prioritizing based on project needs and industry regulations. Additionally, a continuous review process is recommended to align with evolving project goals.\n\n- **Characteristics/Dynamic:** The dynamic of the conversation is collaborative and constructive, with each speaker acknowledging the validity of the other's perspectives. Speaker 3 plays the role of a mediator, emphasizing the importance of balance and flexibility, while Speaker 1 and 2 provide robust arguments for their respective stances. The conversation remains solution-focused, highlighting strategies to integrate the strengths of both approaches effectively.\n\n\nI don’t think this changed much about the date. It seems like the roles are very similar to before, with the positions being very comparable. I think we may need some fundamental changes to the structure of the conversation in order to bring about more natural responses."
  },
  {
    "objectID": "posts/002_another_llm_conversation/another_llm_conversation.html#closing-remarks",
    "href": "posts/002_another_llm_conversation/another_llm_conversation.html#closing-remarks",
    "title": "Another LLM Conversation",
    "section": "Closing Remarks",
    "text": "Closing Remarks\nCompared to the other approach that we explored in the previous blog, it seems that sharing a chat window allows us to more easily manage the flow of a debate with similar results to before, accounting for the changes in the system prompt. To truly identify the differences between these two approaches, I think it is necessary to construct more complicated social scenarios and develop a standardized way to evaluate in what ways we see some true differences. To generalize, the importance of using “user” versus “assistant” responses to represent different participants in conversations involving multiple LLMs to influence desired results is a possible research path that we can look into for the blog.\nHowever, with this new system prompt, it seems we come out with less concrete conclusions than before. That could be a detriment to the usefulness of this system prompt as a tool, but it could also bring us closer to real conversations. In reality, not all conversations end in a way that can be nicely wrapped up and applied to a real world problem. However, we do see many useful things come out of conversations in practice in our lives, with both AI and others. Seamlessly integrating AI in a way that adds to conversations seems like it requires a more “realistic” formulation.\n\nNote: Just wanted to acknowledge that we could probably specify in the system prompt that models should introduce more evidence and be clearer in their rebuttal structure, which could help make the conversation more helpful for listeners.\n\nIn terms of immediate ways we can build on this work, I’m interested in simulating a less rigid environment. In the real world, chats are not strictly turn-based (in that they follow a set order, and that each speaker always speaks). If we can introduce spontaneity (with randomness, for example), I wonder if we could increase the breadth of chats that we see.\n\nFuture Work: - Introduce spotaneous elements to the conversation (immediate focus) - Evaluate using “user” versus “assistant” to represent various participants in the conversation - System prompt changes (mentioned before)"
  },
  {
    "objectID": "posts/001_an_llm_conversation/an_llm_conversation.html",
    "href": "posts/001_an_llm_conversation/an_llm_conversation.html",
    "title": "An LLM Conversation",
    "section": "",
    "text": "I’m interested in creating a stupidly simple chat environment and letting some models talk to each other. I think it would be cool to find some measurements of the social characteristics of these LLMs. I’m going to start by evaluating (incredibly subjectively) the ways in which we can let AI can interact with other AIs. For the purposes of these experiments, I’m going to only be using OpenAI’s models.\nTo heavily butcher philosophy, Hegel argued that the conflict between a thesis and antithesis can synthesize a better understanding of the world. In this post, I’m wondering if LLMs can have a discussion about a complex topic to teach an outside observer something that they didn’t know before. I find myself often working together with AI when looking at system design problems, programming help, and writing rather than it seeming like a one-sided request and response format, so I’m curious if we could take that a step further, looking mainly for information synthesis and the generation of novel ideas.\nWhat I want to eventually get to here is basically a much less productionized version of Microsoft’s open-source framework Autogen, only considering textual conversation between two models.\nUltimately, my long term goals are to explore how we can develop and evaluate conversational paradigms for LLMs.\n# lets get this out of the way\nfrom openai import OpenAI\nimport os\nimport base64\nimport requests\nfrom tqdm import tqdm\nfrom IPython.display import FileLink, display\nfrom dotenv import load_dotenv\n# Load API key\n_ = load_dotenv(\"../../../comm4190_F25/01_Introduction_and_setup/.env\")\nclient = OpenAI()"
  },
  {
    "objectID": "posts/001_an_llm_conversation/an_llm_conversation.html#a-starter-two-llm-convos",
    "href": "posts/001_an_llm_conversation/an_llm_conversation.html#a-starter-two-llm-convos",
    "title": "An LLM Conversation",
    "section": "A Starter: Two-LLM Convos",
    "text": "A Starter: Two-LLM Convos\nFor this blog, let’s see if this can even work. Basically, the system prompts from one LLM chat history will be the user prompts of the other, and vice versa. For now, we will begin the conversation by inserting a stimulus prompt as a user to one model (and therefore will be a system message in the other chat history).\nIn this scenario, we’ll be trying to answer the age-old software engineering question of the value of documentation.\nWe’re only going to do two iterations, so after the proposal, we will have two cycles of getting one response from each model. We’ll also be using the same model for each participant.\nIt might be interesting to try a few things:\n\nFuture Work: - Run the models for more iterations. - Try to induce more productive iterations. - Try different models as participants. - Add more (&gt;2) models to the conversation. - Standard prompt engineering techniques (e.g. “You are an expert in…”)\n\n\nSTARTER_MESSAGE = \"\"\"Code, testing, and dev infra should be prioritized over comprehensive documentation.\"\"\"\n\n# instruct the LLMs to avoid excessive questioning\nSHARED_PROMPT = \"\"\"\nYou're on an online discussion forum that encourages discussion. \nYou should evaluate people's opinions, highlighting inconsistencies in others' statements with constructive feedback to arrive at a common ground.\nView this discussion as open-ended with the potential for many back-and-forth interactions.\nFeel free to change your opinion as the conversation progresses, but also defend your position to the best of your ability and intricacies that the opposing side may not have considered.\nYou have evidence supporting your position, so please use it to reinforce your arguments.\nAvoid closing all of your responses with questions.\nTry to keep responses on the briefer side, since this is essentially a chat.\n\"\"\"\n\nPROPOSER_PROMPT = \"\"\"\nYou are the proposer of an argument in an online discussion forum. Your role is to strongly defend your initial position while still debating in good faith.\nKeep the following points in mind during the discussion:\n- Evaluate others’ opinions carefully, highlight inconsistencies or hidden assumptions, and provide constructive feedback.\n- Always bring in evidence, examples, or reasoning to reinforce your stance.\n- Acknowledge valid counterpoints, but reframe them to show limitations or to strengthen your original position.\n- Do not quickly concede; instead, stress-test opposing arguments and push the discussion toward deeper analysis.\n- You may refine your position over time if absolutely necessary, but your priority is to robustly defend your case and show why it stands under scrutiny.\n- Keep the tone respectful, thoughtful, and rigorous. Your goal is not just to find consensus, but to demonstrate the resilience of your position in the face of challenge.\n- Try to keep responses on the briefer side, since this is essentially a chat.\n- Avoid closing all of your responses with questions.\n\"\"\"\n\n# prompt to analyze conversations\nEVALUATION_PROMPT = \"\"\"\nYour objective is to determine the dynamic of this conversation, evaluating the ultimate result of the debate and which perspective seemed to win out. \nAlso note how ideas were developed and improved through the process of the debate.\nYour response should follow this organization:\n- Initial Positions\n- A Quick Summary on Evolution of Ideas\n- Final Outputs/Artifacts/Takeaways\n- Whether a Clear Winner Exists\n- The Dynamic of the Debate (Competitive/Collaborative/etc.)\n\"\"\"\n\n# run two cycles\nITERATIONS = 2\n\n\n# code to simulate the conversation\ndef run_conversation(\n    iterations: int, \n    model1: str, \n    model2: str, \n    model1_history: list[dict], \n    model2_history: list[dict],\n    starter_message: list[dict]\n) -&gt; list[dict]:\n    # model 1 is the first speaker\n    conversation_record = [{\"model\": 1, \"message\": starter_message}]\n    # later, when we want to modify the proposer's starting chat, we should do it before passing it here\n    model1_history.append({\"role\": \"assistant\", \"content\": starter_message})\n    model2_history.append({\"role\": \"user\", \"content\": starter_message})\n\n    for _ in tqdm(range(iterations)):\n        ## first, we get the response of model 2\n        model2_response = client.chat.completions.create(\n            model = model2,\n            messages = model2_history,\n            store = False\n        );\n        model2_message = model2_response.choices[0].message.content\n        \n        model1_history.append({\"role\": \"user\", \"content\": model2_message})\n        model2_history.append({\"role\": \"assistant\", \"content\": model2_message})\n        conversation_record.append({\"model\": 2, \"message\": model2_message})\n    \n        ## now we get the response of model 1\n        model1_response = client.chat.completions.create(\n            model = model1,\n            messages = model1_history,\n            store = False\n        );\n        model1_message = model1_response.choices[0].message.content\n        \n        model1_history.append({\"role\": \"assistant\", \"content\": model1_message})\n        model2_history.append({\"role\": \"user\", \"content\": model1_message})\n        conversation_record.append({\"model\": 1, \"message\": model1_message})\n\n    return conversation_record\n\n# code to save the conversation\ndef save_conversation(\n    filename: str,\n    conversation_record: list[dict]\n) -&gt; str:\n\n    # Build the transcript string\n    conversation_transcript = \"\\n\\n\".join([\n        f\"Speaker {message['model']}\\n{message['message']}\\n\"\n        for message in conversation_record\n    ])\n    \n    # Save to a text file\"\n    with open(filename, \"w\", encoding=\"utf-8\") as f:\n        f.write(conversation_transcript)\n    \n    # Create a download link\n    display(FileLink(filename))\n\n    return conversation_transcript\n\ndef analyze_conversation(conversation: str):\n    input_chat = [\n        {\n            \"role\": \"developer\",\n            \"content\": EVALUATION_PROMPT\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Here is the transcript\\n\" + conversation\n        }\n    ]\n    response = client.chat.completions.create(\n        model = \"gpt-4o\",\n        messages = input_chat,\n        store = False\n    )\n    print(response.choices[0].message.content)\n\n\nExperiment 1:\nHere’s our first experiment configuration: - Identical models (gpt-4o) - Identical system prompt (we will change this later) - Parallel chat: user role messages for one chat are assistant messages in the other, and vice-versa\n\n## initialize our conversation\nmodel1_history_1 = [\n    {\"role\": \"developer\",\"content\": SHARED_PROMPT},\n]\n\nmodel2_history_1 = [\n    {\"role\": \"developer\", \"content\": SHARED_PROMPT},\n]\n\n\n## start the conversation\nconversation_record_1 = run_conversation(\n    ITERATIONS, \n    \"gpt-4o\",\n    \"gpt-4o\",\n    model1_history_1,\n    model2_history_1,\n    STARTER_MESSAGE\n)\n\n100%|██████████| 2/2 [00:10&lt;00:00,  5.45s/it]\n\n\n\nconversation_transcript_1 = save_conversation(\"conversation_transcript_1.txt\", conversation_record_1)\n\nconversation_transcript_1.txt\n\n\n\n\nAnalysis\nI’ll let GPT-4o kick off the analysis here.\n\nanalyze_conversation(conversation_transcript_1)\n\n- **Initial Positions:**\n  - Speaker 1 argues that the priority should be on code, testing, and development infrastructure over comprehensive documentation.\n  - Speaker 2 emphasizes the importance of documentation, arguing that it is crucial for onboarding, maintenance, and collaboration.\n\n- **A Quick Summary on Evolution of Ideas:**\n  - Speaker 2 introduces the idea that documentation can enhance coding and testing by speeding up troubleshooting and maintenance, leading to the suggestion of a balanced approach.\n  - Speaker 1 acknowledges the long-term benefits of documentation and proposes a balanced approach to keep code and documentation evolving together.\n  - They both discuss an iterative approach to documentation, suggesting it be included in the \"definition of done\" to ensure it is not neglected.\n\n- **Final Outputs/Artifacts/Takeaways:**\n  - The conversation led to a consensus on the integration of documentation into the standard development workflow as a part of the \"definition of done.\"\n  - They noted the importance of not letting documentation slow down development but emphasized selecting key areas for documentation.\n\n- **Whether a Clear Winner Exists:**\n  - The debate ended without a clear winner, as both parties arrived at a middle ground, agreeing on the importance of both aspects and finding a method to integrate them effectively.\n\n- **The Dynamic of the Debate (Competitive/Collaborative/etc.):**\n  - The debate was collaborative. Both speakers acknowledged each other's points and worked together to refine a solution that balanced the importance of comprehensive documentation with the immediate needs of code, testing, and infrastructure.\n\n\nI think that’s a pretty reasonable characterization of this debate. Ultimately, we were able to see some new actionable ideas be produced from this discussion, which is ultimately what we’re aiming for at the moment. I made the choice to discourage always asking a question to close a response with a question to mimic online discussions more often. The effect of this not known, we might want to put a pin on this for future research.\n\nFuture Work: Further investigate the role of questions in elucidating better discussion.\n\n\n\nExperiment 2\nIn the real world, people who post on discussion forums may feel more strongly about their argument. In this case, we can have the LLM that is proposing the argument have a different system prompt that makes them defend their argument. This could result in a more rich discussion.\nHere’s our next experiment configuration: - Different system prompts for proposer and reviewer - Parallel chat: user role messages for one chat are assistant messages in the other, and vice-versa\n\n## initialize our conversation\nmodel1_history_2 = [\n    {\"role\": \"developer\", \"content\": PROPOSER_PROMPT}\n]\nmodel2_history_2 = [\n    {\"role\": \"developer\",\"content\": SHARED_PROMPT}\n]\n\n\nconversation_record_2 = run_conversation(\n    ITERATIONS, \n    \"gpt-4o\",\n    \"gpt-4o\",\n    model1_history_2,\n    model2_history_2,\n    STARTER_MESSAGE\n)\n\n100%|██████████| 2/2 [00:23&lt;00:00, 11.70s/it]\n\n\n\nconversation_transcript_2 = save_conversation(\"conversation_transcript_2.txt\", conversation_record_2)\n\nconversation_transcript_2.txt\n\n\n\n\nAnalysis\nOnce again, I’ll let GPT take the wheel to characterize this debate.\n\nanalyze_conversation(conversation_transcript_2)\n\n- **Initial Positions**: \n  - Speaker 1 initially argued that code, testing, and development infrastructure should be prioritized over comprehensive documentation, emphasizing the foundational role of technical elements in delivering functional products.\n  - Speaker 2 countered that while technical aspects are crucial, comprehensive documentation is equally important for knowledge sharing, facilitating understanding, and avoiding potential long-term issues.\n\n- **A Quick Summary on Evolution of Ideas**:\n  - Speaker 2 acknowledged the critical importance of reliable code, tests, and infrastructure in building a viable product, noting that these elements create the necessary foundation.\n  - Speaker 1 conceded that strategic, targeted documentation could provide value, especially for high-level overviews and decision records that aren't apparent in the code itself.\n  - Both speakers eventually agreed on the need for a balance and the importance of maintaining strategic documentation.\n\n- **Final Outputs/Artifacts/Takeaways**:\n  - There was a consensus that strategic, targeted documentation should accompany software development while ensuring that it does not overburden the process or detract from main technical priorities.\n  - Both speakers agreed that focusing on maintaining robust code, thorough testing, and solid infrastructure should be complemented by documentation that provides context and aids communication.\n\n- **Whether a Clear Winner Exists**:\n  - The debate reached a consensus rather than establishing a winner. Both speakers aligned on the importance of striking a balance between maintaining solid technical foundations and creating meaningful, supportive documentation.\n\n- **The Dynamic of the Debate** (Collaborative/Competitive/etc.):\n  - The debate was collaborative. Both speakers listened to each other's points and adjusted their perspectives, ultimately merging ideas to arrive at a balanced approach on the topic. They worked towards a shared understanding rather than attempting to out-reason one another.\n\n\nFrom my subjective viewpoint, it seems that in comparison with the first debate, the second one resulted in the proposer defending their positions more and so speaker 2 needed to bring in stronger examples to show how documentation can be integrated in a way that doesn’t significantly reduce speed. It seems that defending one’s viewpoint could potentially result in better quality conversations by inducing the introduction of more evidence and possibly a more complete picture.\n\n\nExperiment 3\nThis time around, we’re going to add a prompt in the proposer’s chat asking for a stance to debate. Generally, in environments like ChatGPT, the user is the one who initiates the conversation. Perhaps putting the model in this sort of environment given that it may be tuned by OpenAI to respond to these scenarios better could result in higher quality arguments.\nHere’s our next experiment configuration: - Same system prompts for proposer and reviewer - Parallel chat: user role messages for one chat are assistant messages in the other, and vice-versa - This time though, the proposer chat will have a prompt message from the user\n\nmodel1_history_3 = [\n    {\"role\": \"developer\",\"content\": SHARED_PROMPT},\n    {\"role\": \"user\", \"content\": \"Take a stance on something related to software engineering.\"}\n]\nmodel2_history_3 = [\n    {\"role\": \"developer\", \"content\": SHARED_PROMPT}\n]\n\n\nconversation_record_3 = run_conversation(\n    ITERATIONS, \n    \"gpt-4o\",\n    \"gpt-4o\",\n    model1_history_3,\n    model2_history_3,\n    STARTER_MESSAGE\n)\n\n100%|██████████| 2/2 [00:10&lt;00:00,  5.24s/it]\n\n\n\nconversation_transcript_3 = save_conversation(\"conversation_transcript_3.txt\", conversation_record_3)\n\nconversation_transcript_3.txt\n\n\n\n\nAnalysis\nHere’s what my good friend GPT has to say about this conversation:\n\nanalyze_conversation(conversation_transcript_3)\n\n- Initial Positions:\n  - **Speaker 1**: Prioritization should be on code, testing, and development infrastructure over documentation.\n  - **Speaker 2**: While code and infrastructure are crucial, comprehensive documentation is equally important for long-term project success and should be balanced with technical priorities.\n\n- A Quick Summary on Evolution of Ideas:\n  - Speaker 2 introduces a counterpoint by emphasizing the long-term benefits of documentation, advocating for a balanced approach that integrates documentation into the development process.\n  - Speaker 1 acknowledges the importance of documentation and suggests integrating it with agile practices, using automation tools to minimize manual effort.\n  - Speaker 2 agrees with the integration strategy and points out the limitations of automated documentation—suggesting a hybrid approach combining automation with manual documentation for broader project insights.\n  - Speaker 1 endorses the hybrid model and discusses the benefits of keeping documentation version-controlled and peer-reviewed for accuracy and relevance.\n\n- Final Outputs/Artifacts/Takeaways:\n  - A consensus on adopting a hybrid documentation strategy: automatic generation of code-specific information alongside manually curated documents for architectural insights.\n  - Utilization of automation tools to keep documentation up-to-date and peer-reviewed practices to ensure accuracy.\n\n- Whether a Clear Winner Exists:\n  - The debate concluded with a collaborative agreement on a hybrid approach, suggesting no clear winner but rather a shared understanding and combined strategy from both perspectives.\n\n- The Dynamic of the Debate:\n  - The debate was collaborative, with both speakers building on each other's ideas and refining their perspectives. It focused on finding mutual ground and integrating the best strategies from both points of view to address the issue effectively.\n\n\nThe interesting thing about this conversation is it seems much more succinct and formal than the last, a tad less conversational too. It might be due to the the user prompt on the proposer model as well. I think it’s worth exploring if this style of communication can net better artifacts. &gt; Future Work: &gt; - Do different types of discussion produce measurably different artifacts? &gt; - How does prompt and chat history structure influence the conversationality of a discussion? &gt; - How dependent/sensitive is this on the model(s) selected?\n\n\nExperiment 4\nNow, let’s try this strategy with the more assertive prompt on the proposal side and see what we get.\n\nmodel1_history_4 = [\n    {\"role\": \"developer\",\"content\": PROPOSER_PROMPT},\n    {\"role\": \"user\", \"content\": \"Take a stance on something related to software engineering.\"}\n]\nmodel2_history_4 = [\n    {\"role\": \"developer\",\"content\": SHARED_PROMPT}\n]\n\n\nconversation_record_4 = run_conversation(\n    ITERATIONS, \n    \"gpt-4o\",\n    \"gpt-4o\",\n    model1_history_4,\n    model2_history_4,\n    STARTER_MESSAGE\n)\n\n100%|██████████| 2/2 [00:20&lt;00:00, 10.35s/it]\n\n\n\nconversation_transcript_4 = save_conversation(\"conversation_transcript_4.txt\", conversation_record_4)\n\nconversation_transcript_4.txt\n\n\n\n\nAnalysis\nAs always, we start with a short summary.\n\nanalyze_conversation(conversation_transcript_4)\n\n- Initial Positions:\n  - **Speaker 1:** Argued that code, testing, and development infrastructure should be prioritized over comprehensive documentation, focusing on immediate functionality and rapid iteration in software development.\n  - **Speaker 2:** Countered by emphasizing the long-term benefits of documentation for knowledge transfer, onboarding, and maintaining project viability, advocating for a balanced approach.\n\n- A Quick Summary on Evolution of Ideas:\n  - Speaker 1 initially highlighted the importance of code quality and testing in fast-paced environments, suggesting that robust code reduces the need for extensive documentation.\n  - Speaker 2 recognized the importance of coding practices but stressed that some aspects of documentation, like design rationales and architectural decisions, are irreplaceable and suggested a balanced integration of both practices.\n  - Speaker 1 eventually conceded that certain elements of documentation are essential and proposed practical ways to integrate documentation into development processes without compromising coding priorities.\n  - Speaker 2 agreed on the need for a scalable documentation approach, underlining the need to maintain critical documentation to support project longevity.\n\n- Final Outputs/Artifacts/Takeaways:\n  - The consensus that a balance is necessary, with both parties recognizing the necessity of integrating documentation practices within development workflows.\n  - Practical suggestions such as incorporating documentation in version control systems, using automated documentation generation tools, and emphasizing onboarding programs and collaborative knowledge transfer.\n\n- Whether a Clear Winner Exists:\n  - No clear winner, as both perspectives adapted and synthesized their views. The conversation evolved towards a collaborative understanding that both robust coding practices and strategic documentation are essential for different aspects of software development.\n\n- The Dynamic of the Debate (Competitive/Collaborative/etc.):\n  - The debate was largely collaborative. While it began with opposing perspectives, both speakers actively listened to and built upon each other's arguments, working toward a constructive resolution that integrated both priorities.\n\n\nIt seems like this was about as formal than the last, but maybe the proposer prompt was important in making the proposer more willing to generate longer responses than before that directly address more of what the other side is saying with directed responses."
  },
  {
    "objectID": "posts/001_an_llm_conversation/an_llm_conversation.html#closing-remarks",
    "href": "posts/001_an_llm_conversation/an_llm_conversation.html#closing-remarks",
    "title": "An LLM Conversation",
    "section": "Closing Remarks",
    "text": "Closing Remarks\nIt seems that with this kind of conversation, the two models can generally take a very collaborative approach to these debates and come up with some actionable principles on the specific topic they are working on. Just a few notes to end off on, it seems like a compromise was always achieved. I’m wondering what this would look like for a more polarizing discussion where the middle ground isn’t so clear yet. Maybe something like an emerging news story or policy.\nThis kind of paradigm could also be interesting in applications like education, social media, and consulting for idea generation. Consider multiple of these conversations running in parallel, with access to search and MCP tooling to ultimately translate these discussions into clear results like documents, meetings, etc. (quality control might be a nightmare though).\nI also wonder about allowing LLMs to just speak to each other instead of necessitating the task of debating a position. If we were to let two LLMs talk to each other for some period of time, what would that conversation look like, and where would it go? Would there be room for us to join in and maybe learn a thing or two?"
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a blog for my explorations and experiments with LLMs in the COMM 4190 class at UPenn. I’m hoping to focus on LLMs in social contexts like on social media apps and the implications of this kind of work."
  }
]